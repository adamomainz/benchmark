start dynamic
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/canary_models/torchrec_dlrm/__init__.py", line 14, in <module>
    from pyre_extensions import none_throws
ModuleNotFoundError: No module named 'pyre_extensions'
Failed to import user benchmark module dynamo, error: No module named 'pyre_extensions'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3566, in run
    change_linear_weights_to_int8_dqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 103, in change_linear_weights_to_int8_dqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  [Previous line repeated 2 more times]
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 237, in from_float
    w_int_repr, w_scales, _ = dynamically_quantize_per_channel(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_primitives.py", line 171, in dynamically_quantize_per_channel
    min_val, max_val = torch.aminmax(x, dim=1)
RuntimeError: Object of type 'NoneType' is not an instance of 'sequence'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
cuda eval  Background_Matting                 
AUTOTUNE convolution(1x3x518x518, 64x3x7x7)
  convolution 0.2714 ms 100.0%
  triton_convolution_3 0.4521 ms 60.0%
  triton_convolution_4 0.5057 ms 53.7%
  triton_convolution_0 0.5583 ms 48.6%
  triton_convolution_5 0.5727 ms 47.4%
  triton_convolution_2 0.7064 ms 38.4%
  triton_convolution_1 0.8410 ms 32.3%
SingleProcess AUTOTUNE takes 3.5851 seconds
AUTOTUNE convolution(1x64x512x512, 128x64x3x3)
  convolution 0.0726 ms 100.0%
  triton_convolution_12 0.4440 ms 16.4%
  triton_convolution_6 0.4451 ms 16.3%
  triton_convolution_9 0.5064 ms 14.3%
  triton_convolution_11 0.5515 ms 13.2%
  triton_convolution_7 0.6793 ms 10.7%
  triton_convolution_10 0.9149 ms 7.9%
  triton_convolution_8 1.3003 ms 5.6%
SingleProcess AUTOTUNE takes 4.5588 seconds
AUTOTUNE convolution(1x128x256x256, 256x128x3x3)
  convolution 0.0614 ms 100.0%
  triton_convolution_18 0.4242 ms 14.5%
  triton_convolution_19 0.4546 ms 13.5%
  triton_convolution_16 0.4936 ms 12.4%
  triton_convolution_13 0.5870 ms 10.5%
  triton_convolution_17 0.6703 ms 9.2%
  triton_convolution_14 0.7527 ms 8.2%
  triton_convolution_15 1.4665 ms 4.2%
SingleProcess AUTOTUNE takes 4.5550 seconds
AUTOTUNE mm(16384x512, 512x64)
  triton_mm_41 0.0247 ms 100.0%
  triton_mm_48 0.0256 ms 96.3%
  triton_mm_43 0.0262 ms 94.0%
  triton_mm_42 0.0264 ms 93.4%
  triton_mm_44 0.0266 ms 92.9%
  triton_mm_40 0.0284 ms 86.9%
  mm 0.0287 ms 86.0%
  triton_mm_45 0.0292 ms 84.6%
  triton_mm_47 0.0300 ms 82.3%
  triton_mm_46 0.0307 ms 80.5%
SingleProcess AUTOTUNE takes 4.8972 seconds
AUTOTUNE convolution(1x1x518x518, 64x1x7x7)
  triton_convolution_57 0.2001 ms 100.0%
  convolution 0.2004 ms 99.9%
  triton_convolution_53 0.2223 ms 90.0%
  triton_convolution_55 0.2538 ms 78.9%
  triton_convolution_52 0.3001 ms 66.7%
  triton_convolution_56 0.3561 ms 56.2%
  triton_convolution_54 0.3731 ms 53.6%
SingleProcess AUTOTUNE takes 2.8377 seconds
AUTOTUNE mm(16384x448, 448x256)
  triton_mm_98 0.0337 ms 100.0%
  triton_mm_97 0.0357 ms 94.4%
  triton_mm_99 0.0361 ms 93.5%
  triton_mm_100 0.0361 ms 93.4%
  mm 0.0396 ms 85.1%
  triton_mm_104 0.0412 ms 81.8%
  triton_mm_96 0.0445 ms 75.7%
  triton_mm_103 0.0478 ms 70.5%
  triton_mm_106 0.0622 ms 54.2%
  triton_mm_101 0.0644 ms 52.4%
SingleProcess AUTOTUNE takes 4.5765 seconds
AUTOTUNE convolution(1x256x130x130, 256x256x3x3)
  convolution 0.0960 ms 100.0%
  triton_convolution_113 0.6616 ms 14.5%
  triton_convolution_111 0.6786 ms 14.1%
  triton_convolution_114 0.7294 ms 13.2%
  triton_convolution_112 1.0208 ms 9.4%
  triton_convolution_108 1.0818 ms 8.9%
  triton_convolution_109 1.1658 ms 8.2%
  triton_convolution_110 2.8942 ms 3.3%
SingleProcess AUTOTUNE takes 4.4034 seconds
AUTOTUNE convolution(1x256x256x256, 128x256x3x3)
  convolution 0.1711 ms 100.0%
  triton_convolution_251 1.2266 ms 14.0%
  triton_convolution_254 1.3495 ms 12.7%
  triton_convolution_253 1.3513 ms 12.7%
  triton_convolution_248 1.7067 ms 10.0%
  triton_convolution_252 2.5692 ms 6.7%
  triton_convolution_249 2.6840 ms 6.4%
  triton_convolution_250 5.1949 ms 3.3%
SingleProcess AUTOTUNE takes 4.1565 seconds
AUTOTUNE convolution(1x128x512x512, 64x128x3x3)
  convolution 0.2019 ms 100.0%
  triton_convolution_261 1.1657 ms 17.3%
  triton_convolution_260 1.3215 ms 15.3%
  triton_convolution_255 1.3655 ms 14.8%
  triton_convolution_256 1.7015 ms 11.9%
  triton_convolution_259 1.9592 ms 10.3%
  triton_convolution_258 2.0110 ms 10.0%
  triton_convolution_257 5.0693 ms 4.0%
SingleProcess AUTOTUNE takes 4.1126 seconds
AUTOTUNE convolution(1x64x518x518, 1x64x7x7)
  convolution 0.4003 ms 100.0%
  triton_convolution_262 1.1543 ms 34.7%
  triton_convolution_266 1.3506 ms 29.6%
  triton_convolution_263 1.3537 ms 29.6%
  triton_convolution_267 1.4872 ms 26.9%
  triton_convolution_265 1.7071 ms 23.4%
  triton_convolution_264 22.7965 ms 1.8%
SingleProcess AUTOTUNE takes 4.0596 seconds
AUTOTUNE convolution(1x256x512x512, 64x256x3x3)
  convolution 0.3761 ms 100.0%
  triton_convolution_323 2.6568 ms 14.2%
  triton_convolution_317 2.9993 ms 12.5%
  triton_convolution_322 3.2143 ms 11.7%
  triton_convolution_320 4.0131 ms 9.4%
  triton_convolution_318 4.9109 ms 7.7%
  triton_convolution_321 5.1275 ms 7.3%
  triton_convolution_319 10.4533 ms 3.6%
SingleProcess AUTOTUNE takes 4.5558 seconds
AUTOTUNE convolution(1x64x518x518, 3x64x7x7)
  convolution 0.3960 ms 100.0%
  triton_convolution_328 1.3660 ms 29.0%
  triton_convolution_324 1.4454 ms 27.4%
  triton_convolution_325 1.4589 ms 27.1%
  triton_convolution_329 1.4976 ms 26.4%
  triton_convolution_327 1.7501 ms 22.6%
  triton_convolution_326 24.1723 ms 1.6%
SingleProcess AUTOTUNE takes 4.1210 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 26.00it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 35.75it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 38.84it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 40.32it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 41.15it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 41.63it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 39.98it/s]
2.078x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/DALLE2_pytorch/__init__.py", line 3, in <module>
    from dalle2_pytorch import DALLE2, Unet, Decoder, DiffusionPriorNetwork, DiffusionPrior, OpenAIClipAdapter
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/dalle2_pytorch/__init__.py", line 9, in <module>
    from dalle2_pytorch.dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/dalle2_pytorch/dalle2_pytorch.py", line 18, in <module>
    from kornia.filters import gaussian_blur2d
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/__init__.py", line 8, in <module>
    from . import augmentation, color, contrib, core, enhance, feature, io, losses, metrics, morphology, tracking, utils, x
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/x/__init__.py", line 2, in <module>
    from .trainer import Trainer
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/x/trainer.py", line 11, in <module>
    from accelerate import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  LearningToPaint                    
AUTOTUNE convolution(96x9x128x128, 64x9x3x3)
  triton_convolution_4 0.1737 ms 100.0%
  triton_convolution_3 0.1861 ms 93.3%
  convolution 0.2512 ms 69.1%
  triton_convolution_5 0.2619 ms 66.3%
  triton_convolution_0 0.2812 ms 61.8%
  triton_convolution_1 0.4925 ms 35.3%
  triton_convolution_2 0.5073 ms 34.2%
SingleProcess AUTOTUNE takes 3.0618 seconds
AUTOTUNE convolution(96x64x64x64, 64x64x3x3)
  convolution 0.0826 ms 100.0%
  triton_convolution_12 0.3769 ms 21.9%
  triton_convolution_6 0.3810 ms 21.7%
  triton_convolution_11 0.4259 ms 19.4%
  triton_convolution_7 0.5044 ms 16.4%
  triton_convolution_9 0.6161 ms 13.4%
  triton_convolution_10 0.6898 ms 12.0%
  triton_convolution_8 1.0366 ms 8.0%
SingleProcess AUTOTUNE takes 4.4311 seconds
AUTOTUNE convolution(96x64x32x32, 64x64x3x3)
  convolution 0.0577 ms 100.0%
  triton_convolution_13 0.2202 ms 26.2%
  triton_convolution_19 0.2425 ms 23.8%
  triton_convolution_18 0.2474 ms 23.3%
  triton_convolution_16 0.3166 ms 18.2%
  triton_convolution_14 0.3441 ms 16.8%
  triton_convolution_17 0.3505 ms 16.5%
  triton_convolution_15 0.9628 ms 6.0%
SingleProcess AUTOTUNE takes 4.0759 seconds
AUTOTUNE convolution(96x64x64x64, 64x64x1x1)
  convolution 0.0306 ms 100.0%
  triton_convolution_25 0.0366 ms 83.5%
  triton_convolution_21 0.0370 ms 82.7%
  triton_convolution_20 0.0372 ms 82.1%
  triton_convolution_24 0.0374 ms 81.8%
  triton_convolution_26 0.0394 ms 77.7%
  triton_convolution_23 0.0433 ms 70.7%
  triton_convolution_22 0.1285 ms 23.8%
SingleProcess AUTOTUNE takes 3.5087 seconds
AUTOTUNE convolution(96x64x32x32, 128x64x3x3)
  convolution 0.0338 ms 100.0%
  triton_convolution_47 0.1651 ms 20.5%
  triton_convolution_42 0.1965 ms 17.2%
  triton_convolution_41 0.2015 ms 16.8%
  triton_convolution_44 0.2132 ms 15.8%
  triton_convolution_46 0.2270 ms 14.9%
  triton_convolution_45 0.2862 ms 11.8%
  triton_convolution_43 0.5029 ms 6.7%
SingleProcess AUTOTUNE takes 3.9467 seconds
AUTOTUNE convolution(96x128x16x16, 128x128x3x3)
  convolution 0.0410 ms 100.0%
  triton_convolution_54 0.2220 ms 18.4%
  triton_convolution_51 0.2223 ms 18.4%
  triton_convolution_48 0.2477 ms 16.5%
  triton_convolution_53 0.2654 ms 15.4%
  triton_convolution_52 0.3068 ms 13.3%
  triton_convolution_49 0.3421 ms 12.0%
  triton_convolution_50 0.9508 ms 4.3%
SingleProcess AUTOTUNE takes 4.1720 seconds
AUTOTUNE convolution(96x64x32x32, 128x64x1x1)
  convolution 0.0150 ms 100.0%
  triton_convolution_55 0.0181 ms 82.5%
  triton_convolution_56 0.0196 ms 76.3%
  triton_convolution_58 0.0205 ms 72.9%
  triton_convolution_59 0.0213 ms 70.4%
  triton_convolution_61 0.0224 ms 66.9%
  triton_convolution_60 0.0231 ms 64.8%
  triton_convolution_57 0.0669 ms 22.4%
SingleProcess AUTOTUNE takes 3.8087 seconds
AUTOTUNE convolution(96x128x16x16, 256x128x3x3)
  convolution 0.0296 ms 100.0%
  triton_convolution_81 0.1612 ms 18.3%
  triton_convolution_82 0.1968 ms 15.0%
  triton_convolution_79 0.2403 ms 12.3%
  triton_convolution_76 0.3028 ms 9.8%
  triton_convolution_80 0.3178 ms 9.3%
  triton_convolution_77 0.3443 ms 8.6%
  triton_convolution_78 0.4933 ms 6.0%
SingleProcess AUTOTUNE takes 4.7729 seconds
AUTOTUNE convolution(96x256x8x8, 256x256x3x3)
  convolution 0.0415 ms 100.0%
  triton_convolution_88 0.2473 ms 16.8%
  triton_convolution_89 0.2784 ms 14.9%
  triton_convolution_86 0.2880 ms 14.4%
  triton_convolution_87 0.4049 ms 10.2%
  triton_convolution_83 0.5402 ms 7.7%
  triton_convolution_84 0.5623 ms 7.4%
  triton_convolution_85 0.9036 ms 4.6%
SingleProcess AUTOTUNE takes 4.5908 seconds
AUTOTUNE convolution(96x128x16x16, 256x128x1x1)
  convolution 0.0123 ms 100.0%
  triton_convolution_93 0.0180 ms 68.6%
  triton_convolution_96 0.0186 ms 66.3%
  triton_convolution_94 0.0193 ms 63.8%
  triton_convolution_95 0.0201 ms 61.4%
  triton_convolution_91 0.0231 ms 53.4%
  triton_convolution_90 0.0248 ms 49.7%
  triton_convolution_92 0.0636 ms 19.4%
SingleProcess AUTOTUNE takes 5.0871 seconds
AUTOTUNE convolution(96x256x8x8, 512x256x3x3)
  convolution 0.0282 ms 100.0%
  triton_convolution_116 0.3286 ms 8.6%
  triton_convolution_117 0.3816 ms 7.4%
  triton_convolution_115 0.4058 ms 7.0%
  triton_convolution_114 0.4637 ms 6.1%
  triton_convolution_111 0.6116 ms 4.6%
  triton_convolution_112 0.6920 ms 4.1%
  triton_convolution_113 0.9430 ms 3.0%
SingleProcess AUTOTUNE takes 4.4603 seconds
AUTOTUNE convolution(96x512x4x4, 512x512x3x3)
  convolution 0.0475 ms 100.0%
  triton_convolution_123 0.5374 ms 8.8%
  triton_convolution_122 0.5591 ms 8.5%
  triton_convolution_124 0.6292 ms 7.5%
  triton_convolution_121 0.7268 ms 6.5%
  triton_convolution_119 1.1294 ms 4.2%
  triton_convolution_118 1.2191 ms 3.9%
  triton_convolution_120 1.6135 ms 2.9%
SingleProcess AUTOTUNE takes 4.4244 seconds
AUTOTUNE convolution(96x256x8x8, 512x256x1x1)
  convolution 0.0113 ms 100.0%
  triton_convolution_129 0.0181 ms 62.3%
  triton_convolution_128 0.0259 ms 43.4%
  triton_convolution_131 0.0265 ms 42.6%
  triton_convolution_130 0.0269 ms 41.8%
  triton_convolution_126 0.0354 ms 31.8%
  triton_convolution_125 0.0372 ms 30.3%
  triton_convolution_127 0.1166 ms 9.7%
SingleProcess AUTOTUNE takes 4.3088 seconds
AUTOTUNE int_mm(96x512, 512x65, 96x65)
  triton_mm_151 0.0104 ms 100.0%
  triton_mm_154 0.0106 ms 98.2%
  triton_mm_152 0.0111 ms 94.2%
  triton_mm_150 0.0139 ms 74.9%
  triton_mm_155 0.0146 ms 71.3%
  triton_mm_149 0.0148 ms 70.7%
  triton_mm_147 0.0152 ms 68.5%
  triton_mm_148 0.0158 ms 66.1%
  triton_mm_146 0.0166 ms 62.8%
  triton_mm_153 0.0257 ms 40.5%
SingleProcess AUTOTUNE takes 5.1985 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 68.03it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 100.68it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 103.50it/s]
3.944x
loading model: 0it [00:00, ?it/s]WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
loading model: 0it [00:03, ?it/s]
WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
cuda eval  Super_SloMo                        
WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
AUTOTUNE convolution(6x6x352x352, 32x6x7x7)
  convolution 0.2480 ms 100.0%
  triton_convolution_0 1.0128 ms 24.5%
  triton_convolution_4 1.0358 ms 23.9%
  triton_convolution_3 1.0410 ms 23.8%
  triton_convolution_5 1.2189 ms 20.3%
  triton_convolution_1 1.2248 ms 20.2%
  triton_convolution_2 1.2510 ms 19.8%
SingleProcess AUTOTUNE takes 2.6293 seconds
AUTOTUNE convolution(6x32x352x352, 32x32x7x7)
  convolution 0.6080 ms 100.0%
  triton_convolution_12 1.5715 ms 38.7%
  triton_convolution_9 1.7219 ms 35.3%
  triton_convolution_10 1.9112 ms 31.8%
  triton_convolution_6 1.9830 ms 30.7%
  triton_convolution_11 1.9891 ms 30.6%
  triton_convolution_7 2.4968 ms 24.4%
  triton_convolution_8 5.1375 ms 11.8%
SingleProcess AUTOTUNE takes 3.0266 seconds
AUTOTUNE convolution(6x32x176x176, 64x32x5x5)
  convolution 0.1007 ms 100.0%
  triton_convolution_13 0.4811 ms 20.9%
  triton_convolution_19 0.5037 ms 20.0%
  triton_convolution_18 0.5352 ms 18.8%
  triton_convolution_17 0.7513 ms 13.4%
  triton_convolution_14 0.7653 ms 13.2%
  triton_convolution_16 0.7801 ms 12.9%
  triton_convolution_15 1.3237 ms 7.6%
SingleProcess AUTOTUNE takes 4.7030 seconds
AUTOTUNE convolution(6x64x176x176, 64x64x5x5)
  convolution 0.2059 ms 100.0%
  triton_convolution_26 0.9580 ms 21.5%
  triton_convolution_20 0.9774 ms 21.1%
  triton_convolution_25 1.0714 ms 19.2%
  triton_convolution_24 1.4791 ms 13.9%
  triton_convolution_23 1.5372 ms 13.4%
  triton_convolution_21 1.7256 ms 11.9%
  triton_convolution_22 4.6739 ms 4.4%
SingleProcess AUTOTUNE takes 4.3078 seconds
AUTOTUNE convolution(6x64x88x88, 128x64x3x3)
  convolution 0.0466 ms 100.0%
  triton_convolution_33 0.2151 ms 21.7%
  triton_convolution_30 0.2320 ms 20.1%
  triton_convolution_27 0.2367 ms 19.7%
  triton_convolution_32 0.2476 ms 18.8%
  triton_convolution_31 0.3235 ms 14.4%
  triton_convolution_28 0.3370 ms 13.8%
  triton_convolution_29 0.9692 ms 4.8%
SingleProcess AUTOTUNE takes 4.2727 seconds
AUTOTUNE convolution(6x128x88x88, 128x128x3x3)
  convolution 0.0751 ms 100.0%
  triton_convolution_40 0.4278 ms 17.6%
  triton_convolution_37 0.4557 ms 16.5%
  triton_convolution_39 0.4929 ms 15.2%
  triton_convolution_34 0.5118 ms 14.7%
  triton_convolution_38 0.6468 ms 11.6%
  triton_convolution_35 0.6768 ms 11.1%
  triton_convolution_36 1.9206 ms 3.9%
SingleProcess AUTOTUNE takes 4.0779 seconds
AUTOTUNE convolution(6x128x44x44, 256x128x3x3)
  convolution 0.0398 ms 100.0%
  triton_convolution_46 0.1996 ms 20.0%
  triton_convolution_44 0.2013 ms 19.8%
  triton_convolution_47 0.2197 ms 18.1%
  triton_convolution_41 0.2792 ms 14.3%
  triton_convolution_45 0.3092 ms 12.9%
  triton_convolution_42 0.3424 ms 11.6%
  triton_convolution_43 0.9593 ms 4.2%
SingleProcess AUTOTUNE takes 4.5000 seconds
AUTOTUNE convolution(6x256x44x44, 256x256x3x3)
  convolution 0.0674 ms 100.0%
  triton_convolution_53 0.4286 ms 15.7%
  triton_convolution_51 0.4372 ms 15.4%
  triton_convolution_54 0.5061 ms 13.3%
  triton_convolution_48 0.5590 ms 12.1%
  triton_convolution_49 0.6749 ms 10.0%
  triton_convolution_52 0.7728 ms 8.7%
  triton_convolution_50 1.9068 ms 3.5%
SingleProcess AUTOTUNE takes 4.8417 seconds
AUTOTUNE convolution(6x256x22x22, 512x256x3x3)
  convolution 0.0391 ms 100.0%
  triton_convolution_60 0.2122 ms 18.4%
  triton_convolution_58 0.2297 ms 17.0%
  triton_convolution_61 0.2761 ms 14.2%
  triton_convolution_59 0.4111 ms 9.5%
  triton_convolution_55 0.5212 ms 7.5%
  triton_convolution_56 0.5809 ms 6.7%
  triton_convolution_57 0.9261 ms 4.2%
SingleProcess AUTOTUNE takes 4.8091 seconds
AUTOTUNE convolution(6x512x22x22, 512x512x3x3)
  convolution 0.0666 ms 100.0%
  triton_convolution_67 0.4541 ms 14.7%
  triton_convolution_68 0.5500 ms 12.1%
  triton_convolution_65 0.6634 ms 10.0%
  triton_convolution_62 1.0008 ms 6.7%
  triton_convolution_66 1.0684 ms 6.2%
  triton_convolution_63 1.1691 ms 5.7%
  triton_convolution_64 1.8530 ms 3.6%
SingleProcess AUTOTUNE takes 4.9068 seconds
AUTOTUNE convolution(6x512x11x11, 512x512x3x3)
  convolution 0.0294 ms 100.0%
  triton_convolution_73 0.4467 ms 6.6%
  triton_convolution_74 0.4902 ms 6.0%
  triton_convolution_75 0.6314 ms 4.7%
  triton_convolution_72 0.6673 ms 4.4%
  triton_convolution_69 1.0586 ms 2.8%
  triton_convolution_70 1.2554 ms 2.3%
  triton_convolution_71 1.3353 ms 2.2%
SingleProcess AUTOTUNE takes 4.7133 seconds
AUTOTUNE convolution(6x1024x22x22, 512x1024x3x3)
  convolution 0.1243 ms 100.0%
  triton_convolution_95 0.9078 ms 13.7%
  triton_convolution_96 1.1364 ms 10.9%
  triton_convolution_93 1.3217 ms 9.4%
  triton_convolution_90 2.0153 ms 6.2%
  triton_convolution_94 2.1749 ms 5.7%
  triton_convolution_91 2.3306 ms 5.3%
  triton_convolution_92 3.6941 ms 3.4%
SingleProcess AUTOTUNE takes 4.7419 seconds
AUTOTUNE convolution(6x512x44x44, 256x512x3x3)
  convolution 0.1242 ms 100.0%
  triton_convolution_102 0.9107 ms 13.6%
  triton_convolution_103 1.0348 ms 12.0%
  triton_convolution_97 1.1210 ms 11.1%
  triton_convolution_100 1.2456 ms 10.0%
  triton_convolution_98 1.3627 ms 9.1%
  triton_convolution_101 2.0058 ms 6.2%
  triton_convolution_99 3.8195 ms 3.3%
SingleProcess AUTOTUNE takes 4.6807 seconds
AUTOTUNE convolution(6x256x88x88, 128x256x3x3)
  convolution 0.1328 ms 100.0%
  triton_convolution_114 0.9693 ms 13.7%
  triton_convolution_116 0.9828 ms 13.5%
  triton_convolution_117 1.0784 ms 12.3%
  triton_convolution_111 1.2383 ms 10.7%
  triton_convolution_112 1.7024 ms 7.8%
  triton_convolution_115 1.8870 ms 7.0%
  triton_convolution_113 3.8384 ms 3.5%
SingleProcess AUTOTUNE takes 4.1136 seconds
AUTOTUNE convolution(6x128x176x176, 64x128x3x3)
  convolution 0.1608 ms 100.0%
  triton_convolution_131 0.8228 ms 19.5%
  triton_convolution_130 0.9593 ms 16.8%
  triton_convolution_125 0.9930 ms 16.2%
  triton_convolution_126 1.3979 ms 11.5%
  triton_convolution_129 1.4467 ms 11.1%
  triton_convolution_128 1.4636 ms 11.0%
  triton_convolution_127 3.3933 ms 4.7%
SingleProcess AUTOTUNE takes 3.8272 seconds
AUTOTUNE convolution(6x64x352x352, 32x64x3x3)
  convolution 0.2592 ms 100.0%
  triton_convolution_139 0.8507 ms 30.5%
  triton_convolution_145 0.9456 ms 27.4%
  triton_convolution_142 0.9996 ms 25.9%
  triton_convolution_143 1.0485 ms 24.7%
  triton_convolution_144 1.1079 ms 23.4%
  triton_convolution_140 1.1689 ms 22.2%
  triton_convolution_141 3.4140 ms 7.6%
SingleProcess AUTOTUNE takes 3.4503 seconds
AUTOTUNE convolution(6x32x352x352, 4x32x3x3)
  convolution 0.1160 ms 100.0%
  triton_convolution_154 0.2850 ms 40.7%
  triton_convolution_153 0.3091 ms 37.5%
  triton_convolution_155 0.3137 ms 37.0%
  triton_convolution_157 0.4868 ms 23.8%
  triton_convolution_158 0.4915 ms 23.6%
  triton_convolution_156 0.5060 ms 22.9%
SingleProcess AUTOTUNE takes 2.6662 seconds
AUTOTUNE convolution(6x20x352x352, 32x20x7x7)
  convolution 0.5424 ms 100.0%
  triton_convolution_165 1.3557 ms 40.0%
  triton_convolution_162 1.3781 ms 39.4%
  triton_convolution_164 1.6456 ms 33.0%
  triton_convolution_163 1.7734 ms 30.6%
  triton_convolution_159 2.1295 ms 25.5%
  triton_convolution_160 2.2343 ms 24.3%
  triton_convolution_161 2.9856 ms 18.2%
SingleProcess AUTOTUNE takes 3.4300 seconds
AUTOTUNE convolution(6x32x352x352, 5x32x3x3)
  convolution 0.1162 ms 100.0%
  triton_convolution_313 0.3496 ms 33.2%
  triton_convolution_314 0.4650 ms 25.0%
  triton_convolution_317 0.5045 ms 23.0%
  triton_convolution_318 0.5067 ms 22.9%
  triton_convolution_316 0.5727 ms 20.3%
  triton_convolution_315 7.2492 ms 1.6%
SingleProcess AUTOTUNE takes 3.3286 seconds
AUTOTUNE convolution(6x3x352x352, 64x3x3x3)
  convolution 0.1441 ms 100.0%
  triton_convolution_323 0.2906 ms 49.6%
  triton_convolution_322 0.2992 ms 48.1%
  triton_convolution_324 0.3890 ms 37.0%
  triton_convolution_319 0.4012 ms 35.9%
  triton_convolution_321 0.4413 ms 32.6%
  triton_convolution_320 0.6667 ms 21.6%
SingleProcess AUTOTUNE takes 3.1465 seconds
AUTOTUNE convolution(6x64x352x352, 64x64x3x3)
  convolution 0.3327 ms 100.0%
  triton_convolution_331 1.6298 ms 20.4%
  triton_convolution_325 1.6299 ms 20.4%
  triton_convolution_330 1.8583 ms 17.9%
  triton_convolution_326 2.4178 ms 13.8%
  triton_convolution_328 2.7806 ms 12.0%
  triton_convolution_329 2.8757 ms 11.6%
  triton_convolution_327 6.5993 ms 5.0%
SingleProcess AUTOTUNE takes 4.0657 seconds
AUTOTUNE convolution(6x64x176x176, 128x64x3x3)
  convolution 0.1487 ms 100.0%
  triton_convolution_332 0.8171 ms 18.2%
  triton_convolution_338 0.8217 ms 18.1%
  triton_convolution_335 0.8385 ms 17.7%
  triton_convolution_337 0.9351 ms 15.9%
  triton_convolution_333 1.2252 ms 12.1%
  triton_convolution_336 1.4352 ms 10.4%
  triton_convolution_334 3.4089 ms 4.4%
SingleProcess AUTOTUNE takes 4.1788 seconds
AUTOTUNE convolution(6x128x176x176, 128x128x3x3)
  convolution 0.2641 ms 100.0%
  triton_convolution_345 1.6275 ms 16.2%
  triton_convolution_342 1.6561 ms 15.9%
  triton_convolution_339 1.8232 ms 14.5%
  triton_convolution_344 1.8562 ms 14.2%
  triton_convolution_340 2.5238 ms 10.5%
  triton_convolution_343 2.8459 ms 9.3%
  triton_convolution_341 6.7605 ms 3.9%
SingleProcess AUTOTUNE takes 4.3628 seconds
AUTOTUNE convolution(6x128x88x88, 256x128x3x3)
  convolution 0.1255 ms 100.0%
  triton_convolution_351 0.6660 ms 18.8%
  triton_convolution_352 0.7400 ms 17.0%
  triton_convolution_349 0.7698 ms 16.3%
  triton_convolution_346 1.0309 ms 12.2%
  triton_convolution_350 1.2116 ms 10.4%
  triton_convolution_347 1.3108 ms 9.6%
  triton_convolution_348 3.3621 ms 3.7%
SingleProcess AUTOTUNE takes 4.6203 seconds
AUTOTUNE convolution(6x256x88x88, 256x256x3x3)
  convolution 0.2370 ms 100.0%
  triton_convolution_358 1.4523 ms 16.3%
  triton_convolution_356 1.7243 ms 13.7%
  triton_convolution_359 1.8936 ms 12.5%
  triton_convolution_353 2.1048 ms 11.3%
  triton_convolution_354 3.4720 ms 6.8%
  triton_convolution_357 3.7310 ms 6.4%
  triton_convolution_355 6.7180 ms 3.5%
SingleProcess AUTOTUNE takes 4.8882 seconds
AUTOTUNE convolution(6x256x44x44, 512x256x3x3)
  convolution 0.1290 ms 100.0%
  triton_convolution_420 0.7825 ms 16.5%
  triton_convolution_418 0.7914 ms 16.3%
  triton_convolution_421 0.9396 ms 13.7%
  triton_convolution_415 1.0596 ms 12.2%
  triton_convolution_416 1.2688 ms 10.2%
  triton_convolution_419 1.4007 ms 9.2%
  triton_convolution_417 3.7679 ms 3.4%
SingleProcess AUTOTUNE takes 4.6741 seconds
AUTOTUNE convolution(6x512x44x44, 512x512x3x3)
  convolution 0.2506 ms 100.0%
  triton_convolution_427 1.7058 ms 14.7%
  triton_convolution_428 1.9305 ms 13.0%
  triton_convolution_422 2.1433 ms 11.7%
  triton_convolution_425 2.3038 ms 10.9%
  triton_convolution_423 2.6087 ms 9.6%
  triton_convolution_426 3.5323 ms 7.1%
  triton_convolution_424 7.5586 ms 3.3%
SingleProcess AUTOTUNE takes 5.5221 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:03,  8.91it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 14.58it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:01, 16.44it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 17.36it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 17.85it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 18.16it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 18.31it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 18.39it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 18.44it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 18.45it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 18.48it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 18.51it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 18.54it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 18.60it/s]running benchmark:  97%|█████████▋| 29/30 [00:01<00:00, 18.63it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 17.99it/s]
1.616x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  alexnet                            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 78.31it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 85.99it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 88.61it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 87.40it/s]
3.247x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_edgecnn                  
AUTOTUNE mm(200000x128, 128x64)
  triton_mm_2 0.0630 ms 100.0%
  triton_mm_8 0.0637 ms 98.8%
  triton_mm_1 0.0637 ms 98.8%
  triton_mm_3 0.0641 ms 98.3%
  triton_mm_7 0.0647 ms 97.4%
  triton_mm_0 0.0654 ms 96.3%
  triton_mm_4 0.0661 ms 95.3%
  mm 0.0699 ms 90.2%
  triton_mm_10 0.0791 ms 79.6%
  triton_mm_9 0.0793 ms 79.5%
SingleProcess AUTOTUNE takes 4.1268 seconds
AUTOTUNE addmm(200000x64, 200000x64, 64x64)
  triton_mm_20 0.0432 ms 100.0%
  triton_mm_13 0.0471 ms 91.7%
  triton_mm_14 0.0479 ms 90.3%
  triton_mm_15 0.0486 ms 88.9%
  triton_mm_16 0.0492 ms 87.9%
  bias_addmm 0.0500 ms 86.4%
  triton_mm_12 0.0508 ms 85.1%
  triton_mm_19 0.0520 ms 83.1%
  triton_mm_21 0.0555 ms 77.9%
  triton_mm_23 0.0650 ms 66.5%
SingleProcess AUTOTUNE takes 4.7777 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 209.17it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 212.47it/s]
1.369x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_gcn                      
AUTOTUNE mm(10000x64, 64x64)
  triton_mm_2 0.0088 ms 100.0%
  triton_mm_1 0.0089 ms 99.6%
  triton_mm_3 0.0091 ms 97.5%
  triton_mm_8 0.0091 ms 97.5%
  triton_mm_0 0.0097 ms 91.4%
  triton_mm_7 0.0097 ms 90.8%
  triton_mm_4 0.0099 ms 89.6%
  triton_mm_9 0.0099 ms 89.6%
  triton_mm_5 0.0099 ms 89.3%
  triton_mm_6 0.0101 ms 87.3%
SingleProcess AUTOTUNE takes 4.1997 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 127.24it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 128.59it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 128.41it/s]
1.061x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_gin                      
AUTOTUNE addmm(10000x64, 10000x64, 64x64)
  triton_mm_61 0.0092 ms 100.0%
  triton_mm_60 0.0094 ms 98.0%
  triton_mm_64 0.0097 ms 95.4%
  triton_mm_65 0.0097 ms 95.0%
  triton_mm_68 0.0099 ms 93.2%
  triton_mm_69 0.0099 ms 93.2%
  triton_mm_62 0.0099 ms 92.9%
  triton_mm_63 0.0099 ms 92.9%
  triton_mm_67 0.0104 ms 88.9%
  triton_mm_70 0.0105 ms 87.5%
SingleProcess AUTOTUNE takes 4.8571 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 323.37it/s]
1.207x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_sage                     
AUTOTUNE addmm(10000x64, 10000x64, 64x64)
  triton_mm_63 0.0107 ms 100.0%
  triton_mm_62 0.0107 ms 99.7%
  triton_mm_65 0.0108 ms 98.8%
  triton_mm_69 0.0108 ms 98.2%
  triton_mm_61 0.0110 ms 97.1%
  triton_mm_68 0.0110 ms 96.8%
  triton_mm_67 0.0111 ms 95.7%
  triton_mm_64 0.0112 ms 94.9%
  triton_mm_60 0.0113 ms 94.6%
  triton_mm_66 0.0114 ms 93.3%
SingleProcess AUTOTUNE takes 4.4398 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 164.36it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 169.99it/s]
1.208x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  cm3leon_generate                   
AUTOTUNE mm(1x1536, 1536x1536)
  mm 0.0132 ms 100.0%
  triton_mm_6 0.0177 ms 74.6%
  triton_mm_5 0.0182 ms 72.4%
  triton_mm_8 0.0184 ms 71.8%
  triton_mm_9 0.0193 ms 68.4%
  triton_mm_4 0.0195 ms 67.5%
  triton_mm_3 0.0215 ms 61.2%
  triton_mm_2 0.0276 ms 47.9%
  triton_mm_1 0.0293 ms 45.1%
  triton_mm_0 0.0460 ms 28.6%
SingleProcess AUTOTUNE takes 3.9850 seconds
AUTOTUNE addmm(1x1536, 1x1536, 1536x1536)
  bias_addmm 0.0130 ms 100.0%
  addmm 0.0158 ms 82.4%
  triton_mm_18 0.0186 ms 70.2%
  triton_mm_17 0.0189 ms 68.7%
  triton_mm_20 0.0190 ms 68.5%
  triton_mm_16 0.0196 ms 66.3%
  triton_mm_21 0.0203 ms 64.2%
  triton_mm_15 0.0225 ms 57.9%
  triton_mm_14 0.0276 ms 47.1%
  triton_mm_13 0.0295 ms 44.2%
SingleProcess AUTOTUNE takes 4.2794 seconds
AUTOTUNE bmm(16x1x96, 16x96x1)
  triton_bmm_27 0.0062 ms 100.0%
  triton_bmm_26 0.0063 ms 99.5%
  triton_bmm_28 0.0070 ms 89.4%
  triton_bmm_24 0.0072 ms 86.7%
  triton_bmm_25 0.0073 ms 85.5%
  triton_bmm_29 0.0075 ms 83.0%
  triton_bmm_31 0.0078 ms 79.9%
  triton_bmm_30 0.0079 ms 79.3%
  bmm 0.0096 ms 65.2%
SingleProcess AUTOTUNE takes 2.5318 seconds
AUTOTUNE bmm(16x1x1, 16x1x96)
  bmm 0.0060 ms 100.0%
  triton_bmm_46 0.0062 ms 96.9%
  triton_bmm_48 0.0062 ms 96.9%
  triton_bmm_51 0.0063 ms 96.4%
  triton_bmm_52 0.0063 ms 95.9%
  triton_bmm_45 0.0065 ms 93.6%
  triton_bmm_50 0.0068 ms 88.7%
  triton_bmm_44 0.0068 ms 88.5%
  triton_bmm_47 0.0068 ms 88.3%
  triton_bmm_49 0.0069 ms 87.3%
SingleProcess AUTOTUNE takes 2.4611 seconds
AUTOTUNE mm(1x1536, 1536x6144)
  mm 0.0268 ms 100.0%
  triton_mm_72 0.0272 ms 98.5%
  triton_mm_70 0.0276 ms 97.1%
  triton_mm_74 0.0279 ms 96.1%
  triton_mm_69 0.0284 ms 94.2%
  triton_mm_68 0.0330 ms 81.3%
  triton_mm_67 0.0350 ms 76.6%
  triton_mm_75 0.0393 ms 68.1%
  triton_mm_71 0.0411 ms 65.2%
  triton_mm_66 0.0538 ms 49.8%
SingleProcess AUTOTUNE takes 3.8862 seconds
AUTOTUNE mm(1x6144, 6144x1536)
  mm 0.0238 ms 100.0%
  triton_mm_83 0.0511 ms 46.6%
  triton_mm_84 0.0525 ms 45.4%
  triton_mm_86 0.0550 ms 43.3%
  triton_mm_87 0.0554 ms 43.0%
  triton_mm_82 0.0580 ms 41.0%
  triton_mm_81 0.0671 ms 35.5%
  triton_mm_80 0.0916 ms 26.0%
  triton_mm_79 0.0986 ms 24.2%
  triton_mm_78 0.1472 ms 16.2%
SingleProcess AUTOTUNE takes 3.6205 seconds
AUTOTUNE bmm(16x1x96, 16x96x2)
  triton_bmm_116 0.0063 ms 100.0%
  triton_bmm_118 0.0064 ms 97.3%
  triton_bmm_117 0.0068 ms 91.6%
  triton_bmm_119 0.0069 ms 91.2%
  triton_bmm_115 0.0071 ms 88.7%
  triton_bmm_114 0.0078 ms 80.3%
  triton_bmm_120 0.0079 ms 79.7%
  triton_bmm_121 0.0079 ms 79.7%
  bmm 0.0098 ms 63.8%
SingleProcess AUTOTUNE takes 2.5122 seconds
AUTOTUNE bmm(16x1x2, 16x2x96)
  triton_bmm_134 0.0062 ms 100.0%
  triton_bmm_139 0.0062 ms 100.0%
  triton_bmm_140 0.0062 ms 100.0%
  triton_bmm_142 0.0062 ms 100.0%
  triton_bmm_137 0.0063 ms 99.5%
  triton_bmm_141 0.0063 ms 99.5%
  triton_bmm_135 0.0068 ms 92.0%
  triton_bmm_136 0.0068 ms 91.5%
  triton_bmm_143 0.0068 ms 91.5%
  triton_bmm_138 0.0068 ms 91.3%
SingleProcess AUTOTUNE takes 2.4296 seconds
AUTOTUNE bmm(16x1x96, 16x96x3)
  triton_bmm_206 0.0064 ms 100.0%
  triton_bmm_205 0.0065 ms 98.5%
  triton_bmm_209 0.0070 ms 91.7%
  triton_bmm_207 0.0071 ms 90.5%
  triton_bmm_208 0.0071 ms 90.5%
  triton_bmm_204 0.0074 ms 86.6%
  triton_bmm_210 0.0079 ms 81.3%
  triton_bmm_211 0.0085 ms 75.5%
  bmm 0.0099 ms 64.7%
SingleProcess AUTOTUNE takes 2.4801 seconds
AUTOTUNE bmm(16x1x3, 16x3x96)
  triton_bmm_228 0.0062 ms 100.0%
  triton_bmm_230 0.0062 ms 100.0%
  triton_bmm_231 0.0063 ms 99.5%
  triton_bmm_229 0.0065 ms 96.5%
  triton_bmm_224 0.0067 ms 93.3%
  triton_bmm_226 0.0068 ms 91.5%
  triton_bmm_233 0.0070 ms 89.4%
  triton_bmm_232 0.0070 ms 88.6%
  triton_bmm_225 0.0071 ms 88.2%
  triton_bmm_227 0.0071 ms 88.2%
SingleProcess AUTOTUNE takes 2.6096 seconds
AUTOTUNE bmm(16x1x96, 16x96x4)
  triton_bmm_297 0.0063 ms 100.0%
  triton_bmm_296 0.0070 ms 89.9%
  triton_bmm_295 0.0071 ms 88.7%
  triton_bmm_298 0.0071 ms 88.7%
  triton_bmm_299 0.0075 ms 83.2%
  triton_bmm_294 0.0080 ms 78.7%
  triton_bmm_300 0.0081 ms 77.8%
  triton_bmm_301 0.0086 ms 73.1%
  bmm 0.0100 ms 62.8%
SingleProcess AUTOTUNE takes 2.3870 seconds
AUTOTUNE bmm(16x1x4, 16x4x96)
  triton_bmm_315 0.0062 ms 100.0%
  triton_bmm_318 0.0062 ms 100.0%
  triton_bmm_321 0.0063 ms 99.5%
  triton_bmm_322 0.0065 ms 96.1%
  triton_bmm_323 0.0065 ms 96.1%
  triton_bmm_314 0.0068 ms 91.5%
  triton_bmm_316 0.0068 ms 91.5%
  triton_bmm_320 0.0068 ms 91.5%
  bmm 0.0069 ms 89.9%
  triton_bmm_317 0.0070 ms 88.6%
SingleProcess AUTOTUNE takes 2.8364 seconds
AUTOTUNE bmm(16x1x96, 16x96x5)
  triton_bmm_387 0.0065 ms 100.0%
  triton_bmm_385 0.0066 ms 98.5%
  triton_bmm_388 0.0068 ms 94.8%
  triton_bmm_386 0.0071 ms 91.4%
  triton_bmm_389 0.0077 ms 83.8%
  triton_bmm_384 0.0080 ms 80.8%
  triton_bmm_390 0.0081 ms 79.8%
  triton_bmm_391 0.0087 ms 74.3%
  bmm 0.0101 ms 64.1%
SingleProcess AUTOTUNE takes 2.3732 seconds
AUTOTUNE bmm(16x1x5, 16x5x96)
  triton_bmm_404 0.0063 ms 100.0%
  triton_bmm_406 0.0063 ms 100.0%
  triton_bmm_410 0.0063 ms 100.0%
  triton_bmm_407 0.0064 ms 98.0%
  triton_bmm_411 0.0064 ms 98.0%
  triton_bmm_405 0.0065 ms 96.6%
  triton_bmm_409 0.0065 ms 96.6%
  triton_bmm_412 0.0065 ms 96.6%
  triton_bmm_413 0.0065 ms 96.6%
  triton_bmm_408 0.0068 ms 92.0%
SingleProcess AUTOTUNE takes 2.5476 seconds
AUTOTUNE bmm(16x1x96, 16x96x6)
  triton_bmm_477 0.0064 ms 100.0%
  triton_bmm_478 0.0065 ms 99.0%
  triton_bmm_476 0.0070 ms 91.4%
  triton_bmm_475 0.0071 ms 90.1%
  triton_bmm_479 0.0076 ms 84.5%
  triton_bmm_474 0.0080 ms 80.4%
  triton_bmm_480 0.0087 ms 73.9%
  triton_bmm_481 0.0088 ms 73.4%
  bmm 0.0101 ms 63.8%
SingleProcess AUTOTUNE takes 2.4320 seconds
AUTOTUNE bmm(16x1x6, 16x6x96)
  triton_bmm_494 0.0062 ms 100.0%
  triton_bmm_496 0.0062 ms 100.0%
  triton_bmm_498 0.0063 ms 99.5%
  triton_bmm_499 0.0063 ms 99.5%
  triton_bmm_500 0.0063 ms 99.5%
  triton_bmm_502 0.0063 ms 99.5%
  triton_bmm_503 0.0063 ms 99.5%
  triton_bmm_497 0.0065 ms 96.5%
  bmm 0.0069 ms 89.9%
  triton_bmm_495 0.0070 ms 88.6%
SingleProcess AUTOTUNE takes 2.6369 seconds
AUTOTUNE bmm(16x1x96, 16x96x7)
  triton_bmm_567 0.0065 ms 100.0%
  triton_bmm_568 0.0065 ms 99.5%
  triton_bmm_565 0.0066 ms 97.6%
  triton_bmm_566 0.0070 ms 92.9%
  triton_bmm_569 0.0072 ms 89.8%
  triton_bmm_564 0.0074 ms 87.1%
  triton_bmm_570 0.0082 ms 79.2%
  triton_bmm_571 0.0083 ms 77.7%
  bmm 0.0100 ms 64.3%
SingleProcess AUTOTUNE takes 2.4981 seconds
AUTOTUNE bmm(16x1x7, 16x7x96)
  triton_bmm_584 0.0063 ms 100.0%
  triton_bmm_586 0.0063 ms 100.0%
  triton_bmm_588 0.0063 ms 100.0%
  triton_bmm_590 0.0063 ms 100.0%
  triton_bmm_587 0.0065 ms 97.0%
  triton_bmm_591 0.0065 ms 97.0%
  triton_bmm_585 0.0065 ms 96.6%
  triton_bmm_592 0.0065 ms 96.6%
  triton_bmm_593 0.0065 ms 96.6%
  triton_bmm_589 0.0071 ms 88.7%
SingleProcess AUTOTUNE takes 2.9212 seconds
AUTOTUNE bmm(16x1x96, 16x96x8)
  triton_bmm_657 0.0064 ms 100.0%
  triton_bmm_656 0.0064 ms 99.3%
  triton_bmm_655 0.0065 ms 98.5%
  triton_bmm_658 0.0065 ms 98.5%
  triton_bmm_659 0.0072 ms 89.3%
  triton_bmm_654 0.0074 ms 86.2%
  triton_bmm_660 0.0087 ms 73.8%
  triton_bmm_661 0.0087 ms 73.8%
  bmm 0.0097 ms 66.2%
SingleProcess AUTOTUNE takes 2.5373 seconds
AUTOTUNE bmm(16x1x8, 16x8x96)
  triton_bmm_674 0.0063 ms 100.0%
  triton_bmm_675 0.0063 ms 100.0%
  triton_bmm_676 0.0063 ms 100.0%
  triton_bmm_677 0.0065 ms 97.0%
  triton_bmm_679 0.0065 ms 96.6%
  triton_bmm_682 0.0065 ms 96.6%
  triton_bmm_683 0.0065 ms 96.6%
  bmm 0.0067 ms 93.3%
  triton_bmm_680 0.0068 ms 92.9%
  triton_bmm_678 0.0068 ms 91.6%
SingleProcess AUTOTUNE takes 2.4769 seconds
AUTOTUNE bmm(16x1x96, 16x96x9)
  triton_bmm_748 0.0070 ms 100.0%
  triton_bmm_746 0.0070 ms 99.8%
  triton_bmm_747 0.0071 ms 99.3%
  triton_bmm_745 0.0071 ms 98.9%
  triton_bmm_749 0.0072 ms 98.0%
  triton_bmm_744 0.0074 ms 94.6%
  triton_bmm_751 0.0083 ms 84.4%
  triton_bmm_750 0.0087 ms 80.7%
  bmm 0.0101 ms 69.2%
SingleProcess AUTOTUNE takes 2.3846 seconds
AUTOTUNE bmm(16x1x9, 16x9x96)
  triton_bmm_764 0.0063 ms 100.0%
  triton_bmm_766 0.0063 ms 100.0%
  triton_bmm_768 0.0063 ms 100.0%
  triton_bmm_771 0.0065 ms 97.0%
  triton_bmm_773 0.0065 ms 96.6%
  triton_bmm_770 0.0068 ms 91.6%
  triton_bmm_767 0.0069 ms 90.7%
  triton_bmm_769 0.0070 ms 89.3%
  triton_bmm_765 0.0071 ms 88.7%
  triton_bmm_772 0.0071 ms 88.7%
SingleProcess AUTOTUNE takes 2.6179 seconds
AUTOTUNE bmm(16x1x96, 16x96x10)
  triton_bmm_837 0.0064 ms 100.0%
  triton_bmm_838 0.0065 ms 99.0%
  triton_bmm_836 0.0070 ms 92.4%
  triton_bmm_835 0.0073 ms 88.5%
  triton_bmm_834 0.0074 ms 86.6%
  triton_bmm_839 0.0077 ms 83.2%
  triton_bmm_841 0.0083 ms 77.3%
  triton_bmm_840 0.0087 ms 73.9%
  bmm 0.0101 ms 63.4%
SingleProcess AUTOTUNE takes 2.4697 seconds
AUTOTUNE bmm(16x1x10, 16x10x96)
  triton_bmm_856 0.0063 ms 100.0%
  triton_bmm_858 0.0063 ms 100.0%
  triton_bmm_859 0.0063 ms 100.0%
  triton_bmm_860 0.0063 ms 100.0%
  triton_bmm_862 0.0063 ms 100.0%
  triton_bmm_857 0.0065 ms 97.0%
  triton_bmm_861 0.0065 ms 97.0%
  triton_bmm_854 0.0068 ms 91.6%
  triton_bmm_863 0.0068 ms 91.6%
  triton_bmm_855 0.0071 ms 88.7%
SingleProcess AUTOTUNE takes 2.7909 seconds
AUTOTUNE bmm(16x1x96, 16x96x11)
  triton_bmm_926 0.0065 ms 100.0%
  triton_bmm_927 0.0071 ms 91.9%
  triton_bmm_928 0.0071 ms 91.4%
  triton_bmm_925 0.0072 ms 90.4%
  triton_bmm_924 0.0074 ms 87.5%
  triton_bmm_929 0.0077 ms 84.1%
  triton_bmm_930 0.0084 ms 77.8%
  triton_bmm_931 0.0085 ms 76.0%
  bmm 0.0101 ms 64.4%
SingleProcess AUTOTUNE takes 2.9392 seconds
AUTOTUNE bmm(16x1x11, 16x11x96)
  triton_bmm_944 0.0063 ms 100.0%
  triton_bmm_948 0.0063 ms 100.0%
  triton_bmm_950 0.0063 ms 100.0%
  triton_bmm_947 0.0065 ms 97.0%
  triton_bmm_951 0.0065 ms 97.0%
  triton_bmm_945 0.0065 ms 96.6%
  triton_bmm_953 0.0065 ms 96.6%
  triton_bmm_946 0.0070 ms 89.1%
  triton_bmm_949 0.0071 ms 88.7%
  triton_bmm_952 0.0071 ms 88.7%
SingleProcess AUTOTUNE takes 2.4977 seconds
AUTOTUNE bmm(16x1x96, 16x96x12)
  triton_bmm_1017 0.0065 ms 100.0%
  triton_bmm_1015 0.0067 ms 97.6%
  triton_bmm_1016 0.0070 ms 93.1%
  triton_bmm_1018 0.0072 ms 89.8%
  triton_bmm_1019 0.0075 ms 86.4%
  triton_bmm_1014 0.0080 ms 81.2%
  triton_bmm_1020 0.0084 ms 77.8%
  triton_bmm_1021 0.0087 ms 74.4%
  bmm 0.0101 ms 64.2%
SingleProcess AUTOTUNE takes 2.4478 seconds
AUTOTUNE bmm(16x1x12, 16x12x96)
  triton_bmm_1034 0.0063 ms 100.0%
  triton_bmm_1036 0.0063 ms 100.0%
  triton_bmm_1040 0.0063 ms 100.0%
  triton_bmm_1037 0.0065 ms 96.6%
  triton_bmm_1043 0.0065 ms 96.1%
  bmm 0.0067 ms 93.3%
  triton_bmm_1038 0.0068 ms 91.8%
  triton_bmm_1041 0.0071 ms 88.7%
  triton_bmm_1039 0.0071 ms 88.5%
  triton_bmm_1042 0.0071 ms 88.3%
SingleProcess AUTOTUNE takes 2.9001 seconds
AUTOTUNE bmm(16x1x96, 16x96x13)
  triton_bmm_1106 0.0065 ms 100.0%
  triton_bmm_1107 0.0071 ms 91.9%
  triton_bmm_1108 0.0072 ms 90.6%
  triton_bmm_1105 0.0072 ms 89.8%
  triton_bmm_1104 0.0076 ms 85.3%
  triton_bmm_1109 0.0078 ms 83.5%
  triton_bmm_1110 0.0084 ms 77.8%
  triton_bmm_1111 0.0089 ms 72.8%
  bmm 0.0101 ms 64.0%
SingleProcess AUTOTUNE takes 2.8497 seconds
AUTOTUNE bmm(16x1x13, 16x13x96)
  triton_bmm_1124 0.0063 ms 100.0%
  triton_bmm_1128 0.0063 ms 100.0%
  triton_bmm_1127 0.0065 ms 96.6%
  triton_bmm_1131 0.0065 ms 96.6%
  triton_bmm_1132 0.0065 ms 96.6%
  triton_bmm_1125 0.0066 ms 94.7%
  triton_bmm_1126 0.0070 ms 89.9%
  triton_bmm_1129 0.0071 ms 88.7%
  triton_bmm_1130 0.0071 ms 88.7%
  triton_bmm_1133 0.0071 ms 88.7%
SingleProcess AUTOTUNE takes 2.5773 seconds
AUTOTUNE bmm(16x1x96, 16x96x14)
  triton_bmm_1196 0.0065 ms 100.0%
  triton_bmm_1197 0.0065 ms 100.0%
  triton_bmm_1195 0.0065 ms 99.0%
  triton_bmm_1198 0.0071 ms 90.6%
  triton_bmm_1194 0.0074 ms 87.1%
  triton_bmm_1199 0.0077 ms 83.8%
  triton_bmm_1200 0.0084 ms 77.4%
  triton_bmm_1201 0.0084 ms 77.4%
  bmm 0.0101 ms 63.7%
SingleProcess AUTOTUNE takes 2.4542 seconds
AUTOTUNE bmm(16x1x14, 16x14x96)
  triton_bmm_1214 0.0063 ms 100.0%
  triton_bmm_1216 0.0063 ms 100.0%
  triton_bmm_1218 0.0063 ms 100.0%
  triton_bmm_1219 0.0063 ms 100.0%
  triton_bmm_1222 0.0063 ms 100.0%
  triton_bmm_1223 0.0063 ms 100.0%
  triton_bmm_1220 0.0068 ms 91.8%
  triton_bmm_1221 0.0070 ms 90.1%
  triton_bmm_1217 0.0071 ms 88.7%
  triton_bmm_1215 0.0072 ms 87.3%
SingleProcess AUTOTUNE takes 2.6986 seconds
AUTOTUNE bmm(16x1x96, 16x96x15)
  triton_bmm_1286 0.0065 ms 100.0%
  triton_bmm_1288 0.0065 ms 100.0%
  triton_bmm_1287 0.0069 ms 93.8%
  triton_bmm_1285 0.0071 ms 91.4%
  triton_bmm_1284 0.0076 ms 85.3%
  triton_bmm_1289 0.0077 ms 83.9%
  triton_bmm_1290 0.0084 ms 77.8%
  triton_bmm_1291 0.0088 ms 73.6%
  bmm 0.0102 ms 63.8%
SingleProcess AUTOTUNE takes 2.8293 seconds
AUTOTUNE bmm(16x1x15, 16x15x96)
  triton_bmm_1304 0.0063 ms 100.0%
  triton_bmm_1306 0.0063 ms 100.0%
  triton_bmm_1311 0.0065 ms 96.6%
  triton_bmm_1313 0.0065 ms 96.6%
  triton_bmm_1309 0.0065 ms 96.1%
  triton_bmm_1312 0.0065 ms 96.1%
  triton_bmm_1305 0.0067 ms 93.3%
  triton_bmm_1307 0.0069 ms 91.2%
  triton_bmm_1310 0.0070 ms 90.1%
  triton_bmm_1308 0.0071 ms 88.9%
SingleProcess AUTOTUNE takes 2.7781 seconds
AUTOTUNE bmm(16x1x96, 16x96x16)
  triton_bmm_1375 0.0067 ms 100.0%
  triton_bmm_1377 0.0070 ms 96.6%
  triton_bmm_1376 0.0070 ms 95.7%
  triton_bmm_1378 0.0070 ms 95.5%
  triton_bmm_1374 0.0074 ms 90.5%
  triton_bmm_1379 0.0079 ms 85.0%
  triton_bmm_1381 0.0087 ms 76.9%
  triton_bmm_1380 0.0088 ms 76.4%
  bmm 0.0098 ms 68.4%
SingleProcess AUTOTUNE takes 2.3394 seconds
AUTOTUNE bmm(16x1x16, 16x16x96)
  triton_bmm_1394 0.0063 ms 100.0%
  triton_bmm_1396 0.0063 ms 100.0%
  triton_bmm_1399 0.0063 ms 100.0%
  triton_bmm_1400 0.0063 ms 100.0%
  triton_bmm_1403 0.0063 ms 100.0%
  triton_bmm_1395 0.0065 ms 97.0%
  triton_bmm_1397 0.0065 ms 96.6%
  triton_bmm_1401 0.0065 ms 96.6%
  triton_bmm_1402 0.0069 ms 90.7%
  triton_bmm_1398 0.0070 ms 89.5%
SingleProcess AUTOTUNE takes 2.5590 seconds
AUTOTUNE bmm(16x1x96, 16x96x17)
  triton_bmm_1467 0.0065 ms 100.0%
  triton_bmm_1466 0.0072 ms 90.2%
  triton_bmm_1465 0.0073 ms 89.4%
  triton_bmm_1468 0.0073 ms 89.0%
  triton_bmm_1464 0.0082 ms 79.0%
  triton_bmm_1470 0.0085 ms 76.0%
  triton_bmm_1469 0.0086 ms 75.7%
  triton_bmm_1471 0.0090 ms 72.0%
  bmm 0.0101 ms 64.0%
SingleProcess AUTOTUNE takes 2.5590 seconds
AUTOTUNE bmm(16x1x17, 16x17x96)
  triton_bmm_1485 0.0067 ms 100.0%
  triton_bmm_1487 0.0067 ms 99.5%
  triton_bmm_1488 0.0067 ms 99.5%
  triton_bmm_1491 0.0067 ms 99.5%
  triton_bmm_1484 0.0072 ms 92.9%
  triton_bmm_1490 0.0073 ms 92.1%
  triton_bmm_1486 0.0073 ms 91.7%
  triton_bmm_1492 0.0073 ms 91.7%
  bmm 0.0073 ms 91.3%
  triton_bmm_1489 0.0073 ms 91.3%
SingleProcess AUTOTUNE takes 3.0392 seconds
AUTOTUNE bmm(16x1x96, 16x96x18)
  triton_bmm_1557 0.0065 ms 100.0%
  triton_bmm_1558 0.0065 ms 100.0%
  triton_bmm_1556 0.0072 ms 90.0%
  triton_bmm_1559 0.0073 ms 89.0%
  triton_bmm_1555 0.0076 ms 84.9%
  triton_bmm_1561 0.0089 ms 72.8%
  triton_bmm_1562 0.0089 ms 72.8%
  triton_bmm_1560 0.0092 ms 70.2%
  bmm 0.0101 ms 64.0%
SingleProcess AUTOTUNE takes 2.9467 seconds
AUTOTUNE bmm(16x1x18, 16x18x96)
  triton_bmm_1575 0.0065 ms 100.0%
  triton_bmm_1581 0.0065 ms 100.0%
  triton_bmm_1582 0.0069 ms 93.5%
  triton_bmm_1585 0.0069 ms 93.5%
  triton_bmm_1578 0.0070 ms 93.1%
  triton_bmm_1584 0.0070 ms 93.1%
  triton_bmm_1576 0.0070 ms 92.5%
  triton_bmm_1577 0.0071 ms 91.9%
  triton_bmm_1579 0.0071 ms 91.0%
  triton_bmm_1583 0.0075 ms 87.1%
SingleProcess AUTOTUNE takes 2.9240 seconds
AUTOTUNE bmm(16x1x96, 16x96x19)
  triton_bmm_1648 0.0065 ms 100.0%
  triton_bmm_1649 0.0072 ms 90.2%
  triton_bmm_1650 0.0072 ms 90.2%
  triton_bmm_1647 0.0073 ms 88.8%
  triton_bmm_1646 0.0078 ms 83.5%
  triton_bmm_1651 0.0081 ms 80.2%
  triton_bmm_1652 0.0086 ms 75.7%
  triton_bmm_1653 0.0086 ms 75.7%
  bmm 0.0102 ms 63.8%
SingleProcess AUTOTUNE takes 2.4039 seconds
AUTOTUNE bmm(16x1x19, 16x19x96)
  triton_bmm_1668 0.0067 ms 100.0%
  triton_bmm_1669 0.0067 ms 100.0%
  triton_bmm_1671 0.0067 ms 100.0%
  triton_bmm_1673 0.0067 ms 100.0%
  triton_bmm_1666 0.0072 ms 92.9%
  triton_bmm_1667 0.0073 ms 92.1%
  triton_bmm_1670 0.0073 ms 92.1%
  triton_bmm_1672 0.0073 ms 92.1%
  triton_bmm_1674 0.0073 ms 91.9%
  triton_bmm_1676 0.0076 ms 89.0%
SingleProcess AUTOTUNE takes 2.8047 seconds
AUTOTUNE bmm(16x1x96, 16x96x20)
  triton_bmm_1740 0.0065 ms 100.0%
  triton_bmm_1741 0.0072 ms 90.8%
  triton_bmm_1739 0.0072 ms 89.8%
  triton_bmm_1738 0.0073 ms 89.0%
  triton_bmm_1742 0.0079 ms 82.5%
  triton_bmm_1737 0.0082 ms 79.0%
  triton_bmm_1743 0.0085 ms 76.0%
  triton_bmm_1744 0.0089 ms 72.8%
  bmm 0.0101 ms 64.0%
SingleProcess AUTOTUNE takes 2.2907 seconds
AUTOTUNE bmm(16x1x20, 16x20x96)
  triton_bmm_1757 0.0066 ms 100.0%
  triton_bmm_1759 0.0066 ms 99.0%
  triton_bmm_1763 0.0067 ms 98.6%
  triton_bmm_1762 0.0069 ms 94.5%
  triton_bmm_1765 0.0069 ms 94.5%
  triton_bmm_1760 0.0070 ms 93.6%
  triton_bmm_1767 0.0070 ms 93.6%
  triton_bmm_1758 0.0071 ms 92.8%
  triton_bmm_1761 0.0072 ms 91.1%
  bmm 0.0074 ms 88.6%
SingleProcess AUTOTUNE takes 2.9867 seconds
AUTOTUNE bmm(16x1x96, 16x96x21)
  triton_bmm_1831 0.0065 ms 100.0%
  triton_bmm_1832 0.0067 ms 96.7%
  triton_bmm_1829 0.0073 ms 89.0%
  triton_bmm_1830 0.0073 ms 89.0%
  triton_bmm_1828 0.0083 ms 78.7%
  triton_bmm_1835 0.0086 ms 75.7%
  triton_bmm_1833 0.0089 ms 72.8%
  triton_bmm_1834 0.0090 ms 72.0%
  bmm 0.0102 ms 63.8%
SingleProcess AUTOTUNE takes 2.4296 seconds
AUTOTUNE bmm(16x1x21, 16x21x96)
  triton_bmm_1849 0.0067 ms 100.0%
  triton_bmm_1851 0.0067 ms 100.0%
  triton_bmm_1855 0.0067 ms 100.0%
  triton_bmm_1856 0.0067 ms 100.0%
  triton_bmm_1858 0.0072 ms 93.7%
  triton_bmm_1848 0.0073 ms 92.1%
  triton_bmm_1850 0.0073 ms 92.1%
  triton_bmm_1852 0.0073 ms 92.1%
  triton_bmm_1854 0.0073 ms 92.1%
  triton_bmm_1853 0.0073 ms 91.7%
SingleProcess AUTOTUNE takes 2.9281 seconds
AUTOTUNE bmm(16x1x96, 16x96x22)
  triton_bmm_1921 0.0065 ms 100.0%
  triton_bmm_1922 0.0071 ms 91.4%
  triton_bmm_1923 0.0073 ms 89.4%
  triton_bmm_1920 0.0073 ms 89.0%
  triton_bmm_1919 0.0076 ms 84.9%
  triton_bmm_1926 0.0084 ms 76.9%
  triton_bmm_1925 0.0085 ms 76.0%
  bmm 0.0102 ms 63.8%
  triton_bmm_1924 0.0103 ms 63.0%
SingleProcess AUTOTUNE takes 2.3106 seconds
AUTOTUNE bmm(16x1x22, 16x22x96)
  triton_bmm_1945 0.0065 ms 100.0%
  triton_bmm_1939 0.0066 ms 98.6%
  triton_bmm_1941 0.0066 ms 98.6%
  triton_bmm_1948 0.0070 ms 93.6%
  triton_bmm_1940 0.0071 ms 92.5%
  triton_bmm_1946 0.0072 ms 91.1%
  triton_bmm_1943 0.0072 ms 90.3%
  triton_bmm_1944 0.0073 ms 89.5%
  triton_bmm_1947 0.0075 ms 87.0%
  triton_bmm_1942 0.0076 ms 85.4%
SingleProcess AUTOTUNE takes 3.1739 seconds
AUTOTUNE bmm(16x1x96, 16x96x23)
  triton_bmm_2014 0.0067 ms 100.0%
  triton_bmm_2011 0.0067 ms 99.5%
  triton_bmm_2012 0.0071 ms 93.7%
  triton_bmm_2013 0.0073 ms 91.7%
  triton_bmm_2010 0.0077 ms 86.4%
  triton_bmm_2015 0.0089 ms 74.8%
  triton_bmm_2016 0.0091 ms 73.9%
  triton_bmm_2017 0.0091 ms 73.3%
  bmm 0.0102 ms 65.7%
SingleProcess AUTOTUNE takes 2.5893 seconds
AUTOTUNE bmm(16x1x23, 16x23x96)
  triton_bmm_2031 0.0067 ms 100.0%
  triton_bmm_2032 0.0067 ms 100.0%
  triton_bmm_2034 0.0067 ms 100.0%
  triton_bmm_2036 0.0067 ms 100.0%
  triton_bmm_2030 0.0073 ms 92.1%
  triton_bmm_2033 0.0073 ms 92.1%
  triton_bmm_2037 0.0073 ms 92.1%
  triton_bmm_2038 0.0074 ms 90.5%
  triton_bmm_2039 0.0074 ms 90.5%
  triton_bmm_2035 0.0074 ms 90.3%
SingleProcess AUTOTUNE takes 3.1813 seconds
AUTOTUNE bmm(16x1x96, 16x96x24)
  triton_bmm_2105 0.0066 ms 100.0%
  triton_bmm_2103 0.0071 ms 93.5%
  triton_bmm_2104 0.0071 ms 92.8%
  triton_bmm_2102 0.0073 ms 90.4%
  triton_bmm_2101 0.0082 ms 80.5%
  triton_bmm_2107 0.0084 ms 78.9%
  triton_bmm_2106 0.0085 ms 78.1%
  triton_bmm_2108 0.0089 ms 74.2%
  bmm 0.0098 ms 67.4%
SingleProcess AUTOTUNE takes 2.8000 seconds
AUTOTUNE bmm(16x1x24, 16x24x96)
  triton_bmm_2123 0.0067 ms 100.0%
  triton_bmm_2125 0.0067 ms 100.0%
  triton_bmm_2129 0.0069 ms 96.8%
  triton_bmm_2122 0.0071 ms 95.0%
  triton_bmm_2131 0.0071 ms 95.0%
  triton_bmm_2128 0.0072 ms 93.7%
  triton_bmm_2121 0.0073 ms 92.1%
  triton_bmm_2127 0.0073 ms 92.1%
  bmm 0.0074 ms 90.5%
  triton_bmm_2126 0.0075 ms 89.4%
SingleProcess AUTOTUNE takes 3.0470 seconds
AUTOTUNE mm(1x1536, 1536x2048)
  mm 0.0154 ms 100.0%
  triton_mm_2174 0.0186 ms 82.6%
  triton_mm_2176 0.0193 ms 79.9%
  triton_mm_2172 0.0199 ms 77.5%
  triton_mm_2173 0.0205 ms 74.9%
  triton_mm_2177 0.0208 ms 74.1%
  triton_mm_2171 0.0224 ms 68.6%
  triton_mm_2170 0.0279 ms 55.2%
  triton_mm_2169 0.0292 ms 52.7%
  triton_mm_2168 0.0466 ms 33.1%
SingleProcess AUTOTUNE takes 4.1915 seconds
AUTOTUNE bmm(16x1x96, 16x96x25)
  triton_bmm_2207 0.0065 ms 100.0%
  triton_bmm_2205 0.0067 ms 96.7%
  triton_bmm_2206 0.0073 ms 89.4%
  triton_bmm_2208 0.0073 ms 89.0%
  triton_bmm_2204 0.0083 ms 78.5%
  triton_bmm_2210 0.0091 ms 71.2%
  triton_bmm_2209 0.0092 ms 71.0%
  triton_bmm_2211 0.0092 ms 71.0%
  bmm 0.0102 ms 63.8%
SingleProcess AUTOTUNE takes 2.6701 seconds
AUTOTUNE bmm(16x1x25, 16x25x96)
  triton_bmm_2227 0.0067 ms 100.0%
  triton_bmm_2228 0.0067 ms 100.0%
  triton_bmm_2230 0.0067 ms 100.0%
  triton_bmm_2225 0.0068 ms 99.5%
  triton_bmm_2226 0.0073 ms 92.3%
  triton_bmm_2224 0.0073 ms 92.1%
  triton_bmm_2231 0.0073 ms 92.1%
  triton_bmm_2232 0.0073 ms 92.1%
  triton_bmm_2229 0.0073 ms 91.7%
  triton_bmm_2234 0.0076 ms 88.8%
SingleProcess AUTOTUNE takes 2.9369 seconds
AUTOTUNE bmm(16x1x96, 16x96x26)
  triton_bmm_2299 0.0065 ms 100.0%
  triton_bmm_2298 0.0070 ms 92.3%
  triton_bmm_2297 0.0071 ms 91.0%
  triton_bmm_2296 0.0073 ms 89.0%
  triton_bmm_2295 0.0076 ms 84.9%
  triton_bmm_2302 0.0085 ms 76.5%
  triton_bmm_2301 0.0085 ms 76.0%
  bmm 0.0102 ms 63.8%
  triton_bmm_2300 0.0105 ms 61.6%
SingleProcess AUTOTUNE takes 2.4176 seconds
AUTOTUNE bmm(16x1x26, 16x26x96)
  triton_bmm_2319 0.0067 ms 100.0%
  triton_bmm_2321 0.0067 ms 99.5%
  triton_bmm_2323 0.0069 ms 95.9%
  triton_bmm_2324 0.0071 ms 93.7%
  triton_bmm_2322 0.0072 ms 92.9%
  triton_bmm_2325 0.0072 ms 92.9%
  triton_bmm_2315 0.0072 ms 91.8%
  triton_bmm_2317 0.0073 ms 91.6%
  triton_bmm_2320 0.0075 ms 88.5%
  triton_bmm_2316 0.0076 ms 88.1%
SingleProcess AUTOTUNE takes 3.0135 seconds
AUTOTUNE bmm(16x1x96, 16x96x27)
  triton_bmm_2390 0.0067 ms 100.0%
  triton_bmm_2388 0.0072 ms 94.0%
  triton_bmm_2389 0.0073 ms 92.3%
  triton_bmm_2387 0.0073 ms 92.1%
  triton_bmm_2386 0.0083 ms 81.4%
  triton_bmm_2392 0.0089 ms 75.3%
  triton_bmm_2391 0.0092 ms 73.4%
  triton_bmm_2393 0.0092 ms 73.4%
  bmm 0.0102 ms 66.0%
SingleProcess AUTOTUNE takes 2.6025 seconds
AUTOTUNE bmm(16x1x27, 16x27x96)
  triton_bmm_2406 0.0067 ms 100.0%
  triton_bmm_2409 0.0067 ms 100.0%
  triton_bmm_2410 0.0067 ms 100.0%
  triton_bmm_2411 0.0067 ms 100.0%
  triton_bmm_2412 0.0067 ms 100.0%
  triton_bmm_2408 0.0073 ms 92.1%
  triton_bmm_2414 0.0073 ms 92.1%
  triton_bmm_2413 0.0073 ms 91.7%
  triton_bmm_2407 0.0074 ms 90.7%
  triton_bmm_2416 0.0077 ms 87.5%
SingleProcess AUTOTUNE takes 3.0033 seconds
AUTOTUNE bmm(16x1x96, 16x96x28)
  triton_bmm_2479 0.0065 ms 100.0%
  triton_bmm_2478 0.0067 ms 96.7%
  triton_bmm_2481 0.0073 ms 89.4%
  triton_bmm_2480 0.0073 ms 89.0%
  triton_bmm_2477 0.0076 ms 84.9%
  triton_bmm_2484 0.0084 ms 77.5%
  triton_bmm_2482 0.0089 ms 72.8%
  triton_bmm_2483 0.0089 ms 72.8%
  bmm 0.0102 ms 63.8%
SingleProcess AUTOTUNE takes 2.4010 seconds
AUTOTUNE bmm(16x1x28, 16x28x96)
  triton_bmm_2501 0.0067 ms 100.0%
  triton_bmm_2502 0.0067 ms 100.0%
  triton_bmm_2505 0.0067 ms 100.0%
  bmm 0.0068 ms 99.5%
  triton_bmm_2500 0.0072 ms 93.7%
  triton_bmm_2506 0.0072 ms 93.7%
  triton_bmm_2507 0.0072 ms 93.7%
  triton_bmm_2497 0.0073 ms 92.1%
  triton_bmm_2498 0.0073 ms 92.1%
  triton_bmm_2499 0.0073 ms 92.1%
SingleProcess AUTOTUNE takes 3.5698 seconds
AUTOTUNE bmm(16x1x96, 16x96x29)
  triton_bmm_2571 0.0065 ms 100.0%
  triton_bmm_2569 0.0067 ms 97.1%
  triton_bmm_2572 0.0071 ms 92.3%
  triton_bmm_2570 0.0073 ms 89.5%
  triton_bmm_2568 0.0078 ms 83.6%
  triton_bmm_2573 0.0086 ms 76.1%
  triton_bmm_2574 0.0086 ms 76.1%
  triton_bmm_2575 0.0086 ms 76.1%
  bmm 0.0102 ms 64.2%
SingleProcess AUTOTUNE takes 2.4258 seconds
AUTOTUNE bmm(16x1x29, 16x29x96)
  triton_bmm_2590 0.0067 ms 100.0%
  triton_bmm_2592 0.0067 ms 100.0%
  triton_bmm_2589 0.0068 ms 99.5%
  triton_bmm_2598 0.0072 ms 93.7%
  triton_bmm_2593 0.0072 ms 93.3%
  triton_bmm_2588 0.0073 ms 91.7%
  triton_bmm_2591 0.0073 ms 91.7%
  triton_bmm_2594 0.0073 ms 91.7%
  triton_bmm_2595 0.0073 ms 91.7%
  bmm 0.0074 ms 90.5%
SingleProcess AUTOTUNE takes 2.8900 seconds
AUTOTUNE bmm(16x1x96, 16x96x30)
  triton_bmm_2661 0.0065 ms 100.0%
  triton_bmm_2662 0.0071 ms 91.0%
  triton_bmm_2660 0.0073 ms 89.4%
  triton_bmm_2663 0.0073 ms 89.0%
  triton_bmm_2659 0.0077 ms 84.6%
  triton_bmm_2665 0.0086 ms 75.7%
  triton_bmm_2666 0.0086 ms 75.7%
  bmm 0.0102 ms 63.8%
  triton_bmm_2664 0.0109 ms 59.4%
SingleProcess AUTOTUNE takes 2.5784 seconds
AUTOTUNE bmm(16x1x30, 16x30x96)
  triton_bmm_2679 0.0067 ms 100.0%
  triton_bmm_2681 0.0067 ms 100.0%
  triton_bmm_2683 0.0067 ms 100.0%
  triton_bmm_2685 0.0067 ms 100.0%
  triton_bmm_2682 0.0072 ms 93.7%
  triton_bmm_2686 0.0072 ms 93.7%
  triton_bmm_2688 0.0072 ms 93.7%
  triton_bmm_2689 0.0072 ms 93.7%
  triton_bmm_2687 0.0075 ms 89.2%
  triton_bmm_2680 0.0076 ms 89.0%
SingleProcess AUTOTUNE takes 3.1555 seconds
AUTOTUNE bmm(16x1x96, 16x96x31)
  triton_bmm_2754 0.0067 ms 100.0%
  triton_bmm_2752 0.0072 ms 93.8%
  triton_bmm_2751 0.0073 ms 92.1%
  triton_bmm_2753 0.0073 ms 92.1%
  triton_bmm_2750 0.0078 ms 85.7%
  triton_bmm_2755 0.0086 ms 78.4%
  triton_bmm_2757 0.0086 ms 78.4%
  triton_bmm_2756 0.0086 ms 78.1%
  bmm 0.0102 ms 66.0%
SingleProcess AUTOTUNE takes 2.5194 seconds
AUTOTUNE bmm(16x1x31, 16x31x96)
  triton_bmm_2770 0.0067 ms 100.0%
  triton_bmm_2772 0.0067 ms 100.0%
  triton_bmm_2773 0.0067 ms 100.0%
  triton_bmm_2774 0.0067 ms 100.0%
  triton_bmm_2777 0.0067 ms 100.0%
  triton_bmm_2778 0.0067 ms 100.0%
  triton_bmm_2776 0.0073 ms 92.5%
  triton_bmm_2775 0.0073 ms 92.1%
  triton_bmm_2771 0.0075 ms 89.6%
  triton_bmm_2779 0.0076 ms 87.9%
SingleProcess AUTOTUNE takes 3.0007 seconds
AUTOTUNE bmm(16x1x96, 16x96x32)
  triton_bmm_2843 0.0065 ms 100.0%
  triton_bmm_2844 0.0065 ms 100.0%
  triton_bmm_2845 0.0073 ms 89.4%
  triton_bmm_2842 0.0073 ms 89.0%
  triton_bmm_2841 0.0083 ms 78.7%
  triton_bmm_2848 0.0085 ms 76.6%
  triton_bmm_2846 0.0085 ms 76.5%
  triton_bmm_2847 0.0090 ms 72.0%
  bmm 0.0099 ms 65.9%
SingleProcess AUTOTUNE takes 2.2873 seconds
AUTOTUNE bmm(16x1x32, 16x32x96)
  triton_bmm_2861 0.0065 ms 100.0%
  triton_bmm_2864 0.0065 ms 100.0%
  triton_bmm_2865 0.0065 ms 100.0%
  triton_bmm_2867 0.0065 ms 100.0%
  triton_bmm_2868 0.0065 ms 100.0%
  triton_bmm_2869 0.0068 ms 95.5%
  triton_bmm_2866 0.0068 ms 94.9%
  triton_bmm_2871 0.0069 ms 93.5%
  triton_bmm_2863 0.0071 ms 91.9%
  triton_bmm_2870 0.0072 ms 90.6%
SingleProcess AUTOTUNE takes 3.1818 seconds
AUTOTUNE bmm(16x1x96, 16x96x33)
  triton_bmm_2936 0.0067 ms 100.0%
  triton_bmm_2937 0.0067 ms 100.0%
  triton_bmm_2935 0.0073 ms 92.3%
  triton_bmm_2934 0.0073 ms 91.7%
  triton_bmm_2933 0.0076 ms 89.0%
  triton_bmm_2932 0.0079 ms 85.4%
  triton_bmm_2940 0.0086 ms 78.4%
  triton_bmm_2939 0.0088 ms 76.6%
  triton_bmm_2938 0.0091 ms 73.6%
  bmm 0.0102 ms 66.0%
SingleProcess AUTOTUNE takes 2.6573 seconds
AUTOTUNE bmm(16x1x33, 16x33x96)
  bmm 0.0068 ms 100.0%
  triton_bmm_2959 0.0071 ms 95.0%
  triton_bmm_2955 0.0072 ms 94.2%
  triton_bmm_2957 0.0072 ms 94.2%
  triton_bmm_2956 0.0072 ms 93.8%
  triton_bmm_2953 0.0076 ms 88.3%
  triton_bmm_2958 0.0078 ms 86.8%
  triton_bmm_2961 0.0078 ms 86.5%
  triton_bmm_2962 0.0080 ms 84.7%
  triton_bmm_2954 0.0080 ms 84.4%
SingleProcess AUTOTUNE takes 3.6855 seconds
AUTOTUNE bmm(16x1x96, 16x96x34)
  triton_bmm_3030 0.0067 ms 100.0%
  triton_bmm_3029 0.0072 ms 93.1%
  triton_bmm_3028 0.0073 ms 92.1%
  triton_bmm_3027 0.0073 ms 91.7%
  triton_bmm_3026 0.0074 ms 90.7%
  triton_bmm_3025 0.0084 ms 79.8%
  triton_bmm_3033 0.0086 ms 78.4%
  triton_bmm_3032 0.0088 ms 76.4%
  triton_bmm_3031 0.0102 ms 65.8%
  bmm 0.0103 ms 65.2%
SingleProcess AUTOTUNE takes 2.9977 seconds
AUTOTUNE bmm(16x1x34, 16x34x96)
  triton_bmm_3048 0.0067 ms 100.0%
  triton_bmm_3052 0.0067 ms 100.0%
  triton_bmm_3049 0.0068 ms 98.1%
  triton_bmm_3050 0.0071 ms 95.0%
  triton_bmm_3051 0.0072 ms 94.0%
  triton_bmm_3047 0.0073 ms 92.1%
  triton_bmm_3046 0.0076 ms 89.0%
  triton_bmm_3056 0.0076 ms 87.9%
  triton_bmm_3054 0.0077 ms 87.7%
  bmm 0.0078 ms 86.4%
SingleProcess AUTOTUNE takes 3.5970 seconds
AUTOTUNE bmm(16x1x96, 16x96x35)
  triton_bmm_3120 0.0067 ms 100.0%
  triton_bmm_3122 0.0067 ms 100.0%
  triton_bmm_3123 0.0068 ms 99.5%
  triton_bmm_3119 0.0069 ms 96.8%
  triton_bmm_3121 0.0073 ms 92.1%
  triton_bmm_3118 0.0085 ms 79.2%
  triton_bmm_3124 0.0086 ms 78.4%
  triton_bmm_3126 0.0092 ms 73.4%
  triton_bmm_3125 0.0094 ms 71.7%
  bmm 0.0103 ms 65.2%
SingleProcess AUTOTUNE takes 3.2030 seconds
AUTOTUNE bmm(16x1x35, 16x35x96)
  triton_bmm_3144 0.0072 ms 100.0%
  triton_bmm_3148 0.0075 ms 95.7%
  bmm 0.0076 ms 94.1%
  triton_bmm_3139 0.0076 ms 94.1%
  triton_bmm_3141 0.0077 ms 93.0%
  triton_bmm_3145 0.0077 ms 93.0%
  triton_bmm_3143 0.0078 ms 92.6%
  triton_bmm_3142 0.0078 ms 92.4%
  triton_bmm_3140 0.0080 ms 90.2%
  triton_bmm_3147 0.0080 ms 90.0%
SingleProcess AUTOTUNE takes 3.3439 seconds
AUTOTUNE bmm(16x1x96, 16x96x36)
  triton_bmm_3213 0.0067 ms 100.0%
  triton_bmm_3216 0.0067 ms 100.0%
  triton_bmm_3214 0.0072 ms 93.3%
  triton_bmm_3215 0.0073 ms 92.1%
  triton_bmm_3212 0.0073 ms 91.5%
  triton_bmm_3211 0.0084 ms 79.5%
  triton_bmm_3219 0.0086 ms 78.4%
  triton_bmm_3217 0.0089 ms 75.5%
  triton_bmm_3218 0.0092 ms 73.2%
  bmm 0.0103 ms 65.2%
SingleProcess AUTOTUNE takes 2.7596 seconds
AUTOTUNE bmm(16x1x36, 16x36x96)
  triton_bmm_3234 0.0067 ms 100.0%
  triton_bmm_3236 0.0067 ms 100.0%
  triton_bmm_3237 0.0071 ms 94.2%
  triton_bmm_3232 0.0072 ms 93.7%
  triton_bmm_3233 0.0073 ms 92.1%
  triton_bmm_3238 0.0073 ms 92.1%
  triton_bmm_3235 0.0074 ms 91.1%
  triton_bmm_3241 0.0074 ms 90.5%
  bmm 0.0075 ms 89.4%
  triton_bmm_3242 0.0076 ms 87.9%
SingleProcess AUTOTUNE takes 3.3422 seconds
AUTOTUNE bmm(16x1x96, 16x96x37)
  triton_bmm_3307 0.0067 ms 100.0%
  triton_bmm_3308 0.0067 ms 100.0%
  triton_bmm_3306 0.0073 ms 92.5%
  triton_bmm_3309 0.0074 ms 90.9%
  triton_bmm_3305 0.0075 ms 89.6%
  triton_bmm_3304 0.0085 ms 78.9%
  triton_bmm_3310 0.0086 ms 78.4%
  triton_bmm_3312 0.0086 ms 78.4%
  triton_bmm_3311 0.0093 ms 72.3%
  bmm 0.0103 ms 65.2%
SingleProcess AUTOTUNE takes 2.5943 seconds
AUTOTUNE bmm(16x1x37, 16x37x96)
  triton_bmm_3331 0.0072 ms 100.0%
  triton_bmm_3330 0.0072 ms 99.1%
  triton_bmm_3333 0.0074 ms 97.0%
  triton_bmm_3326 0.0074 ms 96.6%
  triton_bmm_3325 0.0076 ms 93.7%
  triton_bmm_3336 0.0076 ms 93.7%
  triton_bmm_3329 0.0077 ms 92.6%
  triton_bmm_3327 0.0078 ms 92.2%
  triton_bmm_3332 0.0079 ms 91.1%
  triton_bmm_3328 0.0080 ms 90.1%
SingleProcess AUTOTUNE takes 3.2471 seconds
AUTOTUNE bmm(16x1x96, 16x96x38)
  triton_bmm_3399 0.0067 ms 100.0%
  triton_bmm_3398 0.0069 ms 96.8%
  triton_bmm_3401 0.0072 ms 93.3%
  triton_bmm_3400 0.0073 ms 92.1%
  triton_bmm_3402 0.0073 ms 92.1%
  triton_bmm_3397 0.0084 ms 79.5%
  triton_bmm_3405 0.0090 ms 75.0%
  triton_bmm_3404 0.0093 ms 72.4%
  bmm 0.0103 ms 65.2%
  triton_bmm_3403 0.0107 ms 63.1%
SingleProcess AUTOTUNE takes 3.1135 seconds
AUTOTUNE bmm(16x1x38, 16x38x96)
  triton_bmm_3423 0.0067 ms 100.0%
  triton_bmm_3421 0.0069 ms 95.9%
  triton_bmm_3419 0.0073 ms 91.4%
  triton_bmm_3420 0.0073 ms 91.2%
  triton_bmm_3422 0.0073 ms 91.2%
  triton_bmm_3424 0.0073 ms 91.2%
  triton_bmm_3418 0.0078 ms 85.8%
  triton_bmm_3426 0.0078 ms 85.6%
  triton_bmm_3425 0.0080 ms 83.2%
  triton_bmm_3427 0.0080 ms 83.2%
SingleProcess AUTOTUNE takes 3.4809 seconds
AUTOTUNE bmm(16x1x96, 16x96x39)
  triton_bmm_3492 0.0067 ms 100.0%
  triton_bmm_3493 0.0067 ms 100.0%
  triton_bmm_3495 0.0069 ms 97.7%
  triton_bmm_3491 0.0069 ms 96.8%
  triton_bmm_3494 0.0073 ms 92.1%
  triton_bmm_3490 0.0079 ms 85.0%
  triton_bmm_3496 0.0086 ms 78.4%
  triton_bmm_3498 0.0086 ms 78.4%
  triton_bmm_3497 0.0094 ms 71.7%
  bmm 0.0103 ms 65.2%
SingleProcess AUTOTUNE takes 3.1174 seconds
AUTOTUNE bmm(16x1x39, 16x39x96)
  triton_bmm_3513 0.0072 ms 100.0%
  triton_bmm_3515 0.0072 ms 100.0%
  triton_bmm_3516 0.0072 ms 100.0%
  triton_bmm_3517 0.0072 ms 100.0%
  triton_bmm_3514 0.0074 ms 97.0%
  triton_bmm_3519 0.0079 ms 91.1%
  triton_bmm_3512 0.0080 ms 89.8%
  triton_bmm_3520 0.0082 ms 88.2%
  triton_bmm_3522 0.0082 ms 87.5%
  triton_bmm_3511 0.0083 ms 87.2%
SingleProcess AUTOTUNE takes 3.7148 seconds
AUTOTUNE bmm(16x1x96, 16x96x40)
  triton_bmm_3584 0.0069 ms 100.0%
  triton_bmm_3585 0.0072 ms 96.2%
  triton_bmm_3587 0.0073 ms 95.6%
  triton_bmm_3586 0.0073 ms 95.2%
  triton_bmm_3588 0.0074 ms 94.3%
  triton_bmm_3583 0.0079 ms 88.2%
  triton_bmm_3589 0.0081 ms 85.8%
  triton_bmm_3591 0.0091 ms 76.3%
  triton_bmm_3590 0.0092 ms 75.6%
  bmm 0.0100 ms 69.6%
SingleProcess AUTOTUNE takes 2.6364 seconds
AUTOTUNE bmm(16x1x40, 16x40x96)
  triton_bmm_3606 0.0067 ms 100.0%
  triton_bmm_3609 0.0069 ms 96.8%
  bmm 0.0070 ms 96.3%
  triton_bmm_3608 0.0073 ms 92.5%
  triton_bmm_3610 0.0073 ms 92.5%
  triton_bmm_3605 0.0073 ms 92.1%
  triton_bmm_3613 0.0074 ms 90.5%
  triton_bmm_3607 0.0075 ms 89.6%
  triton_bmm_3612 0.0076 ms 89.0%
  triton_bmm_3614 0.0076 ms 87.9%
SingleProcess AUTOTUNE takes 3.3174 seconds
AUTOTUNE bmm(16x1x96, 16x96x41)
  triton_bmm_3680 0.0067 ms 100.0%
  triton_bmm_3681 0.0068 ms 99.5%
  triton_bmm_3678 0.0072 ms 93.1%
  triton_bmm_3679 0.0073 ms 92.1%
  triton_bmm_3677 0.0074 ms 91.3%
  triton_bmm_3676 0.0079 ms 85.0%
  triton_bmm_3682 0.0085 ms 78.7%
  triton_bmm_3684 0.0086 ms 78.1%
  triton_bmm_3683 0.0088 ms 76.4%
  bmm 0.0104 ms 64.6%
SingleProcess AUTOTUNE takes 2.6935 seconds
AUTOTUNE bmm(16x1x41, 16x41x96)
  triton_bmm_3699 0.0072 ms 100.0%
  triton_bmm_3701 0.0072 ms 100.0%
  triton_bmm_3703 0.0072 ms 100.0%
  triton_bmm_3702 0.0073 ms 99.1%
  triton_bmm_3700 0.0074 ms 97.4%
  bmm 0.0074 ms 97.0%
  triton_bmm_3705 0.0074 ms 97.0%
  triton_bmm_3708 0.0076 ms 94.1%
  triton_bmm_3697 0.0078 ms 92.2%
  triton_bmm_3704 0.0079 ms 91.5%
SingleProcess AUTOTUNE takes 3.6819 seconds
AUTOTUNE bmm(16x1x96, 16x96x42)
  triton_bmm_3771 0.0067 ms 100.0%
  triton_bmm_3773 0.0067 ms 100.0%
  triton_bmm_3774 0.0068 ms 99.5%
  triton_bmm_3772 0.0073 ms 92.5%
  triton_bmm_3770 0.0075 ms 89.4%
  triton_bmm_3769 0.0079 ms 85.4%
  triton_bmm_3776 0.0088 ms 76.4%
  triton_bmm_3777 0.0092 ms 73.4%
  bmm 0.0104 ms 64.6%
  triton_bmm_3775 0.0108 ms 62.5%
SingleProcess AUTOTUNE takes 2.5738 seconds
AUTOTUNE bmm(16x1x42, 16x42x96)
  triton_bmm_3796 0.0067 ms 100.0%
  triton_bmm_3798 0.0072 ms 93.7%
  triton_bmm_3791 0.0073 ms 92.1%
  triton_bmm_3792 0.0073 ms 92.1%
  triton_bmm_3794 0.0073 ms 92.1%
  triton_bmm_3795 0.0073 ms 92.1%
  triton_bmm_3799 0.0074 ms 90.5%
  triton_bmm_3793 0.0075 ms 89.2%
  triton_bmm_3800 0.0076 ms 87.9%
  triton_bmm_3801 0.0076 ms 87.9%
SingleProcess AUTOTUNE takes 4.1904 seconds
AUTOTUNE bmm(16x1x96, 16x96x43)
  triton_bmm_3865 0.0067 ms 100.0%
  triton_bmm_3866 0.0067 ms 100.0%
  triton_bmm_3863 0.0070 ms 96.3%
  triton_bmm_3864 0.0074 ms 90.9%
  triton_bmm_3867 0.0075 ms 89.4%
  triton_bmm_3862 0.0081 ms 83.0%
  triton_bmm_3869 0.0090 ms 74.7%
  triton_bmm_3868 0.0092 ms 73.4%
  triton_bmm_3870 0.0092 ms 72.7%
  bmm 0.0104 ms 64.8%
SingleProcess AUTOTUNE takes 2.9531 seconds
AUTOTUNE bmm(16x1x43, 16x43x96)
  triton_bmm_3889 0.0072 ms 100.0%
  triton_bmm_3886 0.0074 ms 97.0%
  triton_bmm_3891 0.0074 ms 97.0%
  triton_bmm_3894 0.0076 ms 94.1%
  triton_bmm_3883 0.0079 ms 91.5%
  triton_bmm_3887 0.0079 ms 91.5%
  triton_bmm_3888 0.0079 ms 90.7%
  triton_bmm_3885 0.0080 ms 90.0%
  triton_bmm_3890 0.0081 ms 88.9%
  triton_bmm_3884 0.0082 ms 87.9%
SingleProcess AUTOTUNE takes 3.7557 seconds
AUTOTUNE bmm(16x1x96, 16x96x44)
  triton_bmm_3958 0.0067 ms 100.0%
  triton_bmm_3959 0.0067 ms 100.0%
  triton_bmm_3957 0.0072 ms 92.9%
  triton_bmm_3956 0.0075 ms 89.4%
  triton_bmm_3960 0.0075 ms 89.4%
  triton_bmm_3961 0.0084 ms 80.5%
  triton_bmm_3955 0.0086 ms 78.4%
  triton_bmm_3963 0.0092 ms 73.2%
  triton_bmm_3962 0.0094 ms 71.4%
  bmm 0.0104 ms 64.8%
SingleProcess AUTOTUNE takes 3.0510 seconds
AUTOTUNE bmm(16x1x44, 16x44x96)
  triton_bmm_3980 0.0067 ms 100.0%
  triton_bmm_3982 0.0067 ms 100.0%
  triton_bmm_3981 0.0072 ms 93.3%
  triton_bmm_3978 0.0073 ms 92.1%
  triton_bmm_3977 0.0073 ms 91.7%
  triton_bmm_3985 0.0074 ms 90.5%
  bmm 0.0075 ms 89.4%
  triton_bmm_3979 0.0075 ms 89.4%
  triton_bmm_3984 0.0076 ms 88.6%
  triton_bmm_3987 0.0077 ms 87.5%
SingleProcess AUTOTUNE takes 3.7170 seconds
AUTOTUNE bmm(16x1x96, 16x96x45)
  triton_bmm_4051 0.0067 ms 100.0%
  triton_bmm_4050 0.0068 ms 99.5%
  triton_bmm_4052 0.0068 ms 99.5%
  triton_bmm_4053 0.0069 ms 96.8%
  triton_bmm_4049 0.0076 ms 89.0%
  triton_bmm_4054 0.0086 ms 78.4%
  triton_bmm_4048 0.0087 ms 77.5%
  triton_bmm_4056 0.0088 ms 76.4%
  triton_bmm_4055 0.0090 ms 74.5%
  bmm 0.0104 ms 64.6%
SingleProcess AUTOTUNE takes 2.9227 seconds
AUTOTUNE bmm(16x1x45, 16x45x96)
  triton_bmm_4073 0.0072 ms 100.0%
  triton_bmm_4080 0.0076 ms 94.1%
  triton_bmm_4071 0.0078 ms 92.6%
  triton_bmm_4075 0.0078 ms 92.2%
  triton_bmm_4074 0.0079 ms 90.7%
  triton_bmm_4072 0.0080 ms 90.0%
  triton_bmm_4077 0.0080 ms 90.0%
  bmm 0.0081 ms 89.1%
  triton_bmm_4076 0.0081 ms 88.9%
  triton_bmm_4070 0.0082 ms 87.5%
SingleProcess AUTOTUNE takes 3.8980 seconds
AUTOTUNE bmm(16x1x96, 16x96x46)
  triton_bmm_4143 0.0067 ms 100.0%
  triton_bmm_4144 0.0067 ms 100.0%
  triton_bmm_4142 0.0069 ms 96.8%
  triton_bmm_4145 0.0072 ms 92.7%
  triton_bmm_4146 0.0075 ms 89.4%
  triton_bmm_4141 0.0079 ms 85.0%
  triton_bmm_4149 0.0088 ms 76.6%
  triton_bmm_4148 0.0094 ms 71.7%
  bmm 0.0104 ms 64.6%
  triton_bmm_4147 0.0110 ms 61.3%
SingleProcess AUTOTUNE takes 2.7187 seconds
AUTOTUNE bmm(16x1x46, 16x46x96)
  triton_bmm_4163 0.0067 ms 100.0%
  triton_bmm_4166 0.0067 ms 100.0%
  triton_bmm_4167 0.0067 ms 100.0%
  triton_bmm_4170 0.0072 ms 93.7%
  triton_bmm_4162 0.0072 ms 93.3%
  triton_bmm_4164 0.0073 ms 92.1%
  triton_bmm_4168 0.0073 ms 92.1%
  triton_bmm_4165 0.0075 ms 89.4%
  triton_bmm_4173 0.0078 ms 86.4%
  bmm 0.0079 ms 85.0%
SingleProcess AUTOTUNE takes 3.6513 seconds
AUTOTUNE bmm(16x1x96, 16x96x47)
  triton_bmm_4236 0.0073 ms 100.0%
  triton_bmm_4237 0.0073 ms 99.6%
  triton_bmm_4239 0.0075 ms 97.4%
  triton_bmm_4238 0.0075 ms 96.6%
  triton_bmm_4235 0.0076 ms 96.2%
  triton_bmm_4234 0.0087 ms 83.8%
  triton_bmm_4241 0.0088 ms 82.2%
  triton_bmm_4240 0.0091 ms 79.5%
  triton_bmm_4242 0.0092 ms 79.1%
  bmm 0.0104 ms 69.8%
SingleProcess AUTOTUNE takes 2.6841 seconds
AUTOTUNE bmm(16x1x47, 16x47x96)
  triton_bmm_4260 0.0074 ms 100.0%
  triton_bmm_4261 0.0074 ms 100.0%
  triton_bmm_4263 0.0074 ms 99.6%
  triton_bmm_4255 0.0079 ms 93.9%
  triton_bmm_4259 0.0080 ms 92.6%
  triton_bmm_4257 0.0080 ms 92.4%
  triton_bmm_4258 0.0080 ms 92.4%
  triton_bmm_4256 0.0080 ms 92.0%
  triton_bmm_4264 0.0082 ms 89.9%
  triton_bmm_4266 0.0083 ms 89.5%
SingleProcess AUTOTUNE takes 3.4538 seconds
AUTOTUNE bmm(16x1x96, 16x96x48)
  triton_bmm_4329 0.0067 ms 100.0%
  triton_bmm_4330 0.0067 ms 100.0%
  triton_bmm_4331 0.0067 ms 100.0%
  triton_bmm_4328 0.0069 ms 96.8%
  triton_bmm_4332 0.0069 ms 96.8%
  triton_bmm_4327 0.0085 ms 79.2%
  triton_bmm_4335 0.0086 ms 78.4%
  triton_bmm_4333 0.0087 ms 77.5%
  triton_bmm_4334 0.0088 ms 76.4%
  bmm 0.0101 ms 66.7%
SingleProcess AUTOTUNE takes 2.9807 seconds
AUTOTUNE bmm(16x1x48, 16x48x96)
  triton_bmm_4352 0.0067 ms 100.0%
  triton_bmm_4356 0.0067 ms 100.0%
  triton_bmm_4351 0.0069 ms 97.2%
  triton_bmm_4357 0.0069 ms 96.8%
  triton_bmm_4350 0.0072 ms 93.5%
  triton_bmm_4354 0.0073 ms 92.5%
  triton_bmm_4353 0.0073 ms 91.7%
  triton_bmm_4349 0.0074 ms 91.3%
  bmm 0.0075 ms 89.4%
  triton_bmm_4359 0.0076 ms 87.9%
SingleProcess AUTOTUNE takes 3.6562 seconds
AUTOTUNE bmm(16x1x96, 16x96x49)
  triton_bmm_4423 0.0069 ms 100.0%
  triton_bmm_4425 0.0072 ms 96.9%
  triton_bmm_4422 0.0075 ms 92.1%
  triton_bmm_4424 0.0076 ms 91.9%
  triton_bmm_4421 0.0078 ms 89.3%
  triton_bmm_4426 0.0089 ms 78.2%
  triton_bmm_4420 0.0089 ms 77.8%
  triton_bmm_4427 0.0091 ms 76.7%
  triton_bmm_4428 0.0094 ms 73.8%
  bmm 0.0105 ms 66.0%
SingleProcess AUTOTUNE takes 3.4395 seconds
AUTOTUNE bmm(16x1x49, 16x49x96)
  bmm 0.0070 ms 100.0%
  triton_bmm_4450 0.0072 ms 97.3%
  triton_bmm_4446 0.0074 ms 94.4%
  triton_bmm_4447 0.0074 ms 94.0%
  triton_bmm_4449 0.0078 ms 89.7%
  triton_bmm_4442 0.0079 ms 88.6%
  triton_bmm_4445 0.0080 ms 87.2%
  triton_bmm_4443 0.0080 ms 87.0%
  triton_bmm_4444 0.0085 ms 81.6%
  triton_bmm_4441 0.0087 ms 80.4%
SingleProcess AUTOTUNE takes 4.1388 seconds
AUTOTUNE bmm(16x1x96, 16x96x50)
  triton_bmm_4516 0.0069 ms 100.0%
  triton_bmm_4518 0.0071 ms 97.3%
  triton_bmm_4514 0.0072 ms 96.9%
  triton_bmm_4515 0.0076 ms 91.9%
  triton_bmm_4517 0.0078 ms 88.9%
  triton_bmm_4513 0.0087 ms 79.5%
  triton_bmm_4521 0.0094 ms 73.8%
  triton_bmm_4520 0.0096 ms 72.1%
  triton_bmm_4519 0.0100 ms 69.8%
  bmm 0.0104 ms 66.6%
SingleProcess AUTOTUNE takes 3.5024 seconds
AUTOTUNE bmm(16x1x50, 16x50x96)
  triton_bmm_4538 0.0067 ms 100.0%
  triton_bmm_4542 0.0067 ms 100.0%
  triton_bmm_4536 0.0068 ms 99.1%
  triton_bmm_4537 0.0069 ms 96.8%
  triton_bmm_4543 0.0073 ms 92.1%
  triton_bmm_4539 0.0073 ms 92.1%
  triton_bmm_4535 0.0073 ms 91.9%
  triton_bmm_4540 0.0074 ms 91.3%
  bmm 0.0074 ms 90.5%
  triton_bmm_4534 0.0074 ms 90.5%
SingleProcess AUTOTUNE takes 4.1046 seconds
AUTOTUNE bmm(16x1x96, 16x96x51)
  triton_bmm_4609 0.0069 ms 100.0%
  triton_bmm_4611 0.0072 ms 96.9%
  triton_bmm_4607 0.0075 ms 92.3%
  triton_bmm_4610 0.0075 ms 92.1%
  triton_bmm_4608 0.0076 ms 91.6%
  triton_bmm_4606 0.0083 ms 83.5%
  triton_bmm_4614 0.0088 ms 78.6%
  triton_bmm_4612 0.0089 ms 77.8%
  triton_bmm_4613 0.0096 ms 72.1%
  bmm 0.0105 ms 66.0%
SingleProcess AUTOTUNE takes 3.2419 seconds
AUTOTUNE bmm(16x1x51, 16x51x96)
  triton_bmm_4632 0.0073 ms 100.0%
  triton_bmm_4629 0.0074 ms 98.7%
  bmm 0.0077 ms 95.4%
  triton_bmm_4636 0.0078 ms 94.4%
  triton_bmm_4635 0.0078 ms 94.2%
  triton_bmm_4628 0.0079 ms 93.1%
  triton_bmm_4631 0.0080 ms 91.6%
  triton_bmm_4633 0.0080 ms 91.6%
  triton_bmm_4630 0.0081 ms 90.5%
  triton_bmm_4627 0.0086 ms 85.0%
SingleProcess AUTOTUNE takes 3.5210 seconds
AUTOTUNE bmm(16x1x96, 16x96x52)
  triton_bmm_4702 0.0069 ms 100.0%
  triton_bmm_4701 0.0075 ms 92.3%
  triton_bmm_4703 0.0075 ms 92.1%
  triton_bmm_4700 0.0077 ms 90.6%
  triton_bmm_4704 0.0077 ms 89.7%
  triton_bmm_4699 0.0083 ms 83.5%
  triton_bmm_4705 0.0091 ms 76.4%
  triton_bmm_4707 0.0094 ms 74.1%
  triton_bmm_4706 0.0096 ms 72.1%
  bmm 0.0104 ms 66.6%
SingleProcess AUTOTUNE takes 2.9757 seconds
AUTOTUNE bmm(16x1x52, 16x52x96)
  triton_bmm_4722 0.0067 ms 100.0%
  triton_bmm_4725 0.0067 ms 100.0%
  triton_bmm_4726 0.0067 ms 100.0%
  triton_bmm_4729 0.0067 ms 100.0%
  triton_bmm_4721 0.0068 ms 98.1%
  triton_bmm_4724 0.0072 ms 93.3%
  triton_bmm_4728 0.0074 ms 90.3%
  triton_bmm_4723 0.0075 ms 89.4%
  triton_bmm_4727 0.0076 ms 87.9%
  bmm 0.0077 ms 87.7%
SingleProcess AUTOTUNE takes 4.3488 seconds
AUTOTUNE bmm(16x1x96, 16x96x53)
  triton_bmm_4794 0.0069 ms 100.0%
  triton_bmm_4796 0.0069 ms 100.0%
  triton_bmm_4795 0.0075 ms 92.3%
  triton_bmm_4793 0.0077 ms 89.7%
  triton_bmm_4797 0.0078 ms 89.3%
  triton_bmm_4792 0.0083 ms 83.5%
  triton_bmm_4798 0.0089 ms 77.8%
  triton_bmm_4800 0.0094 ms 73.8%
  triton_bmm_4799 0.0096 ms 72.1%
  bmm 0.0105 ms 66.0%
SingleProcess AUTOTUNE takes 2.9276 seconds
AUTOTUNE bmm(16x1x53, 16x53x96)
  triton_bmm_4822 0.0072 ms 100.0%
  triton_bmm_4821 0.0078 ms 92.2%
  triton_bmm_4817 0.0079 ms 91.1%
  triton_bmm_4815 0.0080 ms 90.0%
  triton_bmm_4818 0.0080 ms 89.6%
  triton_bmm_4819 0.0080 ms 89.6%
  triton_bmm_4813 0.0081 ms 88.5%
  bmm 0.0082 ms 87.3%
  triton_bmm_4824 0.0084 ms 85.8%
  triton_bmm_4814 0.0084 ms 84.8%
SingleProcess AUTOTUNE takes 4.2094 seconds
AUTOTUNE bmm(16x1x96, 16x96x54)
  triton_bmm_4888 0.0069 ms 100.0%
  triton_bmm_4886 0.0072 ms 96.9%
  triton_bmm_4889 0.0075 ms 93.1%
  triton_bmm_4887 0.0076 ms 91.9%
  triton_bmm_4890 0.0078 ms 89.3%
  triton_bmm_4885 0.0083 ms 83.5%
  triton_bmm_4893 0.0090 ms 77.0%
  triton_bmm_4892 0.0096 ms 72.1%
  bmm 0.0104 ms 66.6%
  triton_bmm_4891 0.0106 ms 65.8%
SingleProcess AUTOTUNE takes 3.0045 seconds
AUTOTUNE bmm(16x1x54, 16x54x96)
  triton_bmm_4914 0.0067 ms 100.0%
  triton_bmm_4910 0.0068 ms 99.5%
  triton_bmm_4907 0.0069 ms 97.7%
  triton_bmm_4911 0.0073 ms 92.1%
  triton_bmm_4915 0.0073 ms 92.1%
  triton_bmm_4912 0.0074 ms 90.9%
  triton_bmm_4908 0.0075 ms 89.4%
  triton_bmm_4909 0.0076 ms 89.0%
  triton_bmm_4906 0.0079 ms 85.4%
  bmm 0.0079 ms 85.0%
SingleProcess AUTOTUNE takes 4.8092 seconds
AUTOTUNE bmm(16x1x96, 16x96x55)
  triton_bmm_4981 0.0069 ms 100.0%
  triton_bmm_4980 0.0075 ms 93.1%
  triton_bmm_4982 0.0076 ms 91.9%
  triton_bmm_4983 0.0077 ms 89.7%
  triton_bmm_4979 0.0078 ms 89.3%
  triton_bmm_4984 0.0084 ms 83.1%
  triton_bmm_4978 0.0089 ms 77.8%
  triton_bmm_4985 0.0091 ms 76.7%
  triton_bmm_4986 0.0094 ms 73.8%
  bmm 0.0105 ms 66.2%
SingleProcess AUTOTUNE takes 3.0037 seconds
AUTOTUNE bmm(16x1x55, 16x55x96)
  triton_bmm_5007 0.0072 ms 100.0%
  triton_bmm_5004 0.0073 ms 99.1%
  triton_bmm_5005 0.0075 ms 96.6%
  triton_bmm_5008 0.0078 ms 92.6%
  triton_bmm_5000 0.0079 ms 91.5%
  triton_bmm_5002 0.0079 ms 91.1%
  triton_bmm_5001 0.0080 ms 89.6%
  triton_bmm_4999 0.0081 ms 88.9%
  triton_bmm_5003 0.0081 ms 88.9%
  bmm 0.0084 ms 86.2%
SingleProcess AUTOTUNE takes 3.8448 seconds
AUTOTUNE bmm(16x1x96, 16x96x56)
  triton_bmm_5075 0.0069 ms 100.0%
  triton_bmm_5072 0.0072 ms 96.9%
  triton_bmm_5073 0.0074 ms 94.3%
  triton_bmm_5074 0.0075 ms 92.3%
  triton_bmm_5076 0.0077 ms 89.7%
  triton_bmm_5071 0.0082 ms 84.4%
  triton_bmm_5078 0.0090 ms 77.0%
  triton_bmm_5077 0.0091 ms 76.7%
  triton_bmm_5079 0.0094 ms 73.8%
  bmm 0.0101 ms 68.7%
SingleProcess AUTOTUNE takes 2.8028 seconds
AUTOTUNE bmm(16x1x56, 16x56x96)
  triton_bmm_5096 0.0067 ms 100.0%
  triton_bmm_5098 0.0067 ms 100.0%
  triton_bmm_5095 0.0069 ms 96.8%
  triton_bmm_5094 0.0073 ms 91.7%
  triton_bmm_5093 0.0074 ms 90.7%
  triton_bmm_5101 0.0074 ms 90.3%
  triton_bmm_5100 0.0075 ms 89.9%
  triton_bmm_5097 0.0075 ms 89.4%
  bmm 0.0077 ms 86.8%
  triton_bmm_5092 0.0080 ms 84.0%
SingleProcess AUTOTUNE takes 3.7296 seconds
AUTOTUNE bmm(16x1x96, 16x96x57)
  triton_bmm_5166 0.0069 ms 100.0%
  triton_bmm_5167 0.0069 ms 100.0%
  triton_bmm_5168 0.0069 ms 100.0%
  triton_bmm_5169 0.0072 ms 96.9%
  triton_bmm_5165 0.0076 ms 91.9%
  triton_bmm_5164 0.0087 ms 79.5%
  triton_bmm_5170 0.0089 ms 77.8%
  triton_bmm_5172 0.0090 ms 77.0%
  triton_bmm_5171 0.0096 ms 72.1%
  bmm 0.0105 ms 66.2%
SingleProcess AUTOTUNE takes 2.8296 seconds
AUTOTUNE bmm(16x1x57, 16x57x96)
  triton_bmm_5194 0.0072 ms 100.0%
  triton_bmm_5187 0.0074 ms 97.0%
  triton_bmm_5191 0.0074 ms 97.0%
  triton_bmm_5193 0.0078 ms 92.6%
  triton_bmm_5188 0.0079 ms 91.1%
  triton_bmm_5189 0.0080 ms 90.0%
  triton_bmm_5190 0.0080 ms 90.0%
  triton_bmm_5186 0.0084 ms 85.4%
  triton_bmm_5196 0.0085 ms 84.3%
  triton_bmm_5192 0.0086 ms 84.0%
SingleProcess AUTOTUNE takes 3.4657 seconds
AUTOTUNE bmm(16x1x96, 16x96x58)
  triton_bmm_5262 0.0072 ms 100.0%
  triton_bmm_5259 0.0076 ms 94.9%
  triton_bmm_5260 0.0076 ms 94.9%
  triton_bmm_5261 0.0076 ms 94.3%
  triton_bmm_5258 0.0078 ms 92.2%
  triton_bmm_5257 0.0088 ms 81.5%
  triton_bmm_5265 0.0094 ms 76.2%
  triton_bmm_5264 0.0096 ms 74.4%
  bmm 0.0105 ms 68.1%
  triton_bmm_5263 0.0105 ms 68.1%
SingleProcess AUTOTUNE takes 3.0569 seconds
AUTOTUNE bmm(16x1x58, 16x58x96)
  triton_bmm_5282 0.0067 ms 100.0%
  triton_bmm_5283 0.0067 ms 100.0%
  triton_bmm_5284 0.0067 ms 100.0%
  triton_bmm_5286 0.0067 ms 100.0%
  triton_bmm_5287 0.0067 ms 100.0%
  triton_bmm_5280 0.0073 ms 91.7%
  triton_bmm_5279 0.0075 ms 89.7%
  triton_bmm_5281 0.0076 ms 89.0%
  triton_bmm_5278 0.0080 ms 84.0%
  triton_bmm_5285 0.0081 ms 83.3%
SingleProcess AUTOTUNE takes 3.9775 seconds
AUTOTUNE bmm(16x1x96, 16x96x59)
  triton_bmm_5352 0.0069 ms 100.0%
  triton_bmm_5353 0.0069 ms 100.0%
  triton_bmm_5355 0.0072 ms 96.9%
  triton_bmm_5351 0.0077 ms 89.7%
  triton_bmm_5354 0.0077 ms 89.7%
  triton_bmm_5350 0.0089 ms 78.3%
  triton_bmm_5356 0.0089 ms 77.8%
  triton_bmm_5358 0.0090 ms 77.0%
  triton_bmm_5357 0.0096 ms 72.1%
  bmm 0.0105 ms 66.0%
SingleProcess AUTOTUNE takes 2.9863 seconds
AUTOTUNE bmm(16x1x59, 16x59x96)
  triton_bmm_5379 0.0072 ms 100.0%
  triton_bmm_5375 0.0074 ms 97.0%
  triton_bmm_5380 0.0078 ms 92.6%
  triton_bmm_5374 0.0079 ms 90.7%
  triton_bmm_5376 0.0079 ms 90.7%
  triton_bmm_5377 0.0080 ms 89.6%
  triton_bmm_5373 0.0081 ms 88.4%
  triton_bmm_5372 0.0084 ms 85.6%
  triton_bmm_5371 0.0087 ms 83.0%
  triton_bmm_5378 0.0092 ms 78.7%
SingleProcess AUTOTUNE takes 3.4265 seconds
AUTOTUNE bmm(16x1x96, 16x96x60)
  triton_bmm_5445 0.0069 ms 100.0%
  triton_bmm_5446 0.0069 ms 100.0%
  triton_bmm_5447 0.0069 ms 100.0%
  triton_bmm_5444 0.0072 ms 96.9%
  triton_bmm_5448 0.0072 ms 96.9%
  triton_bmm_5443 0.0083 ms 83.5%
  triton_bmm_5449 0.0086 ms 81.0%
  triton_bmm_5451 0.0090 ms 77.0%
  triton_bmm_5450 0.0091 ms 76.7%
  bmm 0.0105 ms 66.0%
SingleProcess AUTOTUNE takes 3.0964 seconds
AUTOTUNE bmm(16x1x60, 16x60x96)
  triton_bmm_5465 0.0067 ms 100.0%
  triton_bmm_5466 0.0067 ms 100.0%
  triton_bmm_5468 0.0067 ms 100.0%
  triton_bmm_5470 0.0067 ms 100.0%
  triton_bmm_5473 0.0067 ms 100.0%
  triton_bmm_5467 0.0069 ms 96.8%
  triton_bmm_5469 0.0073 ms 92.1%
  triton_bmm_5464 0.0074 ms 90.5%
  triton_bmm_5472 0.0075 ms 89.4%
  triton_bmm_5471 0.0076 ms 87.9%
SingleProcess AUTOTUNE takes 3.5663 seconds
AUTOTUNE bmm(16x1x96, 16x96x61)
  triton_bmm_5539 0.0069 ms 100.0%
  triton_bmm_5540 0.0070 ms 99.5%
  triton_bmm_5537 0.0072 ms 96.9%
  triton_bmm_5541 0.0072 ms 96.9%
  triton_bmm_5538 0.0077 ms 90.0%
  triton_bmm_5542 0.0084 ms 83.1%
  triton_bmm_5536 0.0089 ms 77.8%
  triton_bmm_5544 0.0095 ms 73.1%
  triton_bmm_5543 0.0096 ms 72.1%
  bmm 0.0105 ms 66.0%
SingleProcess AUTOTUNE takes 2.9812 seconds
AUTOTUNE bmm(16x1x61, 16x61x96)
  triton_bmm_5565 0.0072 ms 100.0%
  triton_bmm_5566 0.0072 ms 100.0%
  triton_bmm_5562 0.0074 ms 97.4%
  triton_bmm_5559 0.0074 ms 97.0%
  triton_bmm_5563 0.0074 ms 97.0%
  triton_bmm_5560 0.0079 ms 91.1%
  triton_bmm_5561 0.0080 ms 90.2%
  triton_bmm_5558 0.0084 ms 85.2%
  triton_bmm_5557 0.0087 ms 83.0%
  triton_bmm_5568 0.0090 ms 80.4%
SingleProcess AUTOTUNE takes 4.0838 seconds
AUTOTUNE bmm(16x1x96, 16x96x62)
  triton_bmm_5631 0.0069 ms 100.0%
  triton_bmm_5632 0.0069 ms 100.0%
  triton_bmm_5633 0.0074 ms 93.3%
  triton_bmm_5630 0.0075 ms 92.3%
  triton_bmm_5634 0.0077 ms 89.7%
  triton_bmm_5629 0.0089 ms 78.1%
  triton_bmm_5637 0.0094 ms 73.8%
  triton_bmm_5636 0.0096 ms 72.1%
  triton_bmm_5635 0.0100 ms 69.6%
  bmm 0.0105 ms 66.0%
SingleProcess AUTOTUNE takes 3.0944 seconds
AUTOTUNE bmm(16x1x62, 16x62x96)
  triton_bmm_5655 0.0067 ms 100.0%
  triton_bmm_5659 0.0067 ms 100.0%
  triton_bmm_5652 0.0068 ms 99.5%
  triton_bmm_5656 0.0068 ms 99.5%
  triton_bmm_5653 0.0069 ms 96.8%
  triton_bmm_5658 0.0074 ms 90.9%
  triton_bmm_5651 0.0075 ms 89.4%
  triton_bmm_5654 0.0075 ms 89.4%
  triton_bmm_5650 0.0079 ms 85.4%
  triton_bmm_5657 0.0082 ms 81.7%
SingleProcess AUTOTUNE takes 3.8963 seconds
AUTOTUNE bmm(16x1x96, 16x96x63)
  triton_bmm_5724 0.0070 ms 100.0%
  triton_bmm_5726 0.0070 ms 100.0%
  triton_bmm_5723 0.0072 ms 97.3%
  triton_bmm_5725 0.0076 ms 92.0%
  triton_bmm_5727 0.0078 ms 89.7%
  triton_bmm_5728 0.0084 ms 83.2%
  triton_bmm_5722 0.0089 ms 78.7%
  triton_bmm_5729 0.0092 ms 75.4%
  triton_bmm_5730 0.0095 ms 73.2%
  bmm 0.0105 ms 66.3%
SingleProcess AUTOTUNE takes 2.7504 seconds
AUTOTUNE bmm(16x1x63, 16x63x96)
  triton_bmm_5751 0.0072 ms 100.0%
  triton_bmm_5748 0.0074 ms 97.8%
  triton_bmm_5745 0.0074 ms 97.0%
  triton_bmm_5747 0.0074 ms 97.0%
  triton_bmm_5752 0.0078 ms 92.6%
  triton_bmm_5744 0.0079 ms 91.5%
  triton_bmm_5749 0.0080 ms 90.4%
  triton_bmm_5746 0.0080 ms 89.6%
  triton_bmm_5754 0.0085 ms 84.3%
  triton_bmm_5743 0.0087 ms 82.7%
SingleProcess AUTOTUNE takes 3.6198 seconds
AUTOTUNE bmm(16x1x96, 16x96x64)
  triton_bmm_5819 0.0075 ms 100.0%
  triton_bmm_5818 0.0076 ms 99.6%
  triton_bmm_5817 0.0077 ms 97.9%
  triton_bmm_5816 0.0077 ms 97.1%
  triton_bmm_5820 0.0077 ms 97.1%
  triton_bmm_5815 0.0083 ms 90.4%
  triton_bmm_5821 0.0091 ms 83.0%
  triton_bmm_5822 0.0092 ms 81.3%
  triton_bmm_5823 0.0094 ms 79.7%
  bmm 0.0102 ms 73.9%
SingleProcess AUTOTUNE takes 3.0968 seconds
AUTOTUNE bmm(16x1x64, 16x64x96)
  triton_bmm_5840 0.0067 ms 100.0%
  triton_bmm_5841 0.0067 ms 100.0%
  triton_bmm_5842 0.0067 ms 100.0%
  triton_bmm_5839 0.0069 ms 96.8%
  bmm 0.0070 ms 96.3%
  triton_bmm_5844 0.0073 ms 91.7%
  triton_bmm_5845 0.0073 ms 91.7%
  triton_bmm_5838 0.0074 ms 91.3%
  triton_bmm_5836 0.0074 ms 90.5%
  triton_bmm_5837 0.0075 ms 89.4%
SingleProcess AUTOTUNE takes 3.8878 seconds
AUTOTUNE bmm(16x1x96, 16x96x65)
  triton_bmm_5914 0.0070 ms 100.0%
  triton_bmm_5916 0.0072 ms 97.3%
  triton_bmm_5913 0.0074 ms 94.0%
  triton_bmm_5910 0.0076 ms 92.0%
  triton_bmm_5912 0.0077 ms 90.5%
  triton_bmm_5909 0.0080 ms 87.7%
  triton_bmm_5911 0.0082 ms 85.0%
  triton_bmm_5908 0.0083 ms 83.8%
  triton_bmm_5915 0.0088 ms 79.3%
  triton_bmm_5919 0.0088 ms 79.3%
SingleProcess AUTOTUNE takes 4.2412 seconds
AUTOTUNE bmm(16x1x65, 16x65x96)
  bmm 0.0076 ms 100.0%
  triton_bmm_5936 0.0077 ms 99.6%
  triton_bmm_5937 0.0079 ms 96.8%
  triton_bmm_5941 0.0081 ms 94.5%
  triton_bmm_5940 0.0082 ms 93.0%
  triton_bmm_5938 0.0083 ms 92.6%
  triton_bmm_5933 0.0084 ms 90.9%
  triton_bmm_5934 0.0085 ms 90.2%
  triton_bmm_5935 0.0092 ms 83.6%
  triton_bmm_5932 0.0092 ms 83.3%
SingleProcess AUTOTUNE takes 3.9280 seconds
AUTOTUNE bmm(16x1x96, 16x96x66)
  triton_bmm_6008 0.0069 ms 100.0%
  triton_bmm_6006 0.0071 ms 97.3%
  triton_bmm_6012 0.0072 ms 96.9%
  triton_bmm_6005 0.0074 ms 93.9%
  triton_bmm_6010 0.0076 ms 91.8%
  triton_bmm_6009 0.0080 ms 87.1%
  triton_bmm_6007 0.0080 ms 86.3%
  triton_bmm_6004 0.0083 ms 84.1%
  triton_bmm_6011 0.0088 ms 78.9%
  triton_bmm_6015 0.0088 ms 78.9%
SingleProcess AUTOTUNE takes 4.3094 seconds
AUTOTUNE bmm(16x1x66, 16x66x96)
  triton_bmm_6036 0.0070 ms 100.0%
  triton_bmm_6037 0.0071 ms 98.2%
  triton_bmm_6029 0.0072 ms 96.9%
  triton_bmm_6031 0.0072 ms 96.9%
  bmm 0.0074 ms 94.0%
  triton_bmm_6032 0.0075 ms 92.8%
  triton_bmm_6033 0.0075 ms 92.8%
  triton_bmm_6034 0.0075 ms 92.8%
  triton_bmm_6030 0.0078 ms 89.3%
  triton_bmm_6028 0.0080 ms 86.9%
SingleProcess AUTOTUNE takes 4.1841 seconds
AUTOTUNE bmm(16x1x96, 16x96x67)
  triton_bmm_6106 0.0070 ms 100.0%
  triton_bmm_6102 0.0072 ms 97.3%
  triton_bmm_6101 0.0074 ms 94.0%
  triton_bmm_6103 0.0074 ms 94.0%
  triton_bmm_6105 0.0074 ms 94.0%
  triton_bmm_6104 0.0077 ms 90.5%
  triton_bmm_6108 0.0078 ms 89.9%
  triton_bmm_6100 0.0088 ms 79.4%
  triton_bmm_6107 0.0088 ms 79.3%
  triton_bmm_6111 0.0088 ms 79.3%
SingleProcess AUTOTUNE takes 3.6230 seconds
AUTOTUNE bmm(16x1x67, 16x67x96)
  triton_bmm_6129 0.0076 ms 100.0%
  triton_bmm_6132 0.0076 ms 100.0%
  triton_bmm_6128 0.0079 ms 97.2%
  triton_bmm_6130 0.0079 ms 97.2%
  triton_bmm_6126 0.0079 ms 96.8%
  triton_bmm_6133 0.0082 ms 93.0%
  triton_bmm_6125 0.0085 ms 89.8%
  triton_bmm_6127 0.0086 ms 89.2%
  bmm 0.0087 ms 88.0%
  triton_bmm_6124 0.0094 ms 81.6%
SingleProcess AUTOTUNE takes 3.6240 seconds
AUTOTUNE bmm(16x1x96, 16x96x68)
  triton_bmm_6200 0.0069 ms 100.0%
  triton_bmm_6204 0.0072 ms 96.9%
  triton_bmm_6199 0.0074 ms 93.5%
  triton_bmm_6202 0.0076 ms 91.4%
  triton_bmm_6198 0.0077 ms 89.7%
  triton_bmm_6197 0.0080 ms 87.3%
  triton_bmm_6201 0.0080 ms 86.8%
  triton_bmm_6205 0.0085 ms 81.3%
  triton_bmm_6203 0.0087 ms 79.9%
  triton_bmm_6207 0.0088 ms 78.6%
SingleProcess AUTOTUNE takes 3.5736 seconds
AUTOTUNE bmm(16x1x68, 16x68x96)
  triton_bmm_6225 0.0068 ms 100.0%
  triton_bmm_6226 0.0069 ms 98.2%
  triton_bmm_6228 0.0069 ms 98.2%
  bmm 0.0072 ms 94.7%
  triton_bmm_6222 0.0072 ms 94.7%
  triton_bmm_6224 0.0075 ms 90.6%
  triton_bmm_6229 0.0076 ms 90.3%
  triton_bmm_6221 0.0078 ms 87.8%
  triton_bmm_6223 0.0078 ms 87.7%
  triton_bmm_6220 0.0087 ms 78.6%
SingleProcess AUTOTUNE takes 3.8088 seconds
AUTOTUNE bmm(16x1x96, 16x96x69)
  triton_bmm_6294 0.0072 ms 100.0%
  triton_bmm_6300 0.0072 ms 100.0%
  triton_bmm_6297 0.0074 ms 96.6%
  triton_bmm_6298 0.0076 ms 93.7%
  triton_bmm_6296 0.0077 ms 92.9%
  triton_bmm_6293 0.0078 ms 92.2%
  triton_bmm_6295 0.0080 ms 89.4%
  triton_bmm_6292 0.0088 ms 81.8%
  triton_bmm_6301 0.0089 ms 80.3%
  triton_bmm_6299 0.0093 ms 76.7%
SingleProcess AUTOTUNE takes 3.9350 seconds
AUTOTUNE bmm(16x1x69, 16x69x96)
  triton_bmm_6325 0.0076 ms 100.0%
  triton_bmm_6321 0.0076 ms 99.2%
  triton_bmm_6324 0.0076 ms 99.2%
  triton_bmm_6322 0.0079 ms 96.3%
  triton_bmm_6318 0.0080 ms 94.4%
  triton_bmm_6320 0.0085 ms 89.6%
  triton_bmm_6317 0.0086 ms 88.4%
  bmm 0.0087 ms 87.1%
  triton_bmm_6319 0.0094 ms 80.9%
  triton_bmm_6316 0.0094 ms 80.6%
SingleProcess AUTOTUNE takes 3.9948 seconds
AUTOTUNE bmm(16x1x96, 16x96x70)
  triton_bmm_6390 0.0072 ms 100.0%
  triton_bmm_6396 0.0072 ms 100.0%
  triton_bmm_6389 0.0074 ms 97.0%
  triton_bmm_6394 0.0076 ms 94.3%
  triton_bmm_6392 0.0077 ms 92.6%
  triton_bmm_6393 0.0080 ms 89.6%
  triton_bmm_6391 0.0082 ms 87.3%
  triton_bmm_6388 0.0083 ms 86.2%
  triton_bmm_6395 0.0088 ms 81.5%
  triton_bmm_6399 0.0094 ms 76.2%
SingleProcess AUTOTUNE takes 4.1741 seconds
AUTOTUNE bmm(16x1x70, 16x70x96)
  triton_bmm_6417 0.0068 ms 100.0%
  triton_bmm_6416 0.0069 ms 98.6%
  triton_bmm_6418 0.0069 ms 98.6%
  triton_bmm_6421 0.0072 ms 95.5%
  triton_bmm_6413 0.0072 ms 95.1%
  triton_bmm_6415 0.0072 ms 95.1%
  triton_bmm_6414 0.0072 ms 94.7%
  triton_bmm_6420 0.0077 ms 88.4%
  bmm 0.0081 ms 84.6%
  triton_bmm_6412 0.0081 ms 84.6%
SingleProcess AUTOTUNE takes 3.7437 seconds
AUTOTUNE bmm(16x1x96, 16x96x71)
  triton_bmm_6486 0.0072 ms 100.0%
  triton_bmm_6485 0.0074 ms 96.6%
  triton_bmm_6488 0.0075 ms 95.3%
  triton_bmm_6490 0.0076 ms 94.5%
  triton_bmm_6492 0.0078 ms 92.2%
  triton_bmm_6487 0.0079 ms 90.3%
  triton_bmm_6489 0.0080 ms 90.1%
  triton_bmm_6484 0.0083 ms 86.2%
  triton_bmm_6491 0.0088 ms 81.5%
  triton_bmm_6495 0.0089 ms 80.9%
SingleProcess AUTOTUNE takes 3.9073 seconds
AUTOTUNE bmm(16x1x71, 16x71x96)
  triton_bmm_6517 0.0076 ms 100.0%
  triton_bmm_6513 0.0076 ms 99.6%
  triton_bmm_6516 0.0076 ms 99.6%
  triton_bmm_6510 0.0081 ms 94.1%
  triton_bmm_6512 0.0085 ms 89.5%
  triton_bmm_6514 0.0085 ms 89.5%
  triton_bmm_6509 0.0085 ms 89.1%
  triton_bmm_6511 0.0086 ms 88.8%
  bmm 0.0087 ms 87.7%
  triton_bmm_6508 0.0096 ms 79.3%
SingleProcess AUTOTUNE takes 3.7517 seconds
AUTOTUNE bmm(16x1x96, 16x96x72)
  triton_bmm_6584 0.0070 ms 100.0%
  triton_bmm_6586 0.0070 ms 100.0%
  triton_bmm_6588 0.0072 ms 97.3%
  triton_bmm_6581 0.0073 ms 95.2%
  triton_bmm_6585 0.0074 ms 94.0%
  triton_bmm_6582 0.0077 ms 90.8%
  triton_bmm_6583 0.0080 ms 87.2%
  triton_bmm_6580 0.0083 ms 83.8%
  triton_bmm_6587 0.0087 ms 80.1%
  triton_bmm_6591 0.0088 ms 79.0%
SingleProcess AUTOTUNE takes 3.7025 seconds
AUTOTUNE bmm(16x1x72, 16x72x96)
  triton_bmm_6608 0.0069 ms 100.0%
  triton_bmm_6610 0.0069 ms 100.0%
  triton_bmm_6612 0.0071 ms 97.7%
  triton_bmm_6609 0.0072 ms 96.9%
  triton_bmm_6613 0.0072 ms 96.9%
  triton_bmm_6605 0.0073 ms 95.6%
  bmm 0.0078 ms 89.3%
  triton_bmm_6606 0.0078 ms 89.3%
  triton_bmm_6607 0.0078 ms 89.3%
  triton_bmm_6604 0.0085 ms 81.3%
SingleProcess AUTOTUNE takes 3.9720 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:01<00:39,  1.38s/it]running benchmark:   7%|▋         | 2/30 [00:02<00:36,  1.29s/it]running benchmark:  10%|█         | 3/30 [00:03<00:34,  1.27s/it]running benchmark:  13%|█▎        | 4/30 [00:05<00:32,  1.25s/it]running benchmark:  17%|█▋        | 5/30 [00:06<00:30,  1.24s/it]running benchmark:  20%|██        | 6/30 [00:07<00:29,  1.25s/it]running benchmark:  23%|██▎       | 7/30 [00:08<00:28,  1.25s/it]running benchmark:  27%|██▋       | 8/30 [00:10<00:27,  1.25s/it]running benchmark:  30%|███       | 9/30 [00:11<00:25,  1.24s/it]running benchmark:  33%|███▎      | 10/30 [00:12<00:24,  1.24s/it]running benchmark:  37%|███▋      | 11/30 [00:13<00:23,  1.24s/it]running benchmark:  40%|████      | 12/30 [00:14<00:22,  1.24s/it]running benchmark:  43%|████▎     | 13/30 [00:16<00:20,  1.23s/it]running benchmark:  47%|████▋     | 14/30 [00:17<00:19,  1.22s/it]running benchmark:  50%|█████     | 15/30 [00:18<00:18,  1.22s/it]running benchmark:  53%|█████▎    | 16/30 [00:19<00:17,  1.22s/it]running benchmark:  57%|█████▋    | 17/30 [00:21<00:15,  1.22s/it]running benchmark:  60%|██████    | 18/30 [00:22<00:14,  1.22s/it]running benchmark:  63%|██████▎   | 19/30 [00:23<00:13,  1.22s/it]running benchmark:  67%|██████▋   | 20/30 [00:24<00:12,  1.23s/it]running benchmark:  70%|███████   | 21/30 [00:25<00:11,  1.23s/it]running benchmark:  73%|███████▎  | 22/30 [00:27<00:09,  1.24s/it]running benchmark:  77%|███████▋  | 23/30 [00:28<00:08,  1.23s/it]running benchmark:  80%|████████  | 24/30 [00:29<00:07,  1.22s/it]running benchmark:  83%|████████▎ | 25/30 [00:30<00:06,  1.22s/it]running benchmark:  87%|████████▋ | 26/30 [00:32<00:04,  1.21s/it]running benchmark:  90%|█████████ | 27/30 [00:33<00:03,  1.21s/it]running benchmark:  93%|█████████▎| 28/30 [00:34<00:02,  1.21s/it]running benchmark:  97%|█████████▋| 29/30 [00:35<00:01,  1.21s/it]running benchmark: 100%|██████████| 30/30 [00:36<00:00,  1.21s/it]running benchmark: 100%|██████████| 30/30 [00:36<00:00,  1.23s/it]
4.441x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  dcgan                              
AUTOTUNE convolution(256x3x64x64, 64x3x4x4)
  triton_convolution_3 0.1603 ms 100.0%
  triton_convolution_4 0.1661 ms 96.5%
  convolution 0.1886 ms 85.0%
  triton_convolution_5 0.2186 ms 73.3%
  triton_convolution_0 0.2227 ms 72.0%
  triton_convolution_2 0.3602 ms 44.5%
  triton_convolution_1 0.3659 ms 43.8%
SingleProcess AUTOTUNE takes 3.3123 seconds
AUTOTUNE convolution(256x64x32x32, 128x64x4x4)
  convolution 0.0930 ms 100.0%
  triton_convolution_12 0.6637 ms 14.0%
  triton_convolution_6 0.6682 ms 13.9%
  triton_convolution_9 0.7069 ms 13.1%
  triton_convolution_11 0.7440 ms 12.5%
  triton_convolution_7 1.0755 ms 8.6%
  triton_convolution_10 1.1596 ms 8.0%
  triton_convolution_8 2.1753 ms 4.3%
SingleProcess AUTOTUNE takes 4.1132 seconds
AUTOTUNE convolution(256x128x16x16, 256x128x4x4)
  convolution 0.0914 ms 100.0%
  triton_convolution_18 0.6714 ms 13.6%
  triton_convolution_19 0.7383 ms 12.4%
  triton_convolution_16 0.8036 ms 11.4%
  triton_convolution_13 0.9944 ms 9.2%
  triton_convolution_17 1.0665 ms 8.6%
  triton_convolution_14 1.1070 ms 8.3%
  triton_convolution_15 2.4455 ms 3.7%
SingleProcess AUTOTUNE takes 4.6375 seconds
AUTOTUNE convolution(256x256x8x8, 512x256x4x4)
  convolution 0.0988 ms 100.0%
  triton_convolution_25 0.8614 ms 11.5%
  triton_convolution_26 0.9977 ms 9.9%
  triton_convolution_20 1.0444 ms 9.5%
  triton_convolution_21 1.0623 ms 9.3%
  triton_convolution_23 1.2186 ms 8.1%
  triton_convolution_24 1.3911 ms 7.1%
  triton_convolution_22 3.0570 ms 3.2%
SingleProcess AUTOTUNE takes 5.1993 seconds
AUTOTUNE convolution(256x512x4x4, 1x512x4x4)
  convolution 0.0480 ms 100.0%
  triton_convolution_31 0.2601 ms 18.5%
  triton_convolution_27 0.3489 ms 13.8%
  triton_convolution_30 0.3585 ms 13.4%
  triton_convolution_32 0.4403 ms 10.9%
  triton_convolution_28 0.5686 ms 8.4%
  triton_convolution_29 0.8639 ms 5.6%
SingleProcess AUTOTUNE takes 2.5293 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 447.26it/s]
1.385x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
WARNING:root:demucs failed to load
Original Error: "_thnn_fused_lstm_cell_cuda" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/__init__.py", line 31, in forward
    return sources, self.model(mix)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/demucs/model.py", line 209, in forward
    x = self.lstm(x)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/demucs/model.py", line 27, in forward
    x = self.lstm(x)[0]
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/rnn.py", line 878, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
RuntimeError: "_thnn_fused_lstm_cell_cuda" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  densenet121                        
AUTOTUNE convolution(64x3x224x224, 64x3x7x7)
  convolution 0.8473 ms 100.0%
  triton_convolution_3 1.3908 ms 60.9%
  triton_convolution_4 1.5175 ms 55.8%
  triton_convolution_5 1.6251 ms 52.1%
  triton_convolution_0 1.8661 ms 45.4%
  triton_convolution_2 2.0439 ms 41.5%
  triton_convolution_1 5.7787 ms 14.7%
SingleProcess AUTOTUNE takes 3.2812 seconds
AUTOTUNE mm(200704x64, 64x128)
  triton_mm_8 0.0596 ms 100.0%
  triton_mm_14 0.0599 ms 99.5%
  triton_mm_7 0.0609 ms 98.0%
  triton_mm_10 0.0636 ms 93.7%
  triton_mm_9 0.0653 ms 91.3%
  mm 0.0675 ms 88.4%
  triton_mm_6 0.0724 ms 82.4%
  triton_mm_13 0.0730 ms 81.7%
  triton_mm_15 0.0862 ms 69.2%
  triton_mm_16 0.0955 ms 62.4%
SingleProcess AUTOTUNE takes 4.2840 seconds
AUTOTUNE convolution(64x128x56x56, 32x128x3x3)
  convolution 0.1365 ms 100.0%
  triton_convolution_18 0.5232 ms 26.1%
  triton_convolution_24 0.5472 ms 24.9%
  triton_convolution_21 0.5523 ms 24.7%
  triton_convolution_22 0.5815 ms 23.5%
  triton_convolution_23 0.6086 ms 22.4%
  triton_convolution_19 1.3348 ms 10.2%
  triton_convolution_20 1.9017 ms 7.2%
SingleProcess AUTOTUNE takes 3.3900 seconds
AUTOTUNE mm(200704x96, 96x128)
  triton_mm_29 0.0729 ms 100.0%
  triton_mm_28 0.0736 ms 99.0%
  triton_mm_26 0.0741 ms 98.4%
  triton_mm_27 0.0744 ms 98.0%
  triton_mm_32 0.0781 ms 93.2%
  triton_mm_25 0.0797 ms 91.4%
  mm 0.0866 ms 84.1%
  triton_mm_33 0.0916 ms 79.5%
  triton_mm_31 0.1234 ms 59.1%
  triton_mm_35 0.1266 ms 57.6%
SingleProcess AUTOTUNE takes 4.5017 seconds
AUTOTUNE mm(200704x128, 128x128)
  triton_mm_46 0.0797 ms 100.0%
  triton_mm_45 0.0804 ms 99.2%
  triton_mm_47 0.0870 ms 91.7%
  triton_mm_48 0.0873 ms 91.4%
  triton_mm_44 0.0878 ms 90.8%
  triton_mm_51 0.0885 ms 90.1%
  triton_mm_52 0.0907 ms 87.9%
  mm 0.0921 ms 86.6%
  triton_mm_54 0.1338 ms 59.6%
  triton_mm_53 0.1386 ms 57.5%
SingleProcess AUTOTUNE takes 4.6011 seconds
AUTOTUNE mm(200704x160, 160x128)
  mm 0.0954 ms 100.0%
  triton_mm_65 0.0959 ms 99.5%
  triton_mm_64 0.0995 ms 95.9%
  triton_mm_70 0.1025 ms 93.1%
  triton_mm_67 0.1049 ms 91.0%
  triton_mm_66 0.1058 ms 90.1%
  triton_mm_63 0.1130 ms 84.4%
  triton_mm_71 0.1176 ms 81.1%
  triton_mm_68 0.1708 ms 55.8%
  triton_mm_69 0.1770 ms 53.9%
SingleProcess AUTOTUNE takes 4.5151 seconds
AUTOTUNE mm(200704x192, 192x128)
  mm 0.0995 ms 100.0%
  triton_mm_84 0.1020 ms 97.6%
  triton_mm_83 0.1032 ms 96.4%
  triton_mm_86 0.1105 ms 90.0%
  triton_mm_85 0.1132 ms 87.9%
  triton_mm_89 0.1166 ms 85.3%
  triton_mm_90 0.1188 ms 83.7%
  triton_mm_82 0.1195 ms 83.2%
  triton_mm_88 0.1965 ms 50.6%
  triton_mm_92 0.1971 ms 50.5%
SingleProcess AUTOTUNE takes 4.5181 seconds
AUTOTUNE mm(200704x224, 224x128)
  mm 0.1118 ms 100.0%
  triton_mm_102 0.1147 ms 97.5%
  triton_mm_103 0.1160 ms 96.4%
  triton_mm_104 0.1237 ms 90.4%
  triton_mm_105 0.1237 ms 90.4%
  triton_mm_101 0.1269 ms 88.1%
  triton_mm_108 0.1301 ms 86.0%
  triton_mm_109 0.1399 ms 79.9%
  triton_mm_111 0.1898 ms 58.9%
  triton_mm_107 0.2046 ms 54.7%
SingleProcess AUTOTUNE takes 4.6756 seconds
AUTOTUNE mm(200704x256, 256x128)
  mm 0.1172 ms 100.0%
  triton_mm_122 0.1212 ms 96.7%
  triton_mm_121 0.1273 ms 92.1%
  triton_mm_124 0.1283 ms 91.4%
  triton_mm_123 0.1336 ms 87.7%
  triton_mm_127 0.1367 ms 85.8%
  triton_mm_128 0.1458 ms 80.4%
  triton_mm_120 0.1533 ms 76.5%
  triton_mm_126 0.2417 ms 48.5%
  triton_mm_125 0.2420 ms 48.4%
SingleProcess AUTOTUNE takes 4.4438 seconds
AUTOTUNE mm(50176x128, 128x128)
  triton_mm_133 0.0284 ms 100.0%
  triton_mm_134 0.0284 ms 99.9%
  triton_mm_139 0.0286 ms 99.4%
  triton_mm_132 0.0302 ms 94.0%
  triton_mm_140 0.0307 ms 92.5%
  triton_mm_135 0.0312 ms 91.2%
  triton_mm_136 0.0317 ms 89.5%
  mm 0.0321 ms 88.4%
  triton_mm_141 0.0426 ms 66.8%
  triton_mm_142 0.0427 ms 66.6%
SingleProcess AUTOTUNE takes 5.0219 seconds
AUTOTUNE convolution(64x128x28x28, 32x128x3x3)
  convolution 0.0430 ms 100.0%
  triton_convolution_150 0.1428 ms 30.1%
  triton_convolution_144 0.1473 ms 29.2%
  triton_convolution_149 0.1669 ms 25.7%
  triton_convolution_147 0.1865 ms 23.0%
  triton_convolution_148 0.1971 ms 21.8%
  triton_convolution_145 0.3875 ms 11.1%
  triton_convolution_146 0.4718 ms 9.1%
SingleProcess AUTOTUNE takes 3.1374 seconds
AUTOTUNE mm(50176x160, 160x128)
  triton_mm_158 0.0334 ms 100.0%
  triton_mm_153 0.0335 ms 99.7%
  triton_mm_152 0.0349 ms 95.6%
  triton_mm_154 0.0363 ms 92.0%
  mm 0.0364 ms 91.9%
  triton_mm_155 0.0374 ms 89.3%
  triton_mm_151 0.0376 ms 88.9%
  triton_mm_159 0.0389 ms 85.8%
  triton_mm_156 0.0497 ms 67.2%
  triton_mm_157 0.0512 ms 65.3%
SingleProcess AUTOTUNE takes 4.2793 seconds
AUTOTUNE mm(50176x192, 192x128)
  triton_mm_172 0.0358 ms 100.0%
  triton_mm_171 0.0363 ms 98.7%
  triton_mm_177 0.0364 ms 98.2%
  mm 0.0380 ms 94.4%
  triton_mm_173 0.0386 ms 92.8%
  triton_mm_178 0.0394 ms 90.9%
  triton_mm_174 0.0394 ms 90.8%
  triton_mm_170 0.0406 ms 88.1%
  triton_mm_176 0.0565 ms 63.4%
  triton_mm_175 0.0567 ms 63.1%
SingleProcess AUTOTUNE takes 4.3338 seconds
AUTOTUNE mm(50176x224, 224x128)
  triton_mm_190 0.0396 ms 100.0%
  triton_mm_196 0.0400 ms 99.0%
  triton_mm_191 0.0403 ms 98.3%
  triton_mm_192 0.0426 ms 93.0%
  mm 0.0427 ms 92.8%
  triton_mm_193 0.0438 ms 90.4%
  triton_mm_197 0.0447 ms 88.6%
  triton_mm_189 0.0449 ms 88.3%
  triton_mm_195 0.0578 ms 68.5%
  triton_mm_194 0.0595 ms 66.6%
SingleProcess AUTOTUNE takes 4.4624 seconds
AUTOTUNE mm(50176x256, 256x128)
  triton_mm_210 0.0417 ms 100.0%
  triton_mm_215 0.0428 ms 97.2%
  triton_mm_209 0.0433 ms 96.2%
  mm 0.0440 ms 94.7%
  triton_mm_211 0.0444 ms 93.9%
  triton_mm_212 0.0448 ms 93.1%
  triton_mm_216 0.0460 ms 90.5%
  triton_mm_208 0.0494 ms 84.4%
  triton_mm_213 0.0669 ms 62.3%
  triton_mm_214 0.0677 ms 61.5%
SingleProcess AUTOTUNE takes 4.4046 seconds
AUTOTUNE mm(50176x288, 288x128)
  triton_mm_234 0.0476 ms 100.0%
  triton_mm_229 0.0477 ms 99.7%
  mm 0.0481 ms 98.8%
  triton_mm_228 0.0495 ms 96.0%
  triton_mm_231 0.0506 ms 93.9%
  triton_mm_230 0.0507 ms 93.8%
  triton_mm_235 0.0540 ms 88.0%
  triton_mm_227 0.0576 ms 82.5%
  triton_mm_232 0.0705 ms 67.5%
  triton_mm_233 0.0748 ms 63.6%
SingleProcess AUTOTUNE takes 4.4019 seconds
AUTOTUNE mm(50176x320, 320x128)
  triton_mm_248 0.0487 ms 100.0%
  mm 0.0492 ms 99.0%
  triton_mm_247 0.0505 ms 96.4%
  triton_mm_253 0.0511 ms 95.2%
  triton_mm_250 0.0518 ms 93.9%
  triton_mm_249 0.0523 ms 93.0%
  triton_mm_254 0.0536 ms 90.8%
  triton_mm_246 0.0590 ms 82.5%
  triton_mm_251 0.0794 ms 61.3%
  triton_mm_252 0.0810 ms 60.1%
SingleProcess AUTOTUNE takes 4.9470 seconds
AUTOTUNE mm(50176x352, 352x128)
  triton_mm_267 0.0529 ms 100.0%
  mm 0.0531 ms 99.6%
  triton_mm_269 0.0548 ms 96.6%
  triton_mm_266 0.0552 ms 95.8%
  triton_mm_272 0.0564 ms 93.7%
  triton_mm_268 0.0569 ms 93.1%
  triton_mm_273 0.0609 ms 86.8%
  triton_mm_265 0.0639 ms 82.8%
  triton_mm_270 0.0805 ms 65.7%
  triton_mm_271 0.0875 ms 60.5%
SingleProcess AUTOTUNE takes 4.8467 seconds
AUTOTUNE mm(50176x384, 384x128)
  triton_mm_286 0.0539 ms 100.0%
  mm 0.0544 ms 99.1%
  triton_mm_288 0.0568 ms 94.8%
  triton_mm_285 0.0572 ms 94.2%
  triton_mm_287 0.0583 ms 92.3%
  triton_mm_292 0.0599 ms 90.0%
  triton_mm_291 0.0606 ms 88.8%
  triton_mm_284 0.0683 ms 78.9%
  triton_mm_289 0.0885 ms 60.9%
  triton_mm_290 0.0924 ms 58.3%
SingleProcess AUTOTUNE takes 4.4336 seconds
AUTOTUNE mm(50176x416, 416x128)
  mm 0.0577 ms 100.0%
  triton_mm_305 0.0580 ms 99.4%
  triton_mm_304 0.0593 ms 97.3%
  triton_mm_306 0.0604 ms 95.4%
  triton_mm_307 0.0604 ms 95.4%
  triton_mm_310 0.0643 ms 89.7%
  triton_mm_311 0.0649 ms 88.9%
  triton_mm_303 0.0723 ms 79.7%
  triton_mm_308 0.0902 ms 63.9%
  triton_mm_309 0.0929 ms 62.1%
SingleProcess AUTOTUNE takes 4.7068 seconds
AUTOTUNE mm(50176x448, 448x128)
  mm 0.0584 ms 100.0%
  triton_mm_324 0.0611 ms 95.7%
  triton_mm_326 0.0625 ms 93.5%
  triton_mm_323 0.0664 ms 88.0%
  triton_mm_325 0.0674 ms 86.7%
  triton_mm_329 0.0693 ms 84.3%
  triton_mm_330 0.0728 ms 80.2%
  triton_mm_322 0.0795 ms 73.5%
  triton_mm_327 0.0986 ms 59.3%
  triton_mm_328 0.1103 ms 53.0%
SingleProcess AUTOTUNE takes 4.6345 seconds
AUTOTUNE mm(50176x480, 480x128)
  mm 0.0624 ms 100.0%
  triton_mm_343 0.0633 ms 98.5%
  triton_mm_345 0.0666 ms 93.7%
  triton_mm_342 0.0675 ms 92.4%
  triton_mm_344 0.0698 ms 89.3%
  triton_mm_348 0.0726 ms 86.0%
  triton_mm_349 0.0745 ms 83.8%
  triton_mm_341 0.0826 ms 75.5%
  triton_mm_346 0.1013 ms 61.6%
  triton_mm_347 0.1121 ms 55.7%
SingleProcess AUTOTUNE takes 4.7538 seconds
AUTOTUNE mm(50176x512, 512x256)
  mm 0.0831 ms 100.0%
  triton_mm_362 0.0872 ms 95.3%
  triton_mm_361 0.0884 ms 94.0%
  triton_mm_364 0.0995 ms 83.5%
  triton_mm_363 0.1004 ms 82.8%
  triton_mm_368 0.1134 ms 73.3%
  triton_mm_360 0.1140 ms 72.9%
  triton_mm_367 0.1150 ms 72.2%
  triton_mm_370 0.1857 ms 44.7%
  triton_mm_366 0.2048 ms 40.6%
SingleProcess AUTOTUNE takes 5.1004 seconds
AUTOTUNE mm(12544x256, 256x128)
  triton_mm_374 0.0152 ms 100.0%
  triton_mm_376 0.0154 ms 98.3%
  triton_mm_375 0.0165 ms 92.0%
  triton_mm_373 0.0174 ms 87.0%
  mm 0.0181 ms 83.6%
  triton_mm_372 0.0184 ms 82.6%
  triton_mm_380 0.0188 ms 80.7%
  triton_mm_379 0.0193 ms 78.5%
  triton_mm_377 0.0237 ms 63.9%
  triton_mm_378 0.0240 ms 63.2%
SingleProcess AUTOTUNE takes 4.9249 seconds
AUTOTUNE convolution(64x128x14x14, 32x128x3x3)
  convolution 0.0204 ms 100.0%
  triton_convolution_389 0.0523 ms 38.9%
  triton_convolution_388 0.0606 ms 33.6%
  triton_convolution_387 0.0625 ms 32.6%
  triton_convolution_384 0.0653 ms 31.2%
  triton_convolution_390 0.0893 ms 22.8%
  triton_convolution_385 0.1994 ms 10.2%
  triton_convolution_386 0.4472 ms 4.6%
SingleProcess AUTOTUNE takes 2.9648 seconds
AUTOTUNE mm(12544x288, 288x128)
  triton_mm_393 0.0169 ms 100.0%
  triton_mm_395 0.0171 ms 99.1%
  mm 0.0178 ms 95.0%
  triton_mm_394 0.0184 ms 92.0%
  triton_mm_392 0.0189 ms 89.2%
  triton_mm_391 0.0209 ms 80.9%
  triton_mm_399 0.0214 ms 78.9%
  triton_mm_398 0.0215 ms 78.5%
  triton_mm_396 0.0248 ms 68.0%
  triton_mm_397 0.0250 ms 67.5%
SingleProcess AUTOTUNE takes 4.6274 seconds
AUTOTUNE mm(12544x320, 320x128)
  mm 0.0173 ms 100.0%
  triton_mm_414 0.0175 ms 98.9%
  triton_mm_412 0.0176 ms 98.4%
  triton_mm_411 0.0185 ms 93.6%
  triton_mm_413 0.0190 ms 91.2%
  triton_mm_410 0.0207 ms 83.6%
  triton_mm_418 0.0212 ms 81.5%
  triton_mm_417 0.0227 ms 76.4%
  triton_mm_416 0.0272 ms 63.7%
  triton_mm_415 0.0272 ms 63.6%
SingleProcess AUTOTUNE takes 4.7839 seconds
AUTOTUNE mm(12544x352, 352x128)
  triton_mm_433 0.0183 ms 100.0%
  mm 0.0184 ms 99.5%
  triton_mm_431 0.0188 ms 97.6%
  triton_mm_432 0.0198 ms 92.7%
  triton_mm_430 0.0204 ms 90.0%
  triton_mm_429 0.0228 ms 80.5%
  triton_mm_437 0.0231 ms 79.4%
  triton_mm_436 0.0244 ms 75.1%
  triton_mm_434 0.0272 ms 67.3%
  triton_mm_435 0.0283 ms 64.8%
SingleProcess AUTOTUNE takes 4.6255 seconds
AUTOTUNE mm(12544x384, 384x128)
  mm 0.0191 ms 100.0%
  triton_mm_452 0.0192 ms 99.5%
  triton_mm_450 0.0195 ms 97.9%
  triton_mm_449 0.0205 ms 93.1%
  triton_mm_451 0.0205 ms 93.0%
  triton_mm_456 0.0229 ms 83.4%
  triton_mm_448 0.0241 ms 79.3%
  triton_mm_455 0.0263 ms 72.7%
  triton_mm_453 0.0293 ms 65.2%
  triton_mm_454 0.0300 ms 63.8%
SingleProcess AUTOTUNE takes 4.6399 seconds
AUTOTUNE mm(12544x416, 416x128)
  triton_mm_469 0.0198 ms 100.0%
  mm 0.0199 ms 99.8%
  triton_mm_471 0.0200 ms 99.4%
  triton_mm_470 0.0210 ms 94.4%
  triton_mm_468 0.0215 ms 92.4%
  triton_mm_475 0.0250 ms 79.3%
  triton_mm_467 0.0251 ms 79.2%
  triton_mm_474 0.0280 ms 70.7%
  triton_mm_472 0.0299 ms 66.3%
  triton_mm_473 0.0305 ms 65.1%
SingleProcess AUTOTUNE takes 4.6803 seconds
AUTOTUNE mm(12544x448, 448x128)
  mm 0.0202 ms 100.0%
  triton_mm_490 0.0210 ms 95.9%
  triton_mm_488 0.0214 ms 94.0%
  triton_mm_489 0.0237 ms 84.9%
  triton_mm_487 0.0240 ms 84.1%
  triton_mm_486 0.0277 ms 72.8%
  triton_mm_494 0.0279 ms 72.2%
  triton_mm_493 0.0299 ms 67.4%
  triton_mm_491 0.0325 ms 61.9%
  triton_mm_492 0.0353 ms 57.2%
SingleProcess AUTOTUNE takes 4.5974 seconds
AUTOTUNE mm(12544x480, 480x128)
  mm 0.0216 ms 100.0%
  triton_mm_507 0.0226 ms 95.5%
  triton_mm_509 0.0229 ms 94.4%
  triton_mm_506 0.0238 ms 90.6%
  triton_mm_508 0.0244 ms 88.5%
  triton_mm_505 0.0281 ms 77.0%
  triton_mm_513 0.0285 ms 75.8%
  triton_mm_512 0.0311 ms 69.4%
  triton_mm_510 0.0336 ms 64.3%
  triton_mm_511 0.0357 ms 60.4%
SingleProcess AUTOTUNE takes 4.8257 seconds
AUTOTUNE mm(12544x512, 512x128)
  mm 0.0219 ms 100.0%
  triton_mm_526 0.0223 ms 98.1%
  triton_mm_528 0.0229 ms 95.8%
  triton_mm_527 0.0251 ms 87.3%
  triton_mm_525 0.0252 ms 87.2%
  triton_mm_532 0.0292 ms 75.0%
  triton_mm_524 0.0292 ms 74.9%
  triton_mm_531 0.0323 ms 67.9%
  triton_mm_529 0.0347 ms 63.2%
  triton_mm_530 0.0367 ms 59.7%
SingleProcess AUTOTUNE takes 4.7786 seconds
AUTOTUNE mm(12544x544, 544x128)
  mm 0.0231 ms 100.0%
  triton_mm_545 0.0243 ms 95.0%
  triton_mm_547 0.0244 ms 94.6%
  triton_mm_544 0.0246 ms 93.9%
  triton_mm_546 0.0252 ms 91.6%
  triton_mm_551 0.0300 ms 77.0%
  triton_mm_543 0.0307 ms 75.3%
  triton_mm_550 0.0342 ms 67.5%
  triton_mm_548 0.0361 ms 64.1%
  triton_mm_549 0.0370 ms 62.5%
SingleProcess AUTOTUNE takes 4.5776 seconds
AUTOTUNE mm(12544x576, 576x128)
  mm 0.0233 ms 100.0%
  triton_mm_564 0.0254 ms 91.8%
  triton_mm_565 0.0256 ms 91.1%
  triton_mm_563 0.0256 ms 91.0%
  triton_mm_566 0.0260 ms 89.8%
  triton_mm_570 0.0292 ms 79.9%
  triton_mm_562 0.0326 ms 71.4%
  triton_mm_569 0.0359 ms 64.9%
  triton_mm_567 0.0387 ms 60.2%
  triton_mm_568 0.0391 ms 59.5%
SingleProcess AUTOTUNE takes 4.6826 seconds
AUTOTUNE mm(12544x608, 608x128)
  mm 0.0254 ms 100.0%
  triton_mm_582 0.0257 ms 98.8%
  triton_mm_584 0.0261 ms 97.1%
  triton_mm_583 0.0264 ms 96.2%
  triton_mm_585 0.0265 ms 95.9%
  triton_mm_589 0.0318 ms 79.9%
  triton_mm_581 0.0342 ms 74.3%
  triton_mm_588 0.0369 ms 68.8%
  triton_mm_586 0.0389 ms 65.2%
  triton_mm_587 0.0391 ms 64.9%
SingleProcess AUTOTUNE takes 6.0720 seconds
AUTOTUNE mm(12544x640, 640x128)
  mm 0.0247 ms 100.0%
  triton_mm_601 0.0273 ms 90.7%
  triton_mm_603 0.0276 ms 89.6%
  triton_mm_602 0.0279 ms 88.7%
  triton_mm_604 0.0279 ms 88.7%
  triton_mm_608 0.0310 ms 79.9%
  triton_mm_600 0.0348 ms 71.0%
  triton_mm_607 0.0390 ms 63.4%
  triton_mm_605 0.0427 ms 57.9%
  triton_mm_606 0.0440 ms 56.3%
SingleProcess AUTOTUNE takes 5.1268 seconds
AUTOTUNE mm(12544x672, 672x128)
  mm 0.0270 ms 100.0%
  triton_mm_622 0.0275 ms 98.3%
  triton_mm_620 0.0277 ms 97.6%
  triton_mm_623 0.0285 ms 94.9%
  triton_mm_621 0.0286 ms 94.6%
  triton_mm_627 0.0346 ms 78.2%
  triton_mm_619 0.0365 ms 74.1%
  triton_mm_626 0.0398 ms 68.0%
  triton_mm_625 0.0422 ms 64.0%
  triton_mm_624 0.0425 ms 63.6%
SingleProcess AUTOTUNE takes 4.7579 seconds
AUTOTUNE mm(12544x704, 704x128)
  mm 0.0262 ms 100.0%
  triton_mm_641 0.0281 ms 93.2%
  triton_mm_639 0.0282 ms 92.7%
  triton_mm_642 0.0296 ms 88.4%
  triton_mm_640 0.0303 ms 86.5%
  triton_mm_646 0.0328 ms 79.8%
  triton_mm_638 0.0382 ms 68.5%
  triton_mm_645 0.0422 ms 62.0%
  triton_mm_643 0.0444 ms 59.0%
  triton_mm_644 0.0447 ms 58.5%
SingleProcess AUTOTUNE takes 4.7864 seconds
AUTOTUNE mm(12544x736, 736x128)
  mm 0.0282 ms 100.0%
  triton_mm_660 0.0292 ms 96.7%
  triton_mm_658 0.0296 ms 95.2%
  triton_mm_659 0.0313 ms 90.2%
  triton_mm_661 0.0313 ms 90.1%
  triton_mm_665 0.0347 ms 81.2%
  triton_mm_657 0.0404 ms 69.8%
  triton_mm_664 0.0430 ms 65.6%
  triton_mm_663 0.0454 ms 62.1%
  triton_mm_662 0.0457 ms 61.7%
SingleProcess AUTOTUNE takes 5.0247 seconds
AUTOTUNE mm(12544x768, 768x128)
  mm 0.0279 ms 100.0%
  triton_mm_679 0.0303 ms 92.1%
  triton_mm_677 0.0303 ms 92.0%
  triton_mm_678 0.0313 ms 89.1%
  triton_mm_680 0.0313 ms 89.1%
  triton_mm_684 0.0351 ms 79.3%
  triton_mm_676 0.0417 ms 66.8%
  triton_mm_683 0.0449 ms 62.1%
  triton_mm_681 0.0485 ms 57.4%
  triton_mm_682 0.0488 ms 57.2%
SingleProcess AUTOTUNE takes 5.2967 seconds
AUTOTUNE mm(12544x800, 800x128)
  mm 0.0303 ms 100.0%
  triton_mm_696 0.0312 ms 97.4%
  triton_mm_698 0.0315 ms 96.4%
  triton_mm_697 0.0329 ms 92.3%
  triton_mm_699 0.0334 ms 90.7%
  triton_mm_703 0.0363 ms 83.7%
  triton_mm_695 0.0422 ms 71.9%
  triton_mm_702 0.0467 ms 64.9%
  triton_mm_701 0.0481 ms 63.1%
  triton_mm_700 0.0483 ms 62.8%
SingleProcess AUTOTUNE takes 5.4724 seconds
AUTOTUNE mm(12544x832, 832x128)
  mm 0.0301 ms 100.0%
  triton_mm_715 0.0322 ms 93.5%
  triton_mm_717 0.0323 ms 93.1%
  triton_mm_716 0.0341 ms 88.2%
  triton_mm_718 0.0343 ms 87.7%
  triton_mm_722 0.0378 ms 79.6%
  triton_mm_714 0.0444 ms 67.7%
  triton_mm_721 0.0483 ms 62.3%
  triton_mm_719 0.0508 ms 59.3%
  triton_mm_720 0.0508 ms 59.3%
SingleProcess AUTOTUNE takes 5.4741 seconds
AUTOTUNE mm(12544x864, 864x128)
  mm 0.0317 ms 100.0%
  triton_mm_734 0.0332 ms 95.6%
  triton_mm_736 0.0335 ms 94.7%
  triton_mm_737 0.0350 ms 90.8%
  triton_mm_735 0.0350 ms 90.7%
  triton_mm_741 0.0387 ms 82.0%
  triton_mm_733 0.0454 ms 70.0%
  triton_mm_740 0.0493 ms 64.4%
  triton_mm_739 0.0507 ms 62.6%
  triton_mm_738 0.0511 ms 62.2%
SingleProcess AUTOTUNE takes 4.9929 seconds
AUTOTUNE mm(12544x896, 896x128)
  mm 0.0314 ms 100.0%
  triton_mm_753 0.0343 ms 91.4%
  triton_mm_755 0.0346 ms 90.7%
  triton_mm_756 0.0368 ms 85.2%
  triton_mm_754 0.0372 ms 84.4%
  triton_mm_760 0.0399 ms 78.7%
  triton_mm_752 0.0481 ms 65.3%
  triton_mm_759 0.0518 ms 60.6%
  triton_mm_757 0.0536 ms 58.5%
  triton_mm_758 0.0540 ms 58.2%
SingleProcess AUTOTUNE takes 5.1769 seconds
AUTOTUNE mm(12544x928, 928x128)
  mm 0.0344 ms 100.0%
  triton_mm_774 0.0356 ms 96.6%
  triton_mm_772 0.0367 ms 93.6%
  triton_mm_773 0.0373 ms 92.1%
  triton_mm_775 0.0379 ms 90.8%
  triton_mm_779 0.0405 ms 84.9%
  triton_mm_771 0.0484 ms 71.0%
  triton_mm_778 0.0525 ms 65.4%
  triton_mm_777 0.0544 ms 63.2%
  triton_mm_776 0.0548 ms 62.7%
SingleProcess AUTOTUNE takes 5.1769 seconds
AUTOTUNE mm(12544x960, 960x128)
  mm 0.0339 ms 100.0%
  triton_mm_791 0.0366 ms 92.5%
  triton_mm_793 0.0367 ms 92.3%
  triton_mm_794 0.0380 ms 89.2%
  triton_mm_792 0.0386 ms 87.8%
  triton_mm_798 0.0419 ms 81.0%
  triton_mm_797 0.0450 ms 75.3%
  triton_mm_790 0.0508 ms 66.7%
  triton_mm_796 0.0589 ms 57.6%
  triton_mm_795 0.0591 ms 57.3%
SingleProcess AUTOTUNE takes 5.4680 seconds
AUTOTUNE mm(12544x992, 992x128)
  mm 0.0361 ms 100.0%
  triton_mm_810 0.0371 ms 97.4%
  triton_mm_812 0.0374 ms 96.6%
  triton_mm_813 0.0397 ms 90.8%
  triton_mm_811 0.0399 ms 90.4%
  triton_mm_817 0.0430 ms 83.9%
  triton_mm_809 0.0503 ms 71.7%
  triton_mm_816 0.0513 ms 70.4%
  triton_mm_815 0.0566 ms 63.7%
  triton_mm_814 0.0571 ms 63.3%
SingleProcess AUTOTUNE takes 4.9234 seconds
AUTOTUNE mm(12544x1024, 1024x512)
  mm 0.0681 ms 100.0%
  triton_mm_830 0.0800 ms 85.2%
  triton_mm_829 0.0803 ms 84.9%
  triton_mm_832 0.0933 ms 73.0%
  triton_mm_831 0.0937 ms 72.7%
  triton_mm_836 0.1049 ms 65.0%
  triton_mm_828 0.1122 ms 60.7%
  triton_mm_835 0.1371 ms 49.7%
  triton_mm_838 0.1758 ms 38.7%
  triton_mm_837 0.1891 ms 36.0%
SingleProcess AUTOTUNE takes 4.6989 seconds
AUTOTUNE mm(3136x512, 512x128)
  triton_mm_848 0.0128 ms 100.0%
  triton_mm_844 0.0134 ms 95.2%
  mm 0.0135 ms 94.5%
  triton_mm_843 0.0137 ms 92.9%
  triton_mm_845 0.0143 ms 89.2%
  triton_mm_841 0.0155 ms 82.6%
  triton_mm_842 0.0155 ms 82.4%
  triton_mm_846 0.0157 ms 81.3%
  triton_mm_849 0.0161 ms 79.3%
  triton_mm_840 0.0216 ms 59.0%
SingleProcess AUTOTUNE takes 4.5279 seconds
AUTOTUNE convolution(64x128x7x7, 32x128x3x3)
  convolution 0.0141 ms 100.0%
  triton_convolution_857 0.0453 ms 31.1%
  triton_convolution_856 0.0566 ms 24.9%
  triton_convolution_855 0.0581 ms 24.2%
  triton_convolution_852 0.0624 ms 22.6%
  triton_convolution_858 0.0796 ms 17.7%
  triton_convolution_853 0.1968 ms 7.2%
  triton_convolution_854 0.4091 ms 3.4%
SingleProcess AUTOTUNE takes 2.8305 seconds
AUTOTUNE mm(3136x544, 544x128)
  triton_mm_867 0.0136 ms 100.0%
  mm 0.0137 ms 99.3%
  triton_mm_864 0.0144 ms 94.5%
  triton_mm_862 0.0147 ms 92.8%
  triton_mm_863 0.0147 ms 92.8%
  triton_mm_865 0.0153 ms 89.3%
  triton_mm_860 0.0159 ms 85.7%
  triton_mm_861 0.0164 ms 82.9%
  triton_mm_868 0.0173 ms 78.6%
  triton_mm_859 0.0223 ms 61.1%
SingleProcess AUTOTUNE takes 4.7391 seconds
AUTOTUNE mm(3136x576, 576x128)
  mm 0.0130 ms 100.0%
  triton_mm_886 0.0131 ms 99.0%
  triton_mm_882 0.0144 ms 90.2%
  triton_mm_881 0.0148 ms 87.9%
  triton_mm_883 0.0153 ms 84.9%
  triton_mm_884 0.0159 ms 81.5%
  triton_mm_879 0.0167 ms 78.0%
  triton_mm_887 0.0170 ms 76.5%
  triton_mm_880 0.0172 ms 75.5%
  triton_mm_878 0.0238 ms 54.6%
SingleProcess AUTOTUNE takes 5.5299 seconds
AUTOTUNE mm(3136x608, 608x128)
  triton_mm_905 0.0138 ms 100.0%
  mm 0.0139 ms 99.1%
  triton_mm_900 0.0148 ms 92.9%
  triton_mm_901 0.0149 ms 92.3%
  triton_mm_902 0.0156 ms 88.7%
  triton_mm_903 0.0157 ms 88.0%
  triton_mm_898 0.0169 ms 81.5%
  triton_mm_899 0.0170 ms 81.3%
  triton_mm_906 0.0172 ms 80.0%
  triton_mm_897 0.0247 ms 55.9%
SingleProcess AUTOTUNE takes 4.8497 seconds
AUTOTUNE mm(3136x640, 640x128)
  mm 0.0139 ms 100.0%
  triton_mm_924 0.0143 ms 97.5%
  triton_mm_919 0.0152 ms 91.6%
  triton_mm_920 0.0157 ms 88.6%
  triton_mm_921 0.0157 ms 88.4%
  triton_mm_922 0.0170 ms 82.1%
  triton_mm_925 0.0174 ms 80.1%
  triton_mm_917 0.0178 ms 78.4%
  triton_mm_918 0.0180 ms 77.4%
  triton_mm_916 0.0253 ms 54.9%
SingleProcess AUTOTUNE takes 4.8502 seconds
AUTOTUNE mm(3136x672, 672x128)
  mm 0.0141 ms 100.0%
  triton_mm_943 0.0144 ms 98.0%
  triton_mm_939 0.0157 ms 90.0%
  triton_mm_941 0.0160 ms 88.4%
  triton_mm_938 0.0161 ms 87.8%
  triton_mm_940 0.0164 ms 86.3%
  triton_mm_936 0.0180 ms 78.5%
  triton_mm_937 0.0181 ms 78.1%
  triton_mm_944 0.0181 ms 78.1%
  triton_mm_935 0.0261 ms 54.0%
SingleProcess AUTOTUNE takes 4.8252 seconds
AUTOTUNE mm(3136x704, 704x128)
  mm 0.0136 ms 100.0%
  triton_mm_962 0.0148 ms 91.8%
  triton_mm_957 0.0161 ms 84.3%
  triton_mm_958 0.0163 ms 83.5%
  triton_mm_959 0.0163 ms 83.5%
  triton_mm_960 0.0169 ms 80.3%
  triton_mm_963 0.0180 ms 75.2%
  triton_mm_955 0.0191 ms 71.0%
  triton_mm_956 0.0192 ms 70.5%
  triton_mm_954 0.0270 ms 50.3%
SingleProcess AUTOTUNE takes 5.2368 seconds
AUTOTUNE mm(3136x736, 736x128)
  mm 0.0140 ms 100.0%
  triton_mm_981 0.0150 ms 92.8%
  triton_mm_976 0.0164 ms 85.2%
  triton_mm_978 0.0165 ms 84.5%
  triton_mm_979 0.0165 ms 84.5%
  triton_mm_977 0.0171 ms 81.8%
  triton_mm_982 0.0183 ms 76.4%
  triton_mm_974 0.0190 ms 73.3%
  triton_mm_975 0.0192 ms 72.8%
  triton_mm_973 0.0285 ms 49.0%
SingleProcess AUTOTUNE takes 5.3047 seconds
AUTOTUNE mm(3136x768, 768x128)
  mm 0.0147 ms 100.0%
  triton_mm_1000 0.0149 ms 98.3%
  triton_mm_995 0.0164 ms 89.3%
  triton_mm_996 0.0166 ms 88.4%
  triton_mm_998 0.0172 ms 85.3%
  triton_mm_997 0.0175 ms 83.7%
  triton_mm_1001 0.0182 ms 80.6%
  triton_mm_994 0.0200 ms 73.4%
  triton_mm_993 0.0203 ms 72.4%
  triton_mm_992 0.0289 ms 50.7%
SingleProcess AUTOTUNE takes 5.3167 seconds
AUTOTUNE mm(3136x800, 800x128)
  mm 0.0150 ms 100.0%
  triton_mm_1019 0.0157 ms 95.5%
  triton_mm_1017 0.0172 ms 87.0%
  triton_mm_1015 0.0173 ms 86.7%
  triton_mm_1014 0.0176 ms 85.1%
  triton_mm_1016 0.0178 ms 84.2%
  triton_mm_1020 0.0197 ms 76.1%
  triton_mm_1012 0.0200 ms 75.0%
  triton_mm_1013 0.0207 ms 72.5%
  triton_mm_1011 0.0300 ms 49.9%
SingleProcess AUTOTUNE takes 4.9878 seconds
AUTOTUNE mm(3136x832, 832x128)
  mm 0.0144 ms 100.0%
  triton_mm_1038 0.0160 ms 90.0%
  triton_mm_1033 0.0172 ms 84.1%
  triton_mm_1034 0.0173 ms 83.2%
  triton_mm_1035 0.0183 ms 79.0%
  triton_mm_1036 0.0184 ms 78.4%
  triton_mm_1039 0.0195 ms 74.2%
  triton_mm_1032 0.0211 ms 68.5%
  triton_mm_1031 0.0212 ms 68.0%
  triton_mm_1030 0.0310 ms 46.5%
SingleProcess AUTOTUNE takes 5.0022 seconds
AUTOTUNE mm(3136x864, 864x128)
  mm 0.0152 ms 100.0%
  triton_mm_1057 0.0163 ms 93.3%
  triton_mm_1052 0.0179 ms 85.3%
  triton_mm_1055 0.0179 ms 85.0%
  triton_mm_1054 0.0185 ms 82.5%
  triton_mm_1053 0.0185 ms 82.4%
  triton_mm_1058 0.0188 ms 80.8%
  triton_mm_1051 0.0212 ms 71.8%
  triton_mm_1050 0.0216 ms 70.6%
  triton_mm_1049 0.0319 ms 47.7%
SingleProcess AUTOTUNE takes 4.8989 seconds
AUTOTUNE mm(3136x896, 896x128)
  mm 0.0149 ms 100.0%
  triton_mm_1076 0.0163 ms 91.4%
  triton_mm_1071 0.0180 ms 82.6%
  triton_mm_1073 0.0186 ms 80.0%
  triton_mm_1072 0.0187 ms 79.5%
  triton_mm_1074 0.0188 ms 79.2%
  triton_mm_1077 0.0202 ms 73.6%
  triton_mm_1069 0.0223 ms 66.7%
  triton_mm_1070 0.0226 ms 65.9%
  triton_mm_1068 0.0328 ms 45.4%
SingleProcess AUTOTUNE takes 5.2162 seconds
AUTOTUNE mm(3136x928, 928x128)
  mm 0.0150 ms 100.0%
  triton_mm_1095 0.0170 ms 88.5%
  triton_mm_1093 0.0188 ms 80.2%
  triton_mm_1090 0.0191 ms 78.6%
  triton_mm_1092 0.0193 ms 77.8%
  triton_mm_1091 0.0194 ms 77.6%
  triton_mm_1096 0.0214 ms 70.1%
  triton_mm_1088 0.0221 ms 68.0%
  triton_mm_1089 0.0229 ms 65.6%
  triton_mm_1087 0.0339 ms 44.3%
SingleProcess AUTOTUNE takes 5.2831 seconds
AUTOTUNE mm(3136x960, 960x128)
  mm 0.0149 ms 100.0%
  triton_mm_1114 0.0174 ms 86.0%
  triton_mm_1110 0.0190 ms 78.6%
  triton_mm_1109 0.0191 ms 78.1%
  triton_mm_1111 0.0195 ms 76.8%
  triton_mm_1112 0.0197 ms 75.9%
  triton_mm_1115 0.0204 ms 73.4%
  triton_mm_1108 0.0233 ms 64.1%
  triton_mm_1107 0.0234 ms 63.8%
  triton_mm_1106 0.0355 ms 42.1%
SingleProcess AUTOTUNE takes 5.4282 seconds
AUTOTUNE mm(3136x992, 992x128)
  mm 0.0154 ms 100.0%
  triton_mm_1133 0.0181 ms 85.0%
  triton_mm_1128 0.0194 ms 79.5%
  triton_mm_1130 0.0194 ms 79.4%
  triton_mm_1131 0.0198 ms 77.7%
  triton_mm_1129 0.0201 ms 76.7%
  triton_mm_1134 0.0204 ms 75.5%
  triton_mm_1126 0.0236 ms 65.2%
  triton_mm_1127 0.0239 ms 64.5%
  triton_mm_1125 0.0359 ms 42.9%
SingleProcess AUTOTUNE takes 4.8074 seconds
AUTOTUNE int_mm(64x1024, 1024x1000, 64x1000)
  triton_mm_1154 0.0137 ms 100.0%
  triton_mm_1149 0.0147 ms 93.7%
  triton_mm_1150 0.0150 ms 91.3%
  triton_mm_1152 0.0156 ms 87.9%
  triton_mm_1153 0.0175 ms 78.4%
  triton_mm_1148 0.0183 ms 75.1%
  triton_mm_1147 0.0208 ms 66.1%
  triton_mm_1146 0.0215 ms 63.8%
  triton_mm_1145 0.0244 ms 56.3%
  triton_mm_1144 0.0307 ms 44.8%
SingleProcess AUTOTUNE takes 5.2357 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 24.44it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 29.13it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 30.63it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 31.32it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 31.71it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 31.91it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 31.91it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 31.30it/s]
2.116x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:09, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_dc5 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:09, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:08, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_dc5 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
loading model: 0it [00:05, ?it/s]
WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
cuda eval  detectron2_fcos_r_50_fpn           
WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
AUTOTUNE convolution(1x3x800x1216, 64x3x7x7)
  convolution 0.1096 ms 100.0%
  triton_convolution_3 0.4403 ms 24.9%
  triton_convolution_4 0.4759 ms 23.0%
  triton_convolution_5 0.5220 ms 21.0%
  triton_convolution_0 0.5633 ms 19.5%
  triton_convolution_2 0.6979 ms 15.7%
  triton_convolution_1 0.9244 ms 11.9%
SingleProcess AUTOTUNE takes 3.0850 seconds
AUTOTUNE mm(60800x64, 64x64)
  triton_mm_14 0.0178 ms 100.0%
  triton_mm_8 0.0188 ms 94.6%
  triton_mm_7 0.0199 ms 89.4%
  triton_mm_10 0.0200 ms 89.0%
  triton_mm_6 0.0202 ms 88.3%
  mm 0.0208 ms 85.8%
  triton_mm_13 0.0213 ms 83.6%
  triton_mm_9 0.0216 ms 82.6%
  triton_mm_17 0.0232 ms 76.9%
  triton_mm_15 0.0246 ms 72.3%
SingleProcess AUTOTUNE takes 3.8900 seconds
AUTOTUNE convolution(1x64x200x304, 64x64x3x3)
  convolution 0.0465 ms 100.0%
  triton_convolution_23 0.1775 ms 26.2%
  triton_convolution_18 0.1802 ms 25.8%
  triton_convolution_24 0.1822 ms 25.5%
  triton_convolution_21 0.2594 ms 17.9%
  triton_convolution_22 0.2645 ms 17.6%
  triton_convolution_19 0.3300 ms 14.1%
  triton_convolution_20 0.7245 ms 6.4%
SingleProcess AUTOTUNE takes 3.8359 seconds
AUTOTUNE mm(60800x64, 64x256)
  triton_mm_27 0.0348 ms 100.0%
  triton_mm_26 0.0370 ms 94.1%
  triton_mm_33 0.0396 ms 87.8%
  triton_mm_29 0.0417 ms 83.4%
  triton_mm_28 0.0441 ms 78.9%
  mm 0.0442 ms 78.8%
  triton_mm_25 0.0449 ms 77.4%
  triton_mm_35 0.0501 ms 69.4%
  triton_mm_32 0.0502 ms 69.3%
  triton_mm_36 0.0605 ms 57.5%
SingleProcess AUTOTUNE takes 4.3904 seconds
AUTOTUNE mm(60800x256, 256x64)
  mm 0.0418 ms 100.0%
  triton_mm_51 0.0419 ms 99.8%
  triton_mm_52 0.0420 ms 99.4%
  triton_mm_50 0.0428 ms 97.5%
  triton_mm_53 0.0433 ms 96.3%
  triton_mm_57 0.0442 ms 94.6%
  triton_mm_56 0.0447 ms 93.3%
  triton_mm_49 0.0452 ms 92.3%
  triton_mm_54 0.0483 ms 86.5%
  triton_mm_55 0.0519 ms 80.4%
SingleProcess AUTOTUNE takes 4.1299 seconds
AUTOTUNE convolution(1x256x200x304, 128x256x1x1)
  convolution 0.0188 ms 100.0%
  triton_convolution_114 0.0449 ms 41.9%
  triton_convolution_111 0.0450 ms 41.8%
  triton_convolution_116 0.0482 ms 39.1%
  triton_convolution_117 0.0486 ms 38.7%
  triton_convolution_115 0.0490 ms 38.4%
  triton_convolution_112 0.0918 ms 20.5%
  triton_convolution_113 0.2257 ms 8.3%
SingleProcess AUTOTUNE takes 4.2510 seconds
AUTOTUNE convolution(1x128x100x152, 128x128x3x3)
  convolution 0.0369 ms 100.0%
  triton_convolution_123 0.2085 ms 17.7%
  triton_convolution_124 0.2116 ms 17.4%
  triton_convolution_121 0.2374 ms 15.5%
  triton_convolution_118 0.2435 ms 15.2%
  triton_convolution_122 0.2648 ms 13.9%
  triton_convolution_119 0.3404 ms 10.8%
  triton_convolution_120 0.9404 ms 3.9%
SingleProcess AUTOTUNE takes 4.1195 seconds
AUTOTUNE mm(15200x128, 128x512)
  triton_mm_127 0.0258 ms 100.0%
  triton_mm_126 0.0263 ms 98.1%
  mm 0.0282 ms 91.5%
  triton_mm_125 0.0287 ms 89.8%
  triton_mm_129 0.0287 ms 89.8%
  triton_mm_132 0.0306 ms 84.3%
  triton_mm_128 0.0311 ms 82.8%
  triton_mm_133 0.0324 ms 79.6%
  triton_mm_135 0.0378 ms 68.1%
  triton_mm_131 0.0497 ms 51.9%
SingleProcess AUTOTUNE takes 4.4884 seconds
AUTOTUNE convolution(1x256x200x304, 512x256x1x1)
  convolution 0.0374 ms 100.0%
  triton_convolution_140 0.0981 ms 38.2%
  triton_convolution_142 0.1099 ms 34.1%
  triton_convolution_143 0.1129 ms 33.2%
  triton_convolution_141 0.1413 ms 26.5%
  triton_convolution_137 0.2011 ms 18.6%
  triton_convolution_138 0.2257 ms 16.6%
  triton_convolution_139 0.5538 ms 6.8%
SingleProcess AUTOTUNE takes 4.4241 seconds
AUTOTUNE convolution(1x512x100x152, 128x512x1x1)
  convolution 0.0312 ms 100.0%
  triton_convolution_145 0.0601 ms 52.0%
  triton_convolution_148 0.0743 ms 42.0%
  triton_convolution_150 0.0806 ms 38.7%
  triton_convolution_149 0.0832 ms 37.5%
  triton_convolution_147 0.0839 ms 37.2%
  triton_convolution_144 0.0876 ms 35.7%
  conv1x1_via_mm 0.1204 ms 26.0%
  triton_convolution_146 0.1460 ms 21.4%
SingleProcess AUTOTUNE takes 4.4115 seconds
AUTOTUNE convolution(1x128x100x152, 128x128x3x3)
  convolution 0.0570 ms 100.0%
  triton_convolution_151 0.1208 ms 47.2%
  triton_convolution_152 0.1289 ms 44.3%
  triton_convolution_157 0.1463 ms 39.0%
  triton_convolution_155 0.1472 ms 38.7%
  triton_convolution_154 0.1801 ms 31.7%
  triton_convolution_156 0.2167 ms 26.3%
  triton_convolution_153 0.2970 ms 19.2%
SingleProcess AUTOTUNE takes 5.0182 seconds
AUTOTUNE convolution(1x128x100x152, 512x128x1x1)
  convolution 0.0300 ms 100.0%
  triton_convolution_159 0.0570 ms 52.7%
  triton_convolution_164 0.0647 ms 46.5%
  triton_convolution_161 0.0661 ms 45.5%
  triton_convolution_163 0.0683 ms 44.0%
  triton_convolution_162 0.0712 ms 42.2%
  triton_convolution_158 0.0913 ms 32.9%
  triton_convolution_160 0.1011 ms 29.7%
  conv1x1_via_mm 0.1322 ms 22.7%
SingleProcess AUTOTUNE takes 5.2125 seconds
AUTOTUNE convolution(1x512x100x152, 256x512x1x1)
  triton_convolution_213 0.0459 ms 100.0%
  triton_convolution_211 0.0481 ms 95.3%
  triton_convolution_210 0.0484 ms 94.8%
  triton_convolution_212 0.0490 ms 93.6%
  convolution 0.0499 ms 91.8%
  triton_convolution_208 0.0595 ms 77.0%
  triton_convolution_209 0.0915 ms 50.1%
  triton_convolution_207 0.1211 ms 37.9%
SingleProcess AUTOTUNE takes 5.9003 seconds
AUTOTUNE convolution(1x256x50x76, 256x256x3x3)
  convolution 0.0610 ms 100.0%
  triton_convolution_220 0.1458 ms 41.9%
  triton_convolution_217 0.1712 ms 35.6%
  triton_convolution_218 0.1917 ms 31.8%
  triton_convolution_215 0.2276 ms 26.8%
  triton_convolution_219 0.2602 ms 23.5%
  triton_convolution_216 0.2920 ms 20.9%
  triton_convolution_214 0.4487 ms 13.6%
SingleProcess AUTOTUNE takes 5.9349 seconds
AUTOTUNE convolution(1x256x50x76, 1024x256x1x1)
  convolution 0.0252 ms 100.0%
  triton_convolution_224 0.0612 ms 41.2%
  triton_convolution_227 0.0620 ms 40.7%
  triton_convolution_225 0.0639 ms 39.5%
  triton_convolution_222 0.0661 ms 38.2%
  triton_convolution_226 0.0724 ms 34.9%
  triton_convolution_223 0.1043 ms 24.2%
  triton_convolution_221 0.1447 ms 17.5%
  conv1x1_via_mm 0.2422 ms 10.4%
SingleProcess AUTOTUNE takes 6.5859 seconds
AUTOTUNE convolution(1x512x100x152, 1024x512x1x1)
  convolution 0.0730 ms 100.0%
  triton_convolution_234 0.1162 ms 62.8%
  triton_convolution_231 0.1184 ms 61.7%
  triton_convolution_229 0.1210 ms 60.4%
  triton_convolution_232 0.1227 ms 59.5%
  triton_convolution_233 0.1249 ms 58.5%
  triton_convolution_228 0.2159 ms 33.8%
  triton_convolution_230 0.2184 ms 33.4%
SingleProcess AUTOTUNE takes 6.2017 seconds
AUTOTUNE convolution(1x1024x50x76, 256x1024x1x1)
  convolution 0.0239 ms 100.0%
  triton_convolution_241 0.0747 ms 32.0%
  triton_convolution_238 0.0781 ms 30.5%
  triton_convolution_240 0.0809 ms 29.5%
  triton_convolution_239 0.0811 ms 29.4%
  triton_convolution_236 0.0978 ms 24.4%
  conv1x1_via_mm 0.1447 ms 16.5%
  triton_convolution_237 0.1568 ms 15.2%
  triton_convolution_235 0.2140 ms 11.2%
SingleProcess AUTOTUNE takes 5.3572 seconds
AUTOTUNE convolution(1x256x50x76, 256x256x3x3)
  convolution 0.0611 ms 100.0%
  triton_convolution_248 0.1487 ms 41.1%
  triton_convolution_245 0.1657 ms 36.9%
  triton_convolution_246 0.1846 ms 33.1%
  triton_convolution_243 0.2063 ms 29.6%
  triton_convolution_247 0.2456 ms 24.9%
  triton_convolution_244 0.3024 ms 20.2%
  triton_convolution_242 0.4104 ms 14.9%
SingleProcess AUTOTUNE takes 5.1790 seconds
AUTOTUNE convolution(1x256x50x76, 1024x256x1x1)
  convolution 0.0248 ms 100.0%
  triton_convolution_252 0.0615 ms 40.3%
  triton_convolution_255 0.0624 ms 39.7%
  triton_convolution_250 0.0636 ms 39.0%
  triton_convolution_253 0.0659 ms 37.6%
  triton_convolution_254 0.0702 ms 35.3%
  triton_convolution_249 0.1049 ms 23.6%
  triton_convolution_251 0.1123 ms 22.1%
  conv1x1_via_mm 0.1431 ms 17.3%
SingleProcess AUTOTUNE takes 5.2577 seconds
AUTOTUNE convolution(1x1024x50x76, 512x1024x1x1)
  convolution 0.0366 ms 100.0%
  triton_convolution_344 0.0500 ms 73.2%
  triton_convolution_346 0.0766 ms 47.8%
  triton_convolution_343 0.0812 ms 45.1%
  triton_convolution_345 0.0831 ms 44.1%
  triton_convolution_341 0.1058 ms 34.6%
  triton_convolution_342 0.1639 ms 22.4%
  triton_convolution_340 0.2222 ms 16.5%
SingleProcess AUTOTUNE takes 5.7714 seconds
AUTOTUNE convolution(1x512x25x38, 512x512x3x3)
  convolution 0.0708 ms 100.0%
  triton_convolution_351 0.2675 ms 26.5%
  triton_convolution_353 0.2794 ms 25.4%
  triton_convolution_350 0.3225 ms 22.0%
  triton_convolution_348 0.4244 ms 16.7%
  triton_convolution_352 0.4608 ms 15.4%
  triton_convolution_349 0.5585 ms 12.7%
  triton_convolution_347 0.8222 ms 8.6%
SingleProcess AUTOTUNE takes 6.2130 seconds
AUTOTUNE convolution(1x512x25x38, 2048x512x1x1)
  convolution 0.0354 ms 100.0%
  triton_convolution_355 0.0666 ms 53.1%
  triton_convolution_358 0.0699 ms 50.6%
  triton_convolution_360 0.0770 ms 45.9%
  triton_convolution_357 0.0807 ms 43.8%
  triton_convolution_359 0.0822 ms 43.0%
  triton_convolution_356 0.1364 ms 25.9%
  conv1x1_via_mm 0.1439 ms 24.6%
  triton_convolution_354 0.1583 ms 22.3%
SingleProcess AUTOTUNE takes 6.1792 seconds
AUTOTUNE convolution(1x1024x50x76, 2048x1024x1x1)
  convolution 0.0601 ms 100.0%
  triton_convolution_362 0.1249 ms 48.1%
  triton_convolution_365 0.1401 ms 42.9%
  triton_convolution_367 0.1459 ms 41.2%
  triton_convolution_366 0.1468 ms 40.9%
  triton_convolution_364 0.1542 ms 38.9%
  triton_convolution_361 0.2301 ms 26.1%
  triton_convolution_363 0.3012 ms 19.9%
SingleProcess AUTOTUNE takes 6.1327 seconds
AUTOTUNE convolution(1x2048x25x38, 512x2048x1x1)
  convolution 0.0390 ms 100.0%
  triton_convolution_372 0.0869 ms 44.9%
  triton_convolution_374 0.1370 ms 28.5%
  conv1x1_via_mm 0.1376 ms 28.3%
  triton_convolution_371 0.1409 ms 27.7%
  triton_convolution_373 0.1496 ms 26.0%
  triton_convolution_369 0.1794 ms 21.7%
  triton_convolution_370 0.2871 ms 13.6%
  triton_convolution_368 0.4118 ms 9.5%
SingleProcess AUTOTUNE takes 5.4421 seconds
AUTOTUNE convolution(1x512x25x38, 512x512x3x3)
  convolution 0.0709 ms 100.0%
  triton_convolution_379 0.2631 ms 26.9%
  triton_convolution_381 0.2825 ms 25.1%
  triton_convolution_378 0.3139 ms 22.6%
  triton_convolution_376 0.4045 ms 17.5%
  triton_convolution_380 0.4456 ms 15.9%
  triton_convolution_377 0.5831 ms 12.2%
  triton_convolution_375 0.7859 ms 9.0%
SingleProcess AUTOTUNE takes 5.4115 seconds
AUTOTUNE convolution(1x512x25x38, 2048x512x1x1)
  convolution 0.0354 ms 100.0%
  triton_convolution_383 0.0611 ms 57.9%
  triton_convolution_386 0.0730 ms 48.5%
  triton_convolution_388 0.0779 ms 45.4%
  triton_convolution_387 0.0790 ms 44.8%
  triton_convolution_385 0.0800 ms 44.2%
  triton_convolution_382 0.1149 ms 30.8%
  conv1x1_via_mm 0.1431 ms 24.7%
  triton_convolution_384 0.1481 ms 23.9%
SingleProcess AUTOTUNE takes 5.7673 seconds
AUTOTUNE convolution(1x512x100x152, 256x512x1x1)
  convolution 0.0365 ms 100.0%
  triton_convolution_411 0.1109 ms 32.9%
  triton_convolution_414 0.1227 ms 29.8%
  triton_convolution_413 0.1229 ms 29.7%
  triton_convolution_416 0.1244 ms 29.3%
  conv1x1_via_mm 0.1347 ms 27.1%
  triton_convolution_415 0.1461 ms 25.0%
  triton_convolution_410 0.1796 ms 20.3%
  triton_convolution_412 0.1992 ms 18.3%
SingleProcess AUTOTUNE takes 5.7865 seconds
AUTOTUNE convolution(1x1024x50x76, 256x1024x1x1)
  convolution 0.0238 ms 100.0%
  triton_convolution_421 0.0576 ms 41.4%
  triton_convolution_423 0.0740 ms 32.2%
  triton_convolution_422 0.0780 ms 30.6%
  triton_convolution_420 0.0802 ms 29.7%
  triton_convolution_418 0.1032 ms 23.1%
  triton_convolution_419 0.1393 ms 17.1%
  conv1x1_via_mm 0.1547 ms 15.4%
  triton_convolution_417 0.1812 ms 13.2%
SingleProcess AUTOTUNE takes 5.5426 seconds
AUTOTUNE convolution(1x2048x25x38, 256x2048x1x1)
  convolution 0.0317 ms 100.0%
  triton_convolution_428 0.0857 ms 37.0%
  conv1x1_via_mm 0.1350 ms 23.5%
  triton_convolution_430 0.1372 ms 23.1%
  triton_convolution_427 0.1410 ms 22.5%
  triton_convolution_429 0.1495 ms 21.2%
  triton_convolution_425 0.1809 ms 17.5%
  triton_convolution_426 0.2832 ms 11.2%
  triton_convolution_424 0.4191 ms 7.6%
SingleProcess AUTOTUNE takes 5.7473 seconds
AUTOTUNE convolution(1x256x100x152, 256x256x3x3)
  convolution 0.1279 ms 100.0%
  triton_convolution_437 0.4379 ms 29.2%
  triton_convolution_435 0.4598 ms 27.8%
  triton_convolution_432 0.4840 ms 26.4%
  triton_convolution_434 0.5709 ms 22.4%
  triton_convolution_431 0.7497 ms 17.1%
  triton_convolution_433 0.8105 ms 15.8%
  triton_convolution_436 1.0113 ms 12.6%
SingleProcess AUTOTUNE takes 5.2989 seconds
AUTOTUNE convolution(1x256x50x76, 256x256x3x3)
  convolution 0.0611 ms 100.0%
  triton_convolution_444 0.1406 ms 43.4%
  triton_convolution_441 0.1643 ms 37.2%
  triton_convolution_442 0.1763 ms 34.6%
  triton_convolution_439 0.2091 ms 29.2%
  triton_convolution_443 0.2488 ms 24.5%
  triton_convolution_440 0.2904 ms 21.0%
  triton_convolution_438 0.3817 ms 16.0%
SingleProcess AUTOTUNE takes 5.0253 seconds
AUTOTUNE convolution(1x256x25x38, 256x256x3x3)
  convolution 0.0369 ms 100.0%
  triton_convolution_449 0.1024 ms 36.0%
  triton_convolution_451 0.1452 ms 25.4%
  triton_convolution_448 0.1583 ms 23.3%
  triton_convolution_446 0.2045 ms 18.0%
  triton_convolution_450 0.2222 ms 16.6%
  triton_convolution_447 0.2989 ms 12.3%
  triton_convolution_445 0.3996 ms 9.2%
SingleProcess AUTOTUNE takes 0.9730 seconds
AUTOTUNE convolution(1x256x25x38, 256x256x3x3)
  convolution 0.0340 ms 100.0%
  triton_convolution_456 0.1013 ms 33.6%
  triton_convolution_458 0.1470 ms 23.1%
  triton_convolution_455 0.1624 ms 20.9%
  triton_convolution_457 0.2209 ms 15.4%
  triton_convolution_453 0.2318 ms 14.7%
  triton_convolution_454 0.2675 ms 12.7%
  triton_convolution_452 0.3719 ms 9.1%
SingleProcess AUTOTUNE takes 5.8559 seconds
AUTOTUNE convolution(1x256x13x19, 256x256x3x3)
  convolution 0.0335 ms 100.0%
  triton_convolution_463 0.0964 ms 34.8%
  triton_convolution_461 0.1145 ms 29.3%
  triton_convolution_465 0.1381 ms 24.3%
  triton_convolution_462 0.1590 ms 21.1%
  triton_convolution_460 0.2170 ms 15.5%
  triton_convolution_464 0.2245 ms 14.9%
  triton_convolution_459 0.3822 ms 8.8%
SingleProcess AUTOTUNE takes 5.7713 seconds
[2023-12-05 03:02:21,419] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (8)
[2023-12-05 03:02:21,419] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:318)
[2023-12-05 03:02:21,419] torch._dynamo.convert_frame: [WARNING]    last reason: L['self']._pos == 0                                           # ret = self[self._pos](x)  # miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:319 in forward
[2023-12-05 03:02:21,419] torch._dynamo.convert_frame: [WARNING] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[2023-12-05 03:02:21,419] torch._dynamo.convert_frame: [WARNING] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.
skipping cudagraphs due to ['mutated inputs']
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 16.89it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:01, 17.82it/s]running benchmark:  20%|██        | 6/30 [00:00<00:01, 18.32it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:01, 18.59it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:01, 17.48it/s]running benchmark:  40%|████      | 12/30 [00:00<00:01, 17.19it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 17.92it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 18.48it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 18.68it/s]running benchmark:  67%|██████▋   | 20/30 [00:01<00:00, 18.96it/s]running benchmark:  73%|███████▎  | 22/30 [00:01<00:00, 19.17it/s]running benchmark:  80%|████████  | 24/30 [00:01<00:00, 18.75it/s]running benchmark:  87%|████████▋ | 26/30 [00:01<00:00, 18.85it/s]running benchmark:  93%|█████████▎| 28/30 [00:01<00:00, 18.92it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 19.09it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 18.53it/s]
1.491x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:08, ?it/s]
WARNING:root:detectron2_maskrcnn_r_101_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
WARNING:root:detectron2_maskrcnn_r_101_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
WARNING:root:detectron2_maskrcnn_r_50_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_maskrcnn_r_50_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:13, ?it/s]
cuda eval  dlrm                               
AUTOTUNE int_mm(2048x512, 512x512, 2048x512)
  triton_mm_8 0.0155 ms 100.0%
  triton_mm_1 0.0175 ms 88.4%
  triton_mm_2 0.0176 ms 87.9%
  triton_mm_4 0.0181 ms 85.4%
  triton_mm_3 0.0189 ms 82.0%
  triton_mm_0 0.0193 ms 80.5%
  triton_mm_9 0.0217 ms 71.4%
  triton_mm_10 0.0221 ms 70.2%
  triton_mm_7 0.0248 ms 62.4%
  triton_mm_5 0.0253 ms 61.3%
SingleProcess AUTOTUNE takes 6.7802 seconds
AUTOTUNE int_mm(2048x512, 512x64, 2048x64)
  triton_mm_16 0.0115 ms 100.0%
  triton_mm_20 0.0118 ms 97.0%
  triton_mm_19 0.0121 ms 95.0%
  triton_mm_17 0.0124 ms 92.7%
  triton_mm_14 0.0131 ms 87.7%
  triton_mm_12 0.0144 ms 79.7%
  triton_mm_15 0.0144 ms 79.6%
  triton_mm_21 0.0154 ms 74.6%
  triton_mm_13 0.0167 ms 68.5%
  triton_mm_11 0.0177 ms 64.6%
SingleProcess AUTOTUNE takes 4.9657 seconds
AUTOTUNE bmm(2048x9x64, 2048x64x9)
  triton_bmm_26 0.0102 ms 100.0%
  triton_bmm_23 0.0104 ms 98.1%
  triton_bmm_27 0.0106 ms 95.8%
  triton_bmm_22 0.0111 ms 92.0%
  triton_bmm_24 0.0114 ms 89.3%
  triton_bmm_29 0.0116 ms 88.1%
  triton_bmm_28 0.0122 ms 83.2%
  triton_bmm_25 0.0123 ms 82.6%
  bmm 0.0240 ms 42.5%
SingleProcess AUTOTUNE takes 2.5143 seconds
AUTOTUNE int_mm(2048x100, 100x1024, 2048x1024)
  triton_mm_30 0.0132 ms 100.0%
  triton_mm_31 0.0145 ms 90.9%
  triton_mm_32 0.0151 ms 87.5%
  triton_mm_37 0.0163 ms 81.1%
  triton_mm_34 0.0163 ms 80.9%
  triton_mm_38 0.0165 ms 80.0%
  triton_mm_33 0.0165 ms 79.8%
  triton_mm_35 0.0218 ms 60.4%
  triton_mm_36 0.0229 ms 57.5%
  triton_mm_39 0.0290 ms 45.5%
SingleProcess AUTOTUNE takes 6.3266 seconds
AUTOTUNE int_mm(2048x1024, 1024x1024, 2048x1024)
  triton_mm_51 0.0284 ms 100.0%
  triton_mm_50 0.0288 ms 98.6%
  triton_mm_49 0.0292 ms 97.3%
  triton_mm_42 0.0325 ms 87.4%
  triton_mm_43 0.0336 ms 84.5%
  triton_mm_41 0.0355 ms 80.0%
  triton_mm_45 0.0371 ms 76.6%
  triton_mm_44 0.0376 ms 75.5%
  triton_mm_48 0.0419 ms 67.8%
  triton_mm_46 0.0712 ms 39.9%
SingleProcess AUTOTUNE takes 7.5742 seconds
AUTOTUNE int_mm(2048x1024, 1024x1, 2048x1)
  triton_mm_72 0.0127 ms 100.0%
  triton_mm_73 0.0137 ms 93.0%
  triton_mm_69 0.0145 ms 88.1%
  triton_mm_68 0.0145 ms 87.7%
  triton_mm_71 0.0147 ms 86.5%
  triton_mm_66 0.0158 ms 80.6%
  triton_mm_64 0.0210 ms 60.7%
  triton_mm_65 0.0223 ms 57.0%
  triton_mm_67 0.0288 ms 44.3%
  triton_mm_63 0.0296 ms 43.1%
SingleProcess AUTOTUNE takes 3.5636 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:02, 13.24it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:01, 14.07it/s]running benchmark:  20%|██        | 6/30 [00:00<00:01, 14.40it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:01, 14.25it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:01, 14.39it/s]running benchmark:  40%|████      | 12/30 [00:00<00:01, 14.61it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:01, 14.68it/s]running benchmark:  53%|█████▎    | 16/30 [00:01<00:00, 14.67it/s]running benchmark:  60%|██████    | 18/30 [00:01<00:00, 14.80it/s]running benchmark:  67%|██████▋   | 20/30 [00:01<00:00, 14.84it/s]running benchmark:  73%|███████▎  | 22/30 [00:01<00:00, 14.89it/s]running benchmark:  80%|████████  | 24/30 [00:01<00:00, 14.90it/s]running benchmark:  87%|████████▋ | 26/30 [00:01<00:00, 14.92it/s]running benchmark:  93%|█████████▎| 28/30 [00:01<00:00, 14.90it/s]running benchmark: 100%|██████████| 30/30 [00:02<00:00, 14.80it/s]running benchmark: 100%|██████████| 30/30 [00:02<00:00, 14.67it/s]
41.032x
loading model: 0it [00:00, ?it/s]Downloading https://doctr-static.mindee.com/models?id=v0.3.1/db_resnet50-ac60cadc.pt&src=0 to /home/cdhernandez/.cache/doctr/models/db_resnet50-ac60cadc.pt

  0%|          | 0/101971449 [00:00<?, ?it/s][A
 15%|█▍        | 15122432/101971449 [00:00<00:00, 149696201.73it/s][A
 40%|███▉      | 40409088/101971449 [00:00<00:00, 210125162.56it/s][A
 65%|██████▍   | 66052096/101971449 [00:00<00:00, 231219826.56it/s][A
 87%|████████▋ | 89193472/101971449 [00:00<00:00, 228037727.49it/s][A101971968it [00:00, 220035020.93it/s]                              
Downloading https://doctr-static.mindee.com/models?id=v0.3.1/crnn_vgg16_bn-9762b0b0.pt&src=0 to /home/cdhernandez/.cache/doctr/models/crnn_vgg16_bn-9762b0b0.pt

  0%|          | 0/63286381 [00:00<?, ?it/s][A
 14%|█▍        | 9107456/63286381 [00:00<00:00, 90873370.45it/s][A
 42%|████▏     | 26789888/63286381 [00:00<00:00, 141382424.12it/s][A
 65%|██████▍   | 40933376/63286381 [00:00<00:00, 140744224.29it/s][A
 93%|█████████▎| 59078656/63286381 [00:00<00:00, 156704834.59it/s][A63287296it [00:00, 146621151.73it/s]                              
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
loading model: 0it [00:10, ?it/s]
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
cuda eval  doctr_det_predictor                
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
AUTOTUNE convolution(1x3x1024x1024, 64x3x7x7)
  convolution 0.1125 ms 100.0%
  triton_convolution_3 0.4635 ms 24.3%
  triton_convolution_4 0.5112 ms 22.0%
  triton_convolution_5 0.5896 ms 19.1%
  triton_convolution_0 0.5899 ms 19.1%
  triton_convolution_2 0.7720 ms 14.6%
  triton_convolution_1 0.9443 ms 11.9%
SingleProcess AUTOTUNE takes 2.8593 seconds
AUTOTUNE mm(65536x64, 64x64)
  triton_mm_14 0.0187 ms 100.0%
  triton_mm_8 0.0196 ms 95.6%
  triton_mm_7 0.0204 ms 91.5%
  triton_mm_6 0.0206 ms 90.5%
  triton_mm_10 0.0211 ms 88.6%
  mm 0.0215 ms 86.9%
  triton_mm_9 0.0220 ms 84.9%
  triton_mm_13 0.0223 ms 83.7%
  triton_mm_16 0.0241 ms 77.6%
  triton_mm_15 0.0248 ms 75.2%
SingleProcess AUTOTUNE takes 3.7343 seconds
AUTOTUNE convolution(1x64x256x256, 64x64x3x3)
  convolution 0.0463 ms 100.0%
  triton_convolution_18 0.1763 ms 26.3%
  triton_convolution_23 0.1780 ms 26.0%
  triton_convolution_24 0.1827 ms 25.4%
  triton_convolution_21 0.2587 ms 17.9%
  triton_convolution_22 0.2671 ms 17.4%
  triton_convolution_19 0.3198 ms 14.5%
  triton_convolution_20 0.7635 ms 6.1%
SingleProcess AUTOTUNE takes 3.5043 seconds
AUTOTUNE mm(65536x64, 64x256)
  triton_mm_27 0.0371 ms 100.0%
  triton_mm_26 0.0392 ms 94.6%
  triton_mm_33 0.0412 ms 90.1%
  triton_mm_29 0.0449 ms 82.7%
  mm 0.0465 ms 79.7%
  triton_mm_28 0.0469 ms 79.1%
  triton_mm_25 0.0480 ms 77.3%
  triton_mm_35 0.0520 ms 71.3%
  triton_mm_32 0.0546 ms 67.9%
  triton_mm_34 0.0650 ms 57.1%
SingleProcess AUTOTUNE takes 4.6646 seconds
AUTOTUNE mm(65536x256, 256x64)
  mm 0.0434 ms 100.0%
  triton_mm_51 0.0438 ms 99.1%
  triton_mm_52 0.0444 ms 97.7%
  triton_mm_50 0.0446 ms 97.3%
  triton_mm_53 0.0452 ms 96.0%
  triton_mm_57 0.0465 ms 93.3%
  triton_mm_49 0.0465 ms 93.3%
  triton_mm_56 0.0466 ms 93.1%
  triton_mm_54 0.0502 ms 86.5%
  triton_mm_55 0.0542 ms 80.0%
SingleProcess AUTOTUNE takes 4.1269 seconds
AUTOTUNE mm(65536x256, 256x128)
  mm 0.0528 ms 100.0%
  triton_mm_113 0.0531 ms 99.5%
  triton_mm_112 0.0533 ms 99.0%
  triton_mm_115 0.0548 ms 96.4%
  triton_mm_114 0.0553 ms 95.5%
  triton_mm_118 0.0556 ms 95.0%
  triton_mm_111 0.0575 ms 91.8%
  triton_mm_119 0.0577 ms 91.5%
  triton_mm_121 0.0790 ms 66.9%
  triton_mm_116 0.0821 ms 64.3%
SingleProcess AUTOTUNE takes 4.4779 seconds
AUTOTUNE convolution(1x128x256x256, 128x128x3x3)
  convolution 0.0459 ms 100.0%
  triton_convolution_129 0.3183 ms 14.4%
  triton_convolution_128 0.3437 ms 13.4%
  triton_convolution_124 0.3759 ms 12.2%
  triton_convolution_123 0.3858 ms 11.9%
  triton_convolution_126 0.4182 ms 11.0%
  triton_convolution_127 0.4296 ms 10.7%
  triton_convolution_125 0.9810 ms 4.7%
SingleProcess AUTOTUNE takes 4.0352 seconds
AUTOTUNE mm(16384x128, 128x512)
  triton_mm_132 0.0266 ms 100.0%
  triton_mm_131 0.0274 ms 97.1%
  mm 0.0283 ms 94.1%
  triton_mm_130 0.0297 ms 89.7%
  triton_mm_134 0.0308 ms 86.6%
  triton_mm_137 0.0313 ms 85.1%
  triton_mm_133 0.0315 ms 84.5%
  triton_mm_138 0.0336 ms 79.3%
  triton_mm_140 0.0372 ms 71.5%
  triton_mm_136 0.0544 ms 49.0%
SingleProcess AUTOTUNE takes 4.5172 seconds
AUTOTUNE convolution(1x256x256x256, 512x256x1x1)
  convolution 0.0382 ms 100.0%
  triton_convolution_147 0.1043 ms 36.6%
  triton_convolution_143 0.1064 ms 35.9%
  triton_convolution_145 0.1072 ms 35.6%
  triton_convolution_148 0.1118 ms 34.1%
  triton_convolution_142 0.1141 ms 33.5%
  triton_convolution_146 0.1153 ms 33.1%
  triton_convolution_144 0.5564 ms 6.9%
SingleProcess AUTOTUNE takes 4.7894 seconds
AUTOTUNE mm(16384x512, 512x128)
  triton_mm_150 0.0261 ms 100.0%
  triton_mm_151 0.0296 ms 88.2%
  triton_mm_152 0.0297 ms 87.9%
  triton_mm_157 0.0307 ms 85.1%
  triton_mm_149 0.0319 ms 81.9%
  mm 0.0319 ms 81.9%
  triton_mm_153 0.0331 ms 79.1%
  triton_mm_156 0.0347 ms 75.3%
  triton_mm_159 0.0393 ms 66.6%
  triton_mm_154 0.0419 ms 62.4%
SingleProcess AUTOTUNE takes 4.7463 seconds
AUTOTUNE convolution(1x128x128x128, 128x128x3x3)
  convolution 0.0368 ms 100.0%
  triton_convolution_166 0.2071 ms 17.7%
  triton_convolution_167 0.2180 ms 16.9%
  triton_convolution_164 0.2301 ms 16.0%
  triton_convolution_165 0.2592 ms 14.2%
  triton_convolution_161 0.2684 ms 13.7%
  triton_convolution_162 0.3224 ms 11.4%
  triton_convolution_163 0.9630 ms 3.8%
SingleProcess AUTOTUNE takes 5.0367 seconds
AUTOTUNE mm(16384x512, 512x256)
  mm 0.0355 ms 100.0%
  triton_mm_244 0.0366 ms 96.8%
  triton_mm_243 0.0379 ms 93.6%
  triton_mm_245 0.0388 ms 91.3%
  triton_mm_246 0.0393 ms 90.3%
  triton_mm_250 0.0456 ms 77.7%
  triton_mm_242 0.0474 ms 74.9%
  triton_mm_249 0.0529 ms 67.1%
  triton_mm_252 0.0688 ms 51.6%
  triton_mm_247 0.0709 ms 50.0%
SingleProcess AUTOTUNE takes 4.7142 seconds
AUTOTUNE convolution(1x256x128x128, 256x256x3x3)
  convolution 0.0396 ms 100.0%
  triton_convolution_259 0.3147 ms 12.6%
  triton_convolution_260 0.3570 ms 11.1%
  triton_convolution_257 0.4518 ms 8.8%
  triton_convolution_254 0.5975 ms 6.6%
  triton_convolution_258 0.6045 ms 6.6%
  triton_convolution_255 0.6439 ms 6.2%
  triton_convolution_256 0.9833 ms 4.0%
SingleProcess AUTOTUNE takes 4.3828 seconds
AUTOTUNE mm(4096x256, 256x1024)
  triton_mm_263 0.0227 ms 100.0%
  triton_mm_262 0.0228 ms 99.7%
  mm 0.0234 ms 97.0%
  triton_mm_264 0.0248 ms 91.6%
  triton_mm_265 0.0252 ms 90.1%
  triton_mm_261 0.0281 ms 80.7%
  triton_mm_268 0.0286 ms 79.4%
  triton_mm_269 0.0289 ms 78.4%
  triton_mm_271 0.0369 ms 61.5%
  triton_mm_266 0.0452 ms 50.2%
SingleProcess AUTOTUNE takes 4.4771 seconds
AUTOTUNE convolution(1x512x128x128, 1024x512x1x1)
  convolution 0.0349 ms 100.0%
  triton_convolution_276 0.1084 ms 32.2%
  triton_convolution_277 0.1134 ms 30.7%
  triton_convolution_278 0.1134 ms 30.7%
  triton_convolution_279 0.1177 ms 29.6%
  triton_convolution_274 0.1228 ms 28.4%
  triton_convolution_273 0.1318 ms 26.5%
  triton_convolution_275 0.6568 ms 5.3%
SingleProcess AUTOTUNE takes 4.4140 seconds
AUTOTUNE mm(4096x1024, 1024x256)
  mm 0.0233 ms 100.0%
  triton_mm_282 0.0252 ms 92.4%
  triton_mm_281 0.0268 ms 87.2%
  triton_mm_284 0.0276 ms 84.6%
  triton_mm_283 0.0279 ms 83.6%
  triton_mm_288 0.0279 ms 83.6%
  triton_mm_285 0.0385 ms 60.6%
  triton_mm_286 0.0391 ms 59.7%
  triton_mm_280 0.0399 ms 58.5%
  triton_mm_289 0.0436 ms 53.5%
SingleProcess AUTOTUNE takes 4.6822 seconds
AUTOTUNE convolution(1x256x64x64, 256x256x3x3)
  convolution 0.0422 ms 100.0%
  triton_convolution_297 0.2175 ms 19.4%
  triton_convolution_295 0.2345 ms 18.0%
  triton_convolution_298 0.2658 ms 15.9%
  triton_convolution_296 0.3918 ms 10.8%
  triton_convolution_292 0.4977 ms 8.5%
  triton_convolution_293 0.5304 ms 8.0%
  triton_convolution_294 0.9656 ms 4.4%
SingleProcess AUTOTUNE takes 5.0874 seconds
AUTOTUNE mm(4096x1024, 1024x512)
  triton_mm_437 0.0320 ms 100.0%
  triton_mm_436 0.0324 ms 98.7%
  mm 0.0348 ms 91.7%
  triton_mm_439 0.0393 ms 81.3%
  triton_mm_438 0.0397 ms 80.6%
  triton_mm_443 0.0405 ms 78.9%
  triton_mm_435 0.0437 ms 73.1%
  triton_mm_445 0.0642 ms 49.8%
  triton_mm_440 0.0682 ms 46.9%
  triton_mm_442 0.0692 ms 46.2%
SingleProcess AUTOTUNE takes 4.7967 seconds
AUTOTUNE convolution(1x512x64x64, 512x512x3x3)
  convolution 0.0465 ms 100.0%
  triton_convolution_452 0.5835 ms 8.0%
  triton_convolution_453 0.7362 ms 6.3%
  triton_convolution_451 0.7811 ms 6.0%
  triton_convolution_450 0.8416 ms 5.5%
  triton_convolution_447 1.2886 ms 3.6%
  triton_convolution_448 1.3656 ms 3.4%
  triton_convolution_449 2.0296 ms 2.3%
SingleProcess AUTOTUNE takes 4.3845 seconds
AUTOTUNE mm(1024x512, 512x2048)
  triton_mm_455 0.0209 ms 100.0%
  triton_mm_456 0.0214 ms 97.3%
  triton_mm_458 0.0245 ms 85.2%
  triton_mm_457 0.0245 ms 85.1%
  triton_mm_462 0.0252 ms 82.7%
  triton_mm_454 0.0271 ms 77.1%
  triton_mm_461 0.0297 ms 70.3%
  mm 0.0305 ms 68.4%
  triton_mm_464 0.0336 ms 62.2%
  triton_mm_463 0.0396 ms 52.6%
SingleProcess AUTOTUNE takes 4.6011 seconds
AUTOTUNE convolution(1x1024x64x64, 2048x1024x1x1)
  convolution 0.0376 ms 100.0%
  triton_convolution_467 0.1246 ms 30.1%
  triton_convolution_470 0.1289 ms 29.2%
  triton_convolution_466 0.1436 ms 26.2%
  triton_convolution_469 0.1455 ms 25.8%
  triton_convolution_472 0.1463 ms 25.7%
  triton_convolution_471 0.1465 ms 25.6%
  triton_convolution_468 0.8747 ms 4.3%
SingleProcess AUTOTUNE takes 4.5826 seconds
AUTOTUNE mm(1024x2048, 2048x512)
  mm 0.0225 ms 100.0%
  triton_mm_477 0.0298 ms 75.5%
  triton_mm_476 0.0300 ms 75.1%
  triton_mm_481 0.0326 ms 69.0%
  triton_mm_475 0.0385 ms 58.5%
  triton_mm_474 0.0394 ms 57.2%
  triton_mm_478 0.0426 ms 52.9%
  triton_mm_479 0.0442 ms 51.0%
  triton_mm_482 0.0480 ms 46.9%
  triton_mm_473 0.0588 ms 38.3%
SingleProcess AUTOTUNE takes 5.2675 seconds
AUTOTUNE convolution(1x512x32x32, 512x512x3x3)
  convolution 0.0451 ms 100.0%
  triton_convolution_490 0.4816 ms 9.4%
  triton_convolution_489 0.5815 ms 7.8%
  triton_convolution_491 0.6245 ms 7.2%
  triton_convolution_488 0.6785 ms 6.6%
  triton_convolution_485 1.1572 ms 3.9%
  triton_convolution_486 1.2229 ms 3.7%
  triton_convolution_487 1.9801 ms 2.3%
SingleProcess AUTOTUNE takes 4.9766 seconds
AUTOTUNE mm(1024x2048, 2048x256)
  mm 0.0171 ms 100.0%
  triton_mm_543 0.0251 ms 68.1%
  triton_mm_544 0.0266 ms 64.2%
  triton_mm_539 0.0288 ms 59.2%
  triton_mm_538 0.0297 ms 57.5%
  triton_mm_540 0.0309 ms 55.2%
  triton_mm_541 0.0315 ms 54.2%
  triton_mm_537 0.0374 ms 45.6%
  triton_mm_536 0.0389 ms 43.8%
  triton_mm_535 0.0575 ms 29.6%
SingleProcess AUTOTUNE takes 5.1264 seconds
AUTOTUNE mm(65536x256, 256x256)
  mm 0.0643 ms 100.0%
  triton_mm_573 0.0694 ms 92.7%
  triton_mm_572 0.0713 ms 90.2%
  triton_mm_575 0.0770 ms 83.5%
  triton_mm_574 0.0794 ms 81.0%
  triton_mm_578 0.0808 ms 79.6%
  triton_mm_571 0.0834 ms 77.2%
  triton_mm_579 0.0903 ms 71.3%
  triton_mm_581 0.1024 ms 62.8%
  triton_mm_576 0.1563 ms 41.2%
SingleProcess AUTOTUNE takes 4.4774 seconds
AUTOTUNE convolution(1x256x256x256, 64x256x3x3)
  convolution 0.1151 ms 100.0%
  triton_convolution_589 0.7918 ms 14.5%
  triton_convolution_588 0.8193 ms 14.0%
  triton_convolution_583 0.8996 ms 12.8%
  triton_convolution_586 1.0415 ms 11.1%
  triton_convolution_587 1.2789 ms 9.0%
  triton_convolution_584 1.6536 ms 7.0%
  triton_convolution_585 3.0828 ms 3.7%
SingleProcess AUTOTUNE takes 3.8744 seconds
AUTOTUNE convolution(1x256x128x128, 64x256x3x3)
  convolution 0.0475 ms 100.0%
  triton_convolution_596 0.2747 ms 17.3%
  triton_convolution_595 0.2842 ms 16.7%
  triton_convolution_590 0.3045 ms 15.6%
  triton_convolution_593 0.4041 ms 11.8%
  triton_convolution_594 0.4266 ms 11.1%
  triton_convolution_591 0.5751 ms 8.3%
  triton_convolution_592 0.9678 ms 4.9%
SingleProcess AUTOTUNE takes 3.7758 seconds
AUTOTUNE convolution(1x256x64x64, 64x256x3x3)
  convolution 0.0215 ms 100.0%
  triton_convolution_602 0.1291 ms 16.7%
  triton_convolution_601 0.1998 ms 10.8%
  triton_convolution_600 0.2057 ms 10.5%
  triton_convolution_597 0.2077 ms 10.4%
  triton_convolution_603 0.2742 ms 7.8%
  triton_convolution_598 0.5492 ms 3.9%
  triton_convolution_599 0.9636 ms 2.2%
SingleProcess AUTOTUNE takes 3.7919 seconds
AUTOTUNE convolution(1x256x32x32, 64x256x3x3)
  convolution 0.0170 ms 100.0%
  triton_convolution_609 0.1226 ms 13.8%
  triton_convolution_608 0.1794 ms 9.5%
  triton_convolution_607 0.1901 ms 8.9%
  triton_convolution_604 0.1919 ms 8.8%
  triton_convolution_610 0.2773 ms 6.1%
  triton_convolution_605 0.5452 ms 3.1%
  triton_convolution_606 0.9586 ms 1.8%
SingleProcess AUTOTUNE takes 3.9130 seconds
[2023-12-05 03:07:14,884] [1/0_1] torch._inductor.utils: [WARNING] DeviceCopy in input program
skipping cudagraphs due to ['non-cuda device in graph']
Fatal glibc error: malloc.c:2496 (sysmalloc): assertion failed: (old_top == initial_top (av) && old_size == 0) || ((unsigned long) (old_size) >= MINSIZE && prev_inuse (old_top) && ((unsigned long) old_end & (pagesize - 1)) == 0)
Run failed with return code:  -6
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
loading model: 0it [00:07, ?it/s]
WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
cuda eval  doctr_reco_predictor               
WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
ERROR:common:Backend eager failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/doctr/models/recognition/crnn/pytorch.py", line 212, in forward
    logits = self.linear(logits)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/drq/__init__.py", line 11, in <module>
    from gym import spaces
ModuleNotFoundError: No module named 'gym'
Failed to import user benchmark module dynamo, error: No module named 'gym'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  fastNLP_Bert                       
ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/models/bert.py", line 265, in forward
    sequence_output = self.bert(words)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 137, in forward
    outputs = self.model(words)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 445, in forward
    max_word_piece_length = batch_word_pieces_length.sum(dim=-1).max().item()  # 表示word piece的长度(包括padding)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 482, in resume_in_forward
    bert_outputs, pooled_cls = self.encoder(word_pieces, token_type_ids=token_type_ids, attention_mask=attn_masks,
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/eval_frame.py", line 655, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 722, in _convert_frame
    result = inner_convert(frame, cache_entry, hooks, frame_state)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert
    compiled_product = _compile(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 646, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 562, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object
    transformations(instructions, code_options)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 151, in _fn
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 527, in transform
    tracer.run()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 2123, in run
    super().run()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 818, in run
    and self.step()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 781, in step
    getattr(self, inst.opname)(inst)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 470, in wrapper
    return inner_fn(self, inst)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 1213, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 652, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/functions.py", line 291, in call_function
    return self.obj.call_method(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/nn_module.py", line 494, in call_method
    return wrap_values(module.named_parameters(**get_kwargs("recurse")))
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/nn_module.py", line 430, in wrap_values
    tx.output.register_attr_or_module(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 788, in register_attr_or_module
    return wrap_name(name)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 719, in wrap_name
    return wrap_fx_proxy(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/builder.py", line 1291, in wrap_fx_proxy
    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/builder.py", line 1401, in wrap_fx_proxy_cls
    example_value = wrap_to_fake_tensor_and_record(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/builder.py", line 1767, in wrap_to_fake_tensor_and_record
    fake_e = wrap_fake_exception(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 1026, in wrap_fake_exception
    return fn()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/builder.py", line 1768, in <lambda>
    lambda: tx.fake_mode.from_tensor(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/fake_tensor.py", line 1884, in from_tensor
    return self.fake_tensor_converter(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/fake_tensor.py", line 396, in __call__
    return self.from_real_tensor(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/fake_tensor.py", line 353, in from_real_tensor
    out = self.meta_converter(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 710, in __call__
    r = self.meta_tensor(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 530, in meta_tensor
    r = transform_subclass(
  File "/home/cdhernandez/local/pytorch/torch/utils/_python_dispatch.py", line 168, in transform_subclass
    transformed_tensors_dict[attr] = callback(attr, getattr(t, attr))
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 532, in <lambda>
    lambda attr, inner_t: callback(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/fake_tensor.py", line 348, in mk_fake_tensor
    make_meta_t(),
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 533, in <lambda>
    lambda: empty_create(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 460, in empty_create
    ) = sym_sizes_strides_storage_offset(inner_t, inner_src)
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 246, in sym_sizes_strides_storage_offset
    return shape_env.create_symbolic_sizes_strides_storage_offset(
  File "/home/cdhernandez/local/pytorch/torch/fx/experimental/symbolic_shapes.py", line 2108, in create_symbolic_sizes_strides_storage_offset
    return self._create_symbolic_sizes_strides_storage_offset(
  File "/home/cdhernandez/local/pytorch/torch/fx/experimental/recording.py", line 226, in wrapper
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/fx/experimental/symbolic_shapes.py", line 2159, in _create_symbolic_sizes_strides_storage_offset
    assert len(dynamic_dims) == dim, f"{len(dynamic_dims)} != {dim}"
AssertionError: 2 != 1

from user code:
   File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/modules/encoder/bert.py", line 509, in forward
    extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  functorch_dp_cifar10               
AUTOTUNE convolution(64x3x32x32, 64x3x7x7)
  convolution 0.0426 ms 100.0%
  triton_convolution_4 0.0526 ms 80.9%
  triton_convolution_3 0.0547 ms 77.8%
  triton_convolution_0 0.0599 ms 71.0%
  triton_convolution_5 0.0697 ms 61.0%
  triton_convolution_2 0.0814 ms 52.3%
  triton_convolution_1 0.1412 ms 30.1%
SingleProcess AUTOTUNE takes 3.0944 seconds
AUTOTUNE convolution(64x64x8x8, 64x64x3x3)
  convolution 0.0199 ms 100.0%
  triton_convolution_11 0.0373 ms 53.5%
  triton_convolution_10 0.0492 ms 40.6%
  triton_convolution_6 0.0510 ms 39.1%
  triton_convolution_9 0.0591 ms 33.7%
  triton_convolution_12 0.0703 ms 28.3%
  triton_convolution_7 0.1105 ms 18.0%
  triton_convolution_8 0.2205 ms 9.0%
SingleProcess AUTOTUNE takes 3.8247 seconds
AUTOTUNE convolution(64x64x8x8, 128x64x3x3)
  convolution 0.0131 ms 100.0%
  triton_convolution_39 0.0728 ms 18.0%
  triton_convolution_38 0.0982 ms 13.4%
  triton_convolution_34 0.1077 ms 12.2%
  triton_convolution_40 0.1082 ms 12.1%
  triton_convolution_37 0.1283 ms 10.2%
  triton_convolution_35 0.1835 ms 7.2%
  triton_convolution_36 0.2452 ms 5.3%
SingleProcess AUTOTUNE takes 3.8045 seconds
AUTOTUNE convolution(64x128x4x4, 128x128x3x3)
  convolution 0.0137 ms 100.0%
  triton_convolution_46 0.0911 ms 15.0%
  triton_convolution_45 0.0973 ms 14.0%
  triton_convolution_41 0.1228 ms 11.1%
  triton_convolution_47 0.1444 ms 9.5%
  triton_convolution_44 0.1457 ms 9.4%
  triton_convolution_42 0.2586 ms 5.3%
  triton_convolution_43 0.4150 ms 3.3%
SingleProcess AUTOTUNE takes 4.5416 seconds
AUTOTUNE convolution(64x64x8x8, 128x64x1x1)
  convolution 0.0084 ms 100.0%
  triton_convolution_52 0.0094 ms 88.8%
  triton_convolution_48 0.0101 ms 82.9%
  triton_convolution_53 0.0106 ms 78.9%
  triton_convolution_51 0.0114 ms 73.5%
  triton_convolution_54 0.0131 ms 63.8%
  triton_convolution_49 0.0149 ms 56.1%
  triton_convolution_50 0.0349 ms 23.9%
SingleProcess AUTOTUNE takes 4.1224 seconds
AUTOTUNE convolution(64x128x4x4, 256x128x3x3)
  convolution 0.0135 ms 100.0%
  triton_convolution_71 0.1787 ms 7.5%
  triton_convolution_74 0.1828 ms 7.4%
  triton_convolution_75 0.2177 ms 6.2%
  triton_convolution_73 0.2427 ms 5.6%
  triton_convolution_72 0.2803 ms 4.8%
  triton_convolution_69 0.3303 ms 4.1%
  triton_convolution_70 0.3591 ms 3.8%
SingleProcess AUTOTUNE takes 4.6351 seconds
AUTOTUNE convolution(64x256x2x2, 256x256x3x3)
  convolution 0.0172 ms 100.0%
  triton_convolution_78 0.1843 ms 9.3%
  triton_convolution_80 0.2748 ms 6.3%
  triton_convolution_81 0.2767 ms 6.2%
  triton_convolution_82 0.3128 ms 5.5%
  triton_convolution_79 0.3444 ms 5.0%
  triton_convolution_76 0.5776 ms 3.0%
  triton_convolution_77 0.5780 ms 3.0%
SingleProcess AUTOTUNE takes 4.1843 seconds
AUTOTUNE convolution(64x128x4x4, 256x128x1x1)
  convolution 0.0095 ms 100.0%
  triton_convolution_87 0.0115 ms 82.8%
  triton_convolution_86 0.0166 ms 57.4%
  triton_convolution_89 0.0174 ms 54.8%
  triton_convolution_88 0.0177 ms 54.0%
  triton_convolution_84 0.0214 ms 44.5%
  triton_convolution_83 0.0219 ms 43.6%
  triton_convolution_85 0.0307 ms 31.1%
SingleProcess AUTOTUNE takes 4.4671 seconds
AUTOTUNE convolution(64x256x2x2, 512x256x3x3)
  convolution 0.0186 ms 100.0%
  triton_convolution_106 0.1599 ms 11.6%
  triton_convolution_109 0.1817 ms 10.2%
  triton_convolution_105 0.2068 ms 9.0%
  triton_convolution_110 0.2157 ms 8.6%
  triton_convolution_108 0.2280 ms 8.2%
  triton_convolution_107 0.2393 ms 7.8%
  triton_convolution_104 0.5305 ms 3.5%
SingleProcess AUTOTUNE takes 3.9557 seconds
AUTOTUNE convolution(64x512x1x1, 512x512x3x3)
  convolution 0.0229 ms 100.0%
  triton_convolution_113 0.2138 ms 10.7%
  triton_convolution_117 0.2314 ms 9.9%
  triton_convolution_114 0.2740 ms 8.4%
  triton_convolution_115 0.3104 ms 7.4%
  triton_convolution_116 0.4088 ms 5.6%
  triton_convolution_112 0.4227 ms 5.4%
  triton_convolution_111 0.9017 ms 2.5%
SingleProcess AUTOTUNE takes 3.0631 seconds
AUTOTUNE convolution(64x256x2x2, 512x256x1x1)
  convolution 0.0100 ms 100.0%
  triton_convolution_122 0.0161 ms 62.1%
  triton_convolution_124 0.0206 ms 48.6%
  triton_convolution_121 0.0232 ms 43.0%
  triton_convolution_119 0.0242 ms 41.4%
  triton_convolution_120 0.0257 ms 38.9%
  triton_convolution_123 0.0268 ms 37.2%
  triton_convolution_118 0.0684 ms 14.6%
SingleProcess AUTOTUNE takes 3.6918 seconds
AUTOTUNE int_mm(64x512, 512x1000, 64x1000)
  triton_mm_147 0.0111 ms 100.0%
  triton_mm_145 0.0111 ms 99.7%
  triton_mm_144 0.0114 ms 97.1%
  triton_mm_149 0.0118 ms 93.5%
  triton_mm_143 0.0126 ms 87.8%
  triton_mm_142 0.0143 ms 77.2%
  triton_mm_141 0.0144 ms 77.0%
  triton_mm_148 0.0147 ms 75.2%
  triton_mm_140 0.0156 ms 70.9%
  triton_mm_139 0.0177 ms 62.7%
SingleProcess AUTOTUNE takes 5.3381 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 167.54it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 169.54it/s]
6.956x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/functorch_maml_omniglot/__init__.py", line 73, in __init__
    self.meta_inputs = torch.load(f'{root}/maml_omniglot/batch.pt')
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/models/maml_omniglot/batch.pt'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py", line 36, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Albert/__init__.py", line 10, in __init__
    super().__init__(name="hf_Albert", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.albert.modeling_albert because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 36, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Bart/__init__.py", line 10, in __init__
    super().__init__(name="hf_Bart", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.bart.modeling_bart because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 39, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_BigBird/__init__.py", line 10, in __init__
    super().__init__(name="hf_BigBird", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.big_bird.modeling_big_bird because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 32, in <module>
    from ...integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_DistilBert/__init__.py", line 10, in __init__
    super().__init__(name="hf_DistilBert", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.distilbert.modeling_distilbert because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]
Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s][ADownloading config.json: 100%|██████████| 665/665 [00:00<00:00, 3.71MB/s]
loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel, SequenceSummary
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_GPT2/__init__.py", line 10, in __init__
    super().__init__(name="hf_GPT2", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]
Downloading config.json:   0%|          | 0.00/666 [00:00<?, ?B/s][ADownloading config.json: 100%|██████████| 666/666 [00:00<00:00, 3.82MB/s]
loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel, SequenceSummary
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_GPT2_large/__init__.py", line 10, in __init__
    super().__init__(name="hf_GPT2_large", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]
Downloading config.json:   0%|          | 0.00/694 [00:00<?, ?B/s][ADownloading config.json: 100%|██████████| 694/694 [00:00<00:00, 3.33MB/s]
loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 27, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Longformer/__init__.py", line 10, in __init__
    super().__init__(name="hf_Longformer", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.longformer.modeling_longformer because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 33, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Reformer/__init__.py", line 10, in __init__
    super().__init__(name="hf_Reformer", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.reformer.modeling_reformer because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]
Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s][ADownloading config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 6.78MB/s]
loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]
Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s][ADownloading config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 7.19MB/s]
loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_base/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5_base", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_generate/__init__.py", line 5, in __init__
    super().__init__(name="hf_T5_generate", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 189, in __init__
    super().__init__(name=name, test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]
Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s][ADownloading config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 6.91MB/s]
loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_large/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5_large", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py", line 35, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Whisper/__init__.py", line 12, in __init__
    super().__init__(name="hf_Whisper", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.whisper.modeling_whisper because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py", line 27, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_clip/__init__.py", line 13, in <module>
    from transformers import CLIPProcessor, CLIPModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1205, in __getattr__
    value = getattr(module, name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.clip.modeling_clip because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
cuda eval  lennard_jones                      
ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/eval_frame.py", line 655, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 722, in _convert_frame
    result = inner_convert(frame, cache_entry, hooks, frame_state)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert
    compiled_product = _compile(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 646, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 562, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object
    transformations(instructions, code_options)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 151, in _fn
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 527, in transform
    tracer.run()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 2123, in run
    super().run()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 818, in run
    and self.step()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 781, in step
    getattr(self, inst.opname)(inst)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 2238, in RETURN_VALUE
    self.output.compile_subgraph(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 912, in compile_subgraph
    self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 1080, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 1152, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 1133, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/__init__.py", line 1657, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 952, in compile_fx
    return compile_fx(
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 1168, in compile_fx
    return aot_autograd(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/backends/common.py", line 55, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/aot_autograd.py", line 885, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/aot_autograd.py", line 598, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 424, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 629, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 97, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 1100, in fw_compiler_base
    return inner_compile(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/debug.py", line 305, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 320, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 535, in fx_codegen_and_compile
    graph.run(*example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/graph.py", line 516, in run
    return super().run(*args)
  File "/home/cdhernandez/local/pytorch/torch/fx/interpreter.py", line 138, in run
    self.env[node] = self.run_node(node)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/graph.py", line 820, in run_node
    result = super().run_node(n)
  File "/home/cdhernandez/local/pytorch/torch/fx/interpreter.py", line 195, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/graph.py", line 656, in call_function
    return target(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/fx_passes/post_grad.py", line 987, in fused_int_mm_mul
    return inductor.kernel.mm.tuned_fused_int_mm_mul(mat1, mat2, mat3, out_dtype)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/kernel/mm.py", line 305, in tuned_fused_int_mm_mul
    return autotune_select_algorithm("int_mm", choices, [mat1, mat2, mat3], layout)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 988, in autotune_select_algorithm
    return _ALGORITHM_SELECTOR_CACHE(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 745, in __call__
    timings = self.lookup(
  File "/home/cdhernandez/local/pytorch/torch/_inductor/codecache.py", line 291, in lookup
    timings = benchmark(choices)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 736, in autotune
    return make_benchmark_fn()(choices)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 845, in benchmark_in_current_process
    timing = benchmark_choice_in_current_process(choice)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 835, in benchmark_choice_in_current_process
    result = choice.benchmark(*example_inputs, out=out)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 601, in benchmark
    return self.bmreq.benchmark(*args, output_tensor=out)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/autotune_process.py", line 452, in benchmark
    out = do_bench(fn)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/utils.py", line 167, in do_bench
    return triton_do_bench(*args, **kwargs)[0]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/triton/testing.py", line 106, in do_bench
    fn()
  File "<string>", line 74, in triton_mm
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/triton/compiler/compiler.py", line 566, in compile
    next_module = compile_kernel(module)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/triton/compiler/compiler.py", line 466, in <lambda>
    lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/triton/compiler/code_generator.py", line 1149, in ast_to_ttir
    raise CompilationError(fn.src, node, repr(e)) from e
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
CompilationError: at 55:25:
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
                         ^
AssertionError('small blocks not supported!')

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  llama                              
AUTOTUNE int_mm(1024x512, 512x512, 1024x512)
  triton_mm_8 0.0254 ms 100.0%
  triton_mm_6 0.0277 ms 91.7%
  triton_mm_0 0.0293 ms 86.8%
  triton_mm_4 0.0305 ms 83.5%
  triton_mm_10 0.0305 ms 83.4%
  triton_mm_3 0.0308 ms 82.7%
  triton_mm_7 0.0309 ms 82.4%
  triton_mm_9 0.0341 ms 74.6%
  triton_mm_1 0.0352 ms 72.3%
  triton_mm_2 0.0424 ms 60.0%
SingleProcess AUTOTUNE takes 7.5687 seconds
AUTOTUNE int_mm(1024x512, 512x1536, 1024x1536)
  triton_mm_45 0.0273 ms 100.0%
  triton_mm_44 0.0276 ms 99.0%
  triton_mm_46 0.0288 ms 94.8%
  triton_mm_53 0.0310 ms 88.2%
  triton_mm_47 0.0343 ms 79.7%
  triton_mm_54 0.0432 ms 63.3%
  triton_mm_49 0.0444 ms 61.5%
  triton_mm_48 0.0445 ms 61.4%
  triton_mm_51 0.0450 ms 60.7%
  triton_mm_50 0.0454 ms 60.2%
SingleProcess AUTOTUNE takes 7.5731 seconds
AUTOTUNE int_mm(1024x1536, 1536x512, 1024x512)
  triton_mm_74 0.0306 ms 100.0%
  triton_mm_68 0.0370 ms 82.6%
  triton_mm_67 0.0373 ms 82.0%
  triton_mm_75 0.0414 ms 74.0%
  triton_mm_71 0.0440 ms 69.6%
  triton_mm_66 0.0451 ms 67.9%
  triton_mm_72 0.0491 ms 62.4%
  triton_mm_76 0.0571 ms 53.6%
  triton_mm_73 0.0591 ms 51.8%
  triton_mm_69 0.0599 ms 51.1%
SingleProcess AUTOTUNE takes 7.5089 seconds
AUTOTUNE int_mm(32x512, 512x32000, 32x32000)
  triton_mm_616 0.0323 ms 100.0%
  triton_mm_618 0.0324 ms 99.8%
  triton_mm_617 0.0344 ms 93.7%
  triton_mm_620 0.0345 ms 93.6%
  triton_mm_625 0.0349 ms 92.4%
  triton_mm_623 0.0360 ms 89.7%
  triton_mm_622 0.0363 ms 88.9%
  triton_mm_619 0.0368 ms 87.6%
  triton_mm_626 0.0372 ms 86.9%
  triton_mm_624 0.0404 ms 80.0%
SingleProcess AUTOTUNE takes 4.0967 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:03,  8.24it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:03,  8.50it/s]running benchmark:  10%|█         | 3/30 [00:00<00:03,  8.58it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:03,  8.66it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:02,  8.67it/s]running benchmark:  20%|██        | 6/30 [00:00<00:02,  8.71it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:02,  8.67it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:02,  8.66it/s]running benchmark:  30%|███       | 9/30 [00:01<00:02,  8.69it/s]running benchmark:  33%|███▎      | 10/30 [00:01<00:02,  8.68it/s]running benchmark:  37%|███▋      | 11/30 [00:01<00:02,  8.70it/s]running benchmark:  40%|████      | 12/30 [00:01<00:02,  8.69it/s]running benchmark:  43%|████▎     | 13/30 [00:01<00:01,  8.70it/s]running benchmark:  47%|████▋     | 14/30 [00:01<00:01,  8.70it/s]running benchmark:  50%|█████     | 15/30 [00:01<00:01,  8.72it/s]running benchmark:  53%|█████▎    | 16/30 [00:01<00:01,  8.73it/s]running benchmark:  57%|█████▋    | 17/30 [00:01<00:01,  8.68it/s]running benchmark:  60%|██████    | 18/30 [00:02<00:01,  8.62it/s]running benchmark:  63%|██████▎   | 19/30 [00:02<00:01,  8.68it/s]running benchmark:  67%|██████▋   | 20/30 [00:02<00:01,  8.72it/s]running benchmark:  70%|███████   | 21/30 [00:02<00:01,  8.75it/s]running benchmark:  73%|███████▎  | 22/30 [00:02<00:00,  8.77it/s]running benchmark:  77%|███████▋  | 23/30 [00:02<00:00,  8.78it/s]running benchmark:  80%|████████  | 24/30 [00:02<00:00,  8.73it/s]running benchmark:  83%|████████▎ | 25/30 [00:02<00:00,  8.72it/s]running benchmark:  87%|████████▋ | 26/30 [00:02<00:00,  8.72it/s]running benchmark:  90%|█████████ | 27/30 [00:03<00:00,  8.74it/s]running benchmark:  93%|█████████▎| 28/30 [00:03<00:00,  8.77it/s]running benchmark:  97%|█████████▋| 29/30 [00:03<00:00,  8.78it/s]running benchmark: 100%|██████████| 30/30 [00:03<00:00,  8.78it/s]running benchmark: 100%|██████████| 30/30 [00:03<00:00,  8.71it/s]
40.923x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:llama_v2_7b_16h failed to load
Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/llama_v2_7b_16h/__init__.py", line 11, in __init__
    HuggingFaceAuthMixin.__init__(self)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 177, in __init__
    raise NotImplementedError("Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights")
NotImplementedError: Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights

loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/maml_omniglot/__init__.py", line 25, in <module>
    import higher
ModuleNotFoundError: No module named 'higher'
Failed to import user benchmark module dynamo, error: No module named 'higher'
loading model: 0it [00:00, ?it/s]Downloading: "https://download.pytorch.org/models/mnasnet1.0_top1_73.512-f206786ef8.pth" to /home/cdhernandez/.cache/torch/hub/checkpoints/mnasnet1.0_top1_73.512-f206786ef8.pth

  0%|          | 0.00/16.9M [00:00<?, ?B/s][A
 37%|███▋      | 6.33M/16.9M [00:00<00:00, 45.2MB/s][A
 63%|██████▎   | 10.6M/16.9M [00:00<00:00, 40.4MB/s][A100%|██████████| 16.9M/16.9M [00:00<00:00, 48.7MB/s]
loading model: 0it [00:02, ?it/s]
cuda eval  mnasnet1_0                         
AUTOTUNE convolution(32x3x224x224, 32x3x3x3)
  convolution 0.1094 ms 100.0%
  triton_convolution_4 0.1245 ms 87.9%
  triton_convolution_0 0.1347 ms 81.3%
  triton_convolution_3 0.1373 ms 79.7%
  triton_convolution_2 0.1518 ms 72.1%
  triton_convolution_5 0.1886 ms 58.0%
  triton_convolution_1 0.2268 ms 48.3%
SingleProcess AUTOTUNE takes 3.4360 seconds
AUTOTUNE mm(401408x32, 32x16)
  triton_mm_6 0.0348 ms 100.0%
  triton_mm_10 0.0349 ms 99.8%
  triton_mm_14 0.0351 ms 99.1%
  triton_mm_8 0.0352 ms 99.0%
  triton_mm_7 0.0352 ms 98.8%
  triton_mm_11 0.0352 ms 98.8%
  triton_mm_9 0.0353 ms 98.7%
  triton_mm_13 0.0353 ms 98.6%
  triton_mm_12 0.0356 ms 97.9%
  triton_mm_16 0.0384 ms 90.6%
SingleProcess AUTOTUNE takes 2.9006 seconds
AUTOTUNE mm(401408x16, 16x48)
  triton_mm_17 0.0425 ms 100.0%
  triton_mm_21 0.0425 ms 100.0%
  triton_mm_18 0.0426 ms 99.8%
  triton_mm_24 0.0426 ms 99.8%
  triton_mm_20 0.0427 ms 99.6%
  triton_mm_25 0.0428 ms 99.4%
  triton_mm_19 0.0429 ms 99.1%
  triton_mm_27 0.0429 ms 99.0%
  triton_mm_23 0.0440 ms 96.5%
  triton_mm_26 0.0446 ms 95.3%
SingleProcess AUTOTUNE takes 3.3278 seconds
AUTOTUNE mm(100352x48, 48x24)
  triton_mm_36 0.0194 ms 100.0%
  triton_mm_37 0.0194 ms 99.8%
  triton_mm_39 0.0202 ms 96.0%
  triton_mm_38 0.0203 ms 95.6%
  triton_mm_31 0.0205 ms 94.3%
  triton_mm_35 0.0207 ms 93.4%
  triton_mm_32 0.0210 ms 92.2%
  triton_mm_33 0.0218 ms 88.8%
  triton_mm_30 0.0222 ms 87.2%
  mm 0.0225 ms 86.1%
SingleProcess AUTOTUNE takes 3.6839 seconds
AUTOTUNE mm(100352x24, 24x72)
  triton_mm_41 0.0232 ms 100.0%
  triton_mm_43 0.0233 ms 99.5%
  triton_mm_47 0.0237 ms 97.6%
  triton_mm_42 0.0240 ms 96.4%
  triton_mm_51 0.0241 ms 96.0%
  triton_mm_40 0.0244 ms 95.0%
  triton_mm_45 0.0249 ms 93.2%
  triton_mm_44 0.0254 ms 91.2%
  triton_mm_49 0.0254 ms 91.2%
  triton_mm_48 0.0254 ms 91.1%
SingleProcess AUTOTUNE takes 3.9163 seconds
AUTOTUNE mm(100352x72, 72x24)
  triton_mm_55 0.0258 ms 100.0%
  triton_mm_57 0.0259 ms 99.8%
  triton_mm_56 0.0263 ms 98.3%
  triton_mm_62 0.0265 ms 97.3%
  triton_mm_59 0.0272 ms 94.9%
  triton_mm_63 0.0295 ms 87.6%
  triton_mm_61 0.0301 ms 85.8%
  mm 0.0303 ms 85.1%
  triton_mm_54 0.0315 ms 82.0%
  triton_mm_60 0.0374 ms 69.0%
SingleProcess AUTOTUNE takes 3.4772 seconds
AUTOTUNE mm(25088x72, 72x40)
  triton_mm_111 0.0143 ms 100.0%
  triton_mm_107 0.0151 ms 94.3%
  triton_mm_102 0.0156 ms 91.2%
  triton_mm_104 0.0160 ms 89.0%
  triton_mm_105 0.0163 ms 87.8%
  mm 0.0169 ms 84.3%
  triton_mm_109 0.0176 ms 80.9%
  triton_mm_108 0.0178 ms 80.4%
  triton_mm_100 0.0212 ms 67.5%
  triton_mm_101 0.0212 ms 67.4%
SingleProcess AUTOTUNE takes 4.0267 seconds
AUTOTUNE mm(25088x40, 40x120)
  triton_mm_120 0.0149 ms 100.0%
  triton_mm_123 0.0157 ms 94.9%
  mm 0.0162 ms 92.1%
  triton_mm_119 0.0162 ms 91.9%
  triton_mm_114 0.0165 ms 90.1%
  triton_mm_121 0.0171 ms 86.9%
  triton_mm_116 0.0173 ms 85.8%
  triton_mm_113 0.0192 ms 77.4%
  triton_mm_115 0.0204 ms 73.1%
  triton_mm_112 0.0211 ms 70.6%
SingleProcess AUTOTUNE takes 4.3054 seconds
AUTOTUNE mm(25088x120, 120x40)
  triton_mm_129 0.0182 ms 100.0%
  triton_mm_135 0.0183 ms 99.4%
  mm 0.0186 ms 98.1%
  triton_mm_131 0.0186 ms 98.1%
  triton_mm_126 0.0191 ms 95.3%
  triton_mm_128 0.0196 ms 93.1%
  triton_mm_133 0.0197 ms 92.4%
  triton_mm_132 0.0216 ms 84.2%
  triton_mm_127 0.0242 ms 75.2%
  triton_mm_124 0.0251 ms 72.5%
SingleProcess AUTOTUNE takes 4.4785 seconds
AUTOTUNE mm(25088x40, 40x240)
  triton_mm_168 0.0203 ms 100.0%
  triton_mm_162 0.0209 ms 96.9%
  mm 0.0225 ms 90.1%
  triton_mm_164 0.0226 ms 89.6%
  triton_mm_171 0.0228 ms 89.0%
  triton_mm_161 0.0236 ms 86.0%
  triton_mm_163 0.0245 ms 82.7%
  triton_mm_167 0.0248 ms 81.8%
  triton_mm_160 0.0252 ms 80.4%
  triton_mm_169 0.0264 ms 76.8%
SingleProcess AUTOTUNE takes 4.4058 seconds
AUTOTUNE mm(6272x240, 240x80)
  triton_mm_174 0.0124 ms 100.0%
  triton_mm_176 0.0127 ms 98.0%
  triton_mm_180 0.0129 ms 96.5%
  triton_mm_175 0.0131 ms 95.1%
  triton_mm_173 0.0136 ms 91.5%
  triton_mm_177 0.0140 ms 88.8%
  mm 0.0143 ms 87.0%
  triton_mm_172 0.0152 ms 81.5%
  triton_mm_181 0.0154 ms 80.5%
  triton_mm_179 0.0181 ms 68.6%
SingleProcess AUTOTUNE takes 4.7848 seconds
AUTOTUNE mm(6272x80, 80x480)
  triton_mm_191 0.0147 ms 100.0%
  triton_mm_188 0.0156 ms 94.1%
  triton_mm_186 0.0156 ms 93.9%
  triton_mm_185 0.0161 ms 91.2%
  mm 0.0162 ms 90.5%
  triton_mm_184 0.0165 ms 88.8%
  triton_mm_187 0.0166 ms 88.2%
  triton_mm_192 0.0183 ms 80.2%
  triton_mm_194 0.0194 ms 75.7%
  triton_mm_195 0.0214 ms 68.6%
SingleProcess AUTOTUNE takes 4.6539 seconds
AUTOTUNE mm(6272x480, 480x80)
  triton_mm_200 0.0149 ms 100.0%
  mm 0.0153 ms 97.7%
  triton_mm_199 0.0156 ms 95.9%
  triton_mm_197 0.0168 ms 89.1%
  triton_mm_204 0.0168 ms 89.0%
  triton_mm_198 0.0170 ms 88.1%
  triton_mm_201 0.0179 ms 83.5%
  triton_mm_205 0.0216 ms 69.3%
  triton_mm_202 0.0234 ms 64.0%
  triton_mm_196 0.0235 ms 63.6%
SingleProcess AUTOTUNE takes 4.6948 seconds
AUTOTUNE mm(6272x480, 480x96)
  mm 0.0153 ms 100.0%
  triton_mm_247 0.0154 ms 99.6%
  triton_mm_248 0.0156 ms 98.6%
  triton_mm_245 0.0166 ms 92.1%
  triton_mm_252 0.0167 ms 91.6%
  triton_mm_246 0.0171 ms 89.9%
  triton_mm_249 0.0173 ms 88.4%
  triton_mm_253 0.0211 ms 72.7%
  triton_mm_250 0.0220 ms 69.5%
  triton_mm_244 0.0235 ms 65.1%
SingleProcess AUTOTUNE takes 4.5774 seconds
AUTOTUNE mm(6272x96, 96x576)
  triton_mm_258 0.0152 ms 100.0%
  triton_mm_257 0.0163 ms 93.3%
  triton_mm_260 0.0163 ms 93.1%
  triton_mm_259 0.0168 ms 90.3%
  triton_mm_256 0.0170 ms 89.3%
  triton_mm_263 0.0191 ms 79.7%
  triton_mm_264 0.0193 ms 78.6%
  triton_mm_266 0.0209 ms 72.9%
  mm 0.0226 ms 67.2%
  triton_mm_262 0.0234 ms 64.9%
SingleProcess AUTOTUNE takes 5.3129 seconds
AUTOTUNE mm(6272x576, 576x96)
  mm 0.0162 ms 100.0%
  triton_mm_271 0.0165 ms 97.9%
  triton_mm_272 0.0168 ms 96.4%
  triton_mm_276 0.0174 ms 93.2%
  triton_mm_270 0.0188 ms 86.2%
  triton_mm_269 0.0196 ms 82.8%
  triton_mm_273 0.0198 ms 81.7%
  triton_mm_277 0.0232 ms 69.7%
  triton_mm_274 0.0254 ms 63.8%
  triton_mm_268 0.0263 ms 61.6%
SingleProcess AUTOTUNE takes 4.6819 seconds
AUTOTUNE mm(1568x576, 576x192)
  mm 0.0122 ms 100.0%
  triton_mm_300 0.0124 ms 98.7%
  triton_mm_296 0.0139 ms 87.6%
  triton_mm_295 0.0143 ms 85.2%
  triton_mm_301 0.0143 ms 85.2%
  triton_mm_297 0.0151 ms 80.9%
  triton_mm_298 0.0151 ms 80.9%
  triton_mm_293 0.0162 ms 75.1%
  triton_mm_294 0.0164 ms 74.6%
  triton_mm_292 0.0228 ms 53.4%
SingleProcess AUTOTUNE takes 4.7029 seconds
AUTOTUNE mm(1568x192, 192x1152)
  triton_mm_305 0.0137 ms 100.0%
  triton_mm_306 0.0140 ms 97.9%
  triton_mm_304 0.0150 ms 91.5%
  triton_mm_308 0.0155 ms 88.4%
  triton_mm_312 0.0155 ms 88.4%
  mm 0.0157 ms 87.3%
  triton_mm_307 0.0157 ms 87.0%
  triton_mm_311 0.0165 ms 83.1%
  triton_mm_314 0.0178 ms 77.1%
  triton_mm_309 0.0220 ms 62.4%
SingleProcess AUTOTUNE takes 4.3683 seconds
AUTOTUNE mm(1568x1152, 1152x192)
  mm 0.0143 ms 100.0%
  triton_mm_324 0.0176 ms 81.1%
  triton_mm_325 0.0197 ms 72.4%
  triton_mm_319 0.0204 ms 70.2%
  triton_mm_320 0.0207 ms 69.0%
  triton_mm_322 0.0218 ms 65.5%
  triton_mm_321 0.0219 ms 65.4%
  triton_mm_317 0.0252 ms 56.7%
  triton_mm_318 0.0262 ms 54.5%
  triton_mm_316 0.0388 ms 36.8%
SingleProcess AUTOTUNE takes 4.8313 seconds
AUTOTUNE mm(1568x1152, 1152x320)
  mm 0.0173 ms 100.0%
  triton_mm_392 0.0205 ms 84.4%
  triton_mm_391 0.0206 ms 84.3%
  triton_mm_396 0.0217 ms 80.1%
  triton_mm_389 0.0261 ms 66.5%
  triton_mm_390 0.0262 ms 66.1%
  triton_mm_393 0.0290 ms 59.8%
  triton_mm_394 0.0294 ms 58.9%
  triton_mm_397 0.0324 ms 53.6%
  triton_mm_388 0.0388 ms 44.7%
SingleProcess AUTOTUNE takes 5.0628 seconds
AUTOTUNE mm(1568x320, 320x1280)
  triton_mm_402 0.0179 ms 100.0%
  triton_mm_401 0.0179 ms 99.6%
  mm 0.0181 ms 98.6%
  triton_mm_400 0.0188 ms 94.9%
  triton_mm_403 0.0194 ms 92.1%
  triton_mm_404 0.0196 ms 91.2%
  triton_mm_408 0.0201 ms 88.9%
  triton_mm_407 0.0214 ms 83.3%
  triton_mm_410 0.0263 ms 67.8%
  triton_mm_406 0.0308 ms 58.0%
SingleProcess AUTOTUNE takes 4.3241 seconds
AUTOTUNE int_mm(32x1280, 1280x1000, 32x1000)
  triton_mm_422 0.0144 ms 100.0%
  triton_mm_417 0.0162 ms 88.9%
  triton_mm_420 0.0164 ms 88.1%
  triton_mm_421 0.0166 ms 86.5%
  triton_mm_418 0.0170 ms 84.9%
  triton_mm_416 0.0188 ms 76.5%
  triton_mm_415 0.0213 ms 67.7%
  triton_mm_414 0.0238 ms 60.6%
  triton_mm_413 0.0265 ms 54.3%
  triton_mm_412 0.0353 ms 40.8%
SingleProcess AUTOTUNE takes 4.5024 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 86.24it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 96.21it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 96.48it/s]
3.986x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  mobilenet_v2                       
AUTOTUNE convolution(16x3x224x224, 32x3x3x3)
  convolution 0.0597 ms 100.0%
  triton_convolution_4 0.0664 ms 90.0%
  triton_convolution_0 0.0719 ms 83.1%
  triton_convolution_2 0.0723 ms 82.6%
  triton_convolution_3 0.0737 ms 81.0%
  triton_convolution_5 0.1022 ms 58.4%
  triton_convolution_1 0.1192 ms 50.1%
SingleProcess AUTOTUNE takes 2.8637 seconds
AUTOTUNE mm(200704x32, 32x16)
  triton_mm_6 0.0214 ms 100.0%
  triton_mm_8 0.0215 ms 99.6%
  triton_mm_13 0.0216 ms 99.2%
  triton_mm_14 0.0216 ms 99.2%
  triton_mm_9 0.0218 ms 98.5%
  triton_mm_11 0.0218 ms 98.5%
  triton_mm_7 0.0218 ms 98.2%
  triton_mm_10 0.0219 ms 97.7%
  triton_mm_12 0.0219 ms 97.7%
  triton_mm_16 0.0236 ms 90.7%
SingleProcess AUTOTUNE takes 2.9794 seconds
AUTOTUNE mm(200704x16, 16x96)
  triton_mm_26 0.0356 ms 100.0%
  triton_mm_27 0.0358 ms 99.3%
  triton_mm_22 0.0360 ms 98.8%
  triton_mm_17 0.0364 ms 97.6%
  triton_mm_21 0.0370 ms 96.1%
  triton_mm_25 0.0373 ms 95.3%
  triton_mm_24 0.0376 ms 94.6%
  triton_mm_19 0.0378 ms 94.2%
  triton_mm_20 0.0379 ms 93.9%
  triton_mm_18 0.0381 ms 93.4%
SingleProcess AUTOTUNE takes 3.2168 seconds
AUTOTUNE mm(50176x96, 96x24)
  triton_mm_31 0.0166 ms 100.0%
  triton_mm_35 0.0168 ms 99.0%
  triton_mm_33 0.0172 ms 96.5%
  triton_mm_28 0.0176 ms 94.4%
  triton_mm_38 0.0177 ms 93.9%
  triton_mm_32 0.0181 ms 92.0%
  triton_mm_39 0.0184 ms 90.4%
  triton_mm_29 0.0188 ms 88.4%
  triton_mm_30 0.0192 ms 86.8%
  triton_mm_36 0.0192 ms 86.5%
SingleProcess AUTOTUNE takes 3.8802 seconds
AUTOTUNE mm(50176x24, 24x144)
  triton_mm_42 0.0202 ms 100.0%
  triton_mm_44 0.0207 ms 97.8%
  triton_mm_40 0.0208 ms 97.1%
  triton_mm_48 0.0210 ms 96.3%
  triton_mm_51 0.0216 ms 93.5%
  triton_mm_45 0.0219 ms 92.3%
  triton_mm_49 0.0223 ms 90.8%
  triton_mm_41 0.0229 ms 88.3%
  triton_mm_46 0.0236 ms 85.5%
  triton_mm_47 0.0238 ms 84.8%
SingleProcess AUTOTUNE takes 3.8561 seconds
AUTOTUNE mm(50176x144, 144x24)
  triton_mm_63 0.0231 ms 100.0%
  triton_mm_62 0.0232 ms 99.4%
  triton_mm_52 0.0236 ms 97.7%
  triton_mm_59 0.0237 ms 97.6%
  triton_mm_54 0.0246 ms 93.9%
  triton_mm_53 0.0246 ms 93.8%
  triton_mm_56 0.0250 ms 92.6%
  triton_mm_57 0.0252 ms 91.7%
  triton_mm_60 0.0259 ms 89.4%
  triton_mm_55 0.0261 ms 88.6%
SingleProcess AUTOTUNE takes 3.6281 seconds
AUTOTUNE mm(12544x144, 144x32)
  triton_mm_78 0.0116 ms 100.0%
  triton_mm_77 0.0116 ms 99.7%
  triton_mm_79 0.0117 ms 99.2%
  triton_mm_81 0.0118 ms 98.6%
  triton_mm_80 0.0118 ms 98.1%
  triton_mm_84 0.0122 ms 95.3%
  triton_mm_82 0.0123 ms 94.5%
  triton_mm_83 0.0124 ms 93.3%
  triton_mm_85 0.0126 ms 92.4%
  triton_mm_76 0.0127 ms 91.7%
SingleProcess AUTOTUNE takes 4.3348 seconds
AUTOTUNE mm(12544x32, 32x192)
  triton_mm_94 0.0109 ms 100.0%
  triton_mm_92 0.0109 ms 99.7%
  triton_mm_96 0.0111 ms 98.3%
  triton_mm_88 0.0111 ms 98.0%
  triton_mm_93 0.0114 ms 95.8%
  triton_mm_97 0.0116 ms 94.2%
  triton_mm_91 0.0117 ms 93.2%
  triton_mm_90 0.0118 ms 92.7%
  triton_mm_95 0.0120 ms 91.2%
  triton_mm_99 0.0123 ms 88.8%
SingleProcess AUTOTUNE takes 3.9073 seconds
AUTOTUNE mm(12544x192, 192x32)
  triton_mm_108 0.0113 ms 100.0%
  triton_mm_101 0.0114 ms 99.7%
  triton_mm_102 0.0116 ms 97.5%
  triton_mm_105 0.0118 ms 95.9%
  triton_mm_104 0.0121 ms 93.7%
  triton_mm_106 0.0122 ms 92.9%
  triton_mm_103 0.0125 ms 90.3%
  triton_mm_109 0.0126 ms 89.6%
  triton_mm_100 0.0136 ms 83.3%
  triton_mm_107 0.0141 ms 80.3%
SingleProcess AUTOTUNE takes 3.5625 seconds
AUTOTUNE mm(3136x192, 192x64)
  triton_mm_153 0.0087 ms 100.0%
  triton_mm_154 0.0090 ms 97.5%
  triton_mm_156 0.0090 ms 97.5%
  triton_mm_157 0.0092 ms 95.1%
  triton_mm_151 0.0095 ms 91.9%
  triton_mm_149 0.0100 ms 87.8%
  triton_mm_150 0.0101 ms 86.5%
  triton_mm_152 0.0103 ms 84.5%
  mm 0.0104 ms 84.0%
  triton_mm_148 0.0119 ms 73.4%
SingleProcess AUTOTUNE takes 4.0883 seconds
AUTOTUNE mm(3136x64, 64x384)
  triton_mm_162 0.0092 ms 100.0%
  triton_mm_160 0.0094 ms 98.0%
  triton_mm_161 0.0094 ms 98.0%
  triton_mm_163 0.0099 ms 93.2%
  triton_mm_168 0.0103 ms 89.8%
  mm 0.0104 ms 89.2%
  triton_mm_164 0.0104 ms 88.8%
  triton_mm_167 0.0105 ms 88.1%
  triton_mm_170 0.0109 ms 84.8%
  triton_mm_169 0.0112 ms 82.6%
SingleProcess AUTOTUNE takes 4.3144 seconds
AUTOTUNE mm(3136x384, 384x64)
  triton_mm_177 0.0112 ms 100.0%
  triton_mm_178 0.0112 ms 99.4%
  mm 0.0115 ms 97.5%
  triton_mm_180 0.0116 ms 96.7%
  triton_mm_175 0.0116 ms 96.4%
  triton_mm_181 0.0116 ms 96.1%
  triton_mm_176 0.0125 ms 89.5%
  triton_mm_173 0.0128 ms 87.3%
  triton_mm_174 0.0139 ms 80.6%
  triton_mm_172 0.0173 ms 64.4%
SingleProcess AUTOTUNE takes 4.1292 seconds
AUTOTUNE mm(3136x384, 384x96)
  triton_mm_252 0.0117 ms 100.0%
  mm 0.0121 ms 97.1%
  triton_mm_248 0.0122 ms 96.3%
  triton_mm_249 0.0123 ms 95.3%
  triton_mm_253 0.0124 ms 94.3%
  triton_mm_247 0.0125 ms 94.1%
  triton_mm_250 0.0132 ms 88.9%
  triton_mm_245 0.0138 ms 84.9%
  triton_mm_246 0.0140 ms 84.2%
  triton_mm_244 0.0181 ms 64.8%
SingleProcess AUTOTUNE takes 4.8342 seconds
AUTOTUNE mm(3136x96, 96x576)
  triton_mm_256 0.0117 ms 100.0%
  triton_mm_258 0.0119 ms 98.4%
  triton_mm_260 0.0120 ms 97.6%
  triton_mm_257 0.0122 ms 96.1%
  triton_mm_259 0.0127 ms 92.4%
  triton_mm_263 0.0129 ms 91.3%
  triton_mm_264 0.0144 ms 81.8%
  mm 0.0146 ms 80.7%
  triton_mm_266 0.0149 ms 78.6%
  triton_mm_262 0.0160 ms 73.3%
SingleProcess AUTOTUNE takes 4.6624 seconds
AUTOTUNE mm(3136x576, 576x96)
  triton_mm_276 0.0132 ms 100.0%
  mm 0.0133 ms 99.0%
  triton_mm_272 0.0145 ms 90.9%
  triton_mm_271 0.0148 ms 88.6%
  triton_mm_273 0.0153 ms 86.1%
  triton_mm_277 0.0154 ms 85.3%
  triton_mm_274 0.0169 ms 78.0%
  triton_mm_270 0.0170 ms 77.5%
  triton_mm_269 0.0172 ms 76.3%
  triton_mm_268 0.0237 ms 55.5%
SingleProcess AUTOTUNE takes 5.0112 seconds
AUTOTUNE mm(784x576, 576x160)
  triton_mm_321 0.0109 ms 100.0%
  triton_mm_322 0.0110 ms 98.8%
  mm 0.0112 ms 97.7%
  triton_mm_324 0.0117 ms 93.2%
  triton_mm_325 0.0125 ms 87.4%
  triton_mm_319 0.0138 ms 78.8%
  triton_mm_320 0.0140 ms 77.7%
  triton_mm_317 0.0157 ms 69.3%
  triton_mm_318 0.0161 ms 67.7%
  triton_mm_316 0.0219 ms 49.9%
SingleProcess AUTOTUNE takes 5.1167 seconds
AUTOTUNE mm(784x160, 160x960)
  triton_mm_331 0.0093 ms 100.0%
  triton_mm_332 0.0094 ms 98.3%
  mm 0.0099 ms 93.5%
  triton_mm_329 0.0100 ms 92.8%
  triton_mm_330 0.0101 ms 91.5%
  triton_mm_336 0.0105 ms 88.7%
  triton_mm_328 0.0113 ms 82.4%
  triton_mm_338 0.0126 ms 73.6%
  triton_mm_334 0.0130 ms 71.4%
  triton_mm_333 0.0132 ms 70.2%
SingleProcess AUTOTUNE takes 4.2807 seconds
AUTOTUNE mm(784x960, 960x160)
  mm 0.0124 ms 100.0%
  triton_mm_346 0.0141 ms 87.7%
  triton_mm_345 0.0143 ms 86.4%
  triton_mm_348 0.0153 ms 80.8%
  triton_mm_349 0.0153 ms 80.8%
  triton_mm_343 0.0177 ms 69.8%
  triton_mm_344 0.0180 ms 68.8%
  triton_mm_341 0.0218 ms 56.6%
  triton_mm_342 0.0228 ms 54.1%
  triton_mm_340 0.0332 ms 37.2%
SingleProcess AUTOTUNE takes 4.5824 seconds
AUTOTUNE mm(784x960, 960x320)
  mm 0.0124 ms 100.0%
  triton_mm_396 0.0153 ms 81.0%
  triton_mm_397 0.0176 ms 70.7%
  triton_mm_391 0.0178 ms 69.7%
  triton_mm_392 0.0184 ms 67.5%
  triton_mm_393 0.0192 ms 64.7%
  triton_mm_394 0.0194 ms 64.1%
  triton_mm_389 0.0224 ms 55.3%
  triton_mm_390 0.0225 ms 55.2%
  triton_mm_388 0.0323 ms 38.5%
SingleProcess AUTOTUNE takes 4.9905 seconds
AUTOTUNE mm(784x320, 320x1280)
  triton_mm_401 0.0135 ms 100.0%
  mm 0.0141 ms 95.9%
  triton_mm_402 0.0143 ms 94.4%
  triton_mm_408 0.0151 ms 89.6%
  triton_mm_403 0.0152 ms 89.0%
  triton_mm_404 0.0154 ms 87.8%
  triton_mm_400 0.0167 ms 80.8%
  triton_mm_405 0.0191 ms 70.8%
  triton_mm_410 0.0197 ms 68.4%
  triton_mm_406 0.0198 ms 68.2%
SingleProcess AUTOTUNE takes 5.0079 seconds
AUTOTUNE int_mm(16x1280, 1280x1000, 16x1000)
  triton_mm_422 0.0126 ms 100.0%
  triton_mm_417 0.0149 ms 84.6%
  triton_mm_421 0.0151 ms 83.7%
  triton_mm_420 0.0156 ms 81.3%
  triton_mm_418 0.0158 ms 80.1%
  triton_mm_416 0.0162 ms 77.9%
  triton_mm_414 0.0225 ms 56.1%
  triton_mm_413 0.0248 ms 51.0%
  triton_mm_412 0.0324 ms 39.0%
  triton_mm_415 0.0349 ms 36.2%
SingleProcess AUTOTUNE takes 3.9496 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  20%|██        | 6/30 [00:00<00:00, 55.07it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 60.54it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 62.26it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 63.10it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 62.25it/s]
9.048x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:mobilenet_v2_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/mobilenet_v2_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  mobilenet_v3_large                 
AUTOTUNE convolution(32x3x224x224, 16x3x3x3)
  triton_convolution_4 0.0470 ms 100.0%
  triton_convolution_3 0.0485 ms 96.8%
  triton_convolution_0 0.0520 ms 90.4%
  triton_convolution_1 0.0524 ms 89.7%
  triton_convolution_2 0.0777 ms 60.5%
  convolution 0.1009 ms 46.6%
SingleProcess AUTOTUNE takes 1.9735 seconds
AUTOTUNE mm(401408x16, 16x16)
  triton_mm_7 0.0244 ms 100.0%
  triton_mm_5 0.0246 ms 99.5%
  triton_mm_13 0.0248 ms 98.7%
  triton_mm_11 0.0248 ms 98.6%
  triton_mm_14 0.0248 ms 98.6%
  triton_mm_9 0.0249 ms 98.3%
  triton_mm_10 0.0249 ms 98.1%
  triton_mm_6 0.0249 ms 98.1%
  triton_mm_8 0.0250 ms 97.9%
  triton_mm_12 0.0250 ms 97.9%
SingleProcess AUTOTUNE takes 2.7599 seconds
AUTOTUNE mm(401408x16, 16x64)
  triton_mm_25 0.0484 ms 100.0%
  triton_mm_20 0.0488 ms 99.1%
  triton_mm_24 0.0489 ms 99.0%
  triton_mm_21 0.0505 ms 95.8%
  triton_mm_15 0.0507 ms 95.5%
  triton_mm_19 0.0509 ms 95.1%
  triton_mm_22 0.0509 ms 95.0%
  triton_mm_16 0.0511 ms 94.7%
  triton_mm_23 0.0512 ms 94.4%
  triton_mm_17 0.0515 ms 94.0%
SingleProcess AUTOTUNE takes 2.9408 seconds
AUTOTUNE mm(100352x64, 64x24)
  triton_mm_35 0.0217 ms 100.0%
  triton_mm_34 0.0220 ms 98.3%
  triton_mm_28 0.0224 ms 96.8%
  triton_mm_29 0.0232 ms 93.5%
  triton_mm_27 0.0232 ms 93.4%
  triton_mm_30 0.0236 ms 91.7%
  triton_mm_36 0.0238 ms 90.9%
  triton_mm_31 0.0238 ms 90.9%
  triton_mm_33 0.0240 ms 90.3%
  triton_mm_26 0.0253 ms 85.7%
SingleProcess AUTOTUNE takes 3.5038 seconds
AUTOTUNE addmm(32x24, 32x72, 72x24)
  triton_mm_76 0.0071 ms 100.0%
  bias_addmm 0.0076 ms 93.7%
  triton_mm_77 0.0076 ms 92.9%
  triton_mm_79 0.0078 ms 91.4%
  triton_mm_75 0.0078 ms 91.0%
  triton_mm_78 0.0079 ms 90.2%
  triton_mm_74 0.0083 ms 85.4%
  triton_mm_80 0.0084 ms 84.1%
  triton_mm_81 0.0087 ms 81.9%
  addmm 0.0127 ms 55.8%
SingleProcess AUTOTUNE takes 2.4592 seconds
AUTOTUNE addmm(32x72, 32x24, 24x72)
  triton_mm_87 0.0062 ms 100.0%
  triton_mm_91 0.0062 ms 99.5%
  triton_mm_84 0.0064 ms 96.5%
  triton_mm_86 0.0064 ms 96.0%
  triton_mm_88 0.0064 ms 96.0%
  triton_mm_90 0.0064 ms 96.0%
  triton_mm_85 0.0069 ms 89.8%
  triton_mm_82 0.0069 ms 88.9%
  triton_mm_83 0.0071 ms 86.9%
  triton_mm_93 0.0073 ms 84.3%
SingleProcess AUTOTUNE takes 3.6586 seconds
AUTOTUNE addmm(32x32, 32x120, 120x32)
  triton_mm_123 0.0073 ms 100.0%
  triton_mm_122 0.0073 ms 99.6%
  triton_mm_119 0.0075 ms 97.4%
  triton_mm_120 0.0079 ms 92.7%
  triton_mm_121 0.0079 ms 92.7%
  bias_addmm 0.0080 ms 91.2%
  triton_mm_118 0.0092 ms 79.2%
  triton_mm_124 0.0103 ms 70.8%
  triton_mm_125 0.0111 ms 65.9%
  addmm 0.0128 ms 57.1%
SingleProcess AUTOTUNE takes 2.5950 seconds
AUTOTUNE addmm(32x120, 32x32, 32x120)
  triton_mm_131 0.0063 ms 100.0%
  triton_mm_130 0.0066 ms 96.1%
  triton_mm_132 0.0066 ms 95.7%
  triton_mm_135 0.0069 ms 92.3%
  triton_mm_129 0.0069 ms 92.1%
  triton_mm_128 0.0070 ms 90.2%
  triton_mm_136 0.0071 ms 89.2%
  triton_mm_126 0.0071 ms 89.0%
  triton_mm_134 0.0072 ms 88.6%
  triton_mm_133 0.0076 ms 83.2%
SingleProcess AUTOTUNE takes 3.5478 seconds
AUTOTUNE mm(6272x80, 80x200)
  triton_mm_220 0.0114 ms 100.0%
  triton_mm_221 0.0116 ms 98.3%
  triton_mm_222 0.0116 ms 98.3%
  triton_mm_219 0.0120 ms 94.9%
  triton_mm_218 0.0121 ms 93.7%
  mm 0.0124 ms 91.4%
  triton_mm_225 0.0125 ms 90.6%
  triton_mm_226 0.0133 ms 85.1%
  triton_mm_229 0.0139 ms 81.8%
  triton_mm_228 0.0141 ms 80.7%
SingleProcess AUTOTUNE takes 4.4765 seconds
AUTOTUNE mm(6272x200, 200x80)
  triton_mm_232 0.0124 ms 100.0%
  triton_mm_234 0.0130 ms 95.0%
  triton_mm_233 0.0133 ms 92.8%
  triton_mm_235 0.0136 ms 90.6%
  triton_mm_231 0.0138 ms 89.5%
  triton_mm_238 0.0138 ms 89.4%
  mm 0.0145 ms 85.2%
  triton_mm_230 0.0146 ms 84.8%
  triton_mm_237 0.0168 ms 73.5%
  triton_mm_241 0.0173 ms 71.5%
SingleProcess AUTOTUNE takes 4.5103 seconds
AUTOTUNE mm(6272x80, 80x184)
  triton_mm_244 0.0108 ms 100.0%
  triton_mm_250 0.0116 ms 93.9%
  triton_mm_242 0.0116 ms 93.1%
  triton_mm_246 0.0119 ms 91.4%
  mm 0.0121 ms 89.9%
  triton_mm_243 0.0121 ms 89.4%
  triton_mm_249 0.0121 ms 89.4%
  triton_mm_245 0.0123 ms 88.3%
  triton_mm_253 0.0130 ms 83.5%
  triton_mm_252 0.0132 ms 82.5%
SingleProcess AUTOTUNE takes 4.3507 seconds
AUTOTUNE mm(6272x184, 184x80)
  triton_mm_256 0.0131 ms 100.0%
  triton_mm_259 0.0136 ms 96.2%
  triton_mm_258 0.0138 ms 94.9%
  mm 0.0141 ms 93.2%
  triton_mm_262 0.0149 ms 88.2%
  triton_mm_257 0.0156 ms 83.8%
  triton_mm_255 0.0159 ms 82.5%
  triton_mm_265 0.0160 ms 82.0%
  triton_mm_261 0.0161 ms 81.5%
  triton_mm_254 0.0161 ms 81.3%
SingleProcess AUTOTUNE takes 4.5037 seconds
AUTOTUNE addmm(32x120, 32x480, 480x120)
  triton_mm_307 0.0095 ms 100.0%
  triton_mm_308 0.0103 ms 92.5%
  bias_addmm 0.0108 ms 88.4%
  triton_mm_306 0.0110 ms 86.3%
  triton_mm_310 0.0110 ms 86.1%
  triton_mm_311 0.0110 ms 86.1%
  triton_mm_305 0.0126 ms 75.6%
  triton_mm_304 0.0128 ms 74.4%
  triton_mm_303 0.0132 ms 71.7%
  addmm 0.0140 ms 67.7%
SingleProcess AUTOTUNE takes 4.6410 seconds
AUTOTUNE addmm(32x480, 32x120, 120x480)
  triton_mm_323 0.0076 ms 100.0%
  triton_mm_318 0.0080 ms 94.4%
  triton_mm_319 0.0080 ms 94.0%
  bias_addmm 0.0083 ms 90.8%
  triton_mm_322 0.0084 ms 90.4%
  triton_mm_316 0.0085 ms 88.4%
  triton_mm_320 0.0085 ms 88.4%
  triton_mm_317 0.0087 ms 86.8%
  triton_mm_315 0.0090 ms 83.7%
  triton_mm_314 0.0099 ms 76.1%
SingleProcess AUTOTUNE takes 3.9506 seconds
AUTOTUNE mm(6272x480, 480x112)
  triton_mm_330 0.0149 ms 100.0%
  triton_mm_329 0.0149 ms 99.8%
  mm 0.0155 ms 96.2%
  triton_mm_334 0.0163 ms 91.4%
  triton_mm_327 0.0166 ms 89.6%
  triton_mm_328 0.0170 ms 87.7%
  triton_mm_331 0.0203 ms 73.3%
  triton_mm_335 0.0211 ms 70.7%
  triton_mm_332 0.0212 ms 70.2%
  triton_mm_326 0.0228 ms 65.1%
SingleProcess AUTOTUNE takes 5.2682 seconds
AUTOTUNE mm(6272x112, 112x672)
  triton_mm_340 0.0191 ms 100.0%
  triton_mm_339 0.0196 ms 97.5%
  triton_mm_338 0.0210 ms 91.1%
  triton_mm_345 0.0220 ms 87.1%
  triton_mm_342 0.0221 ms 86.6%
  triton_mm_346 0.0222 ms 86.2%
  triton_mm_341 0.0226 ms 84.8%
  mm 0.0249 ms 76.7%
  triton_mm_348 0.0255 ms 74.9%
  triton_mm_347 0.0310 ms 61.7%
SingleProcess AUTOTUNE takes 4.4462 seconds
AUTOTUNE addmm(32x168, 32x672, 672x168)
  triton_mm_355 0.0107 ms 100.0%
  bias_addmm 0.0112 ms 95.3%
  triton_mm_356 0.0122 ms 88.2%
  triton_mm_358 0.0124 ms 86.3%
  triton_mm_354 0.0125 ms 85.5%
  triton_mm_359 0.0126 ms 85.2%
  triton_mm_353 0.0137 ms 78.3%
  addmm 0.0148 ms 72.4%
  triton_mm_352 0.0149 ms 71.9%
  triton_mm_351 0.0157 ms 68.4%
SingleProcess AUTOTUNE takes 4.1926 seconds
AUTOTUNE addmm(32x672, 32x168, 168x672)
  triton_mm_367 0.0078 ms 100.0%
  triton_mm_370 0.0082 ms 94.6%
  triton_mm_368 0.0084 ms 92.0%
  triton_mm_364 0.0087 ms 89.3%
  triton_mm_371 0.0087 ms 89.0%
  bias_addmm 0.0090 ms 86.8%
  triton_mm_366 0.0092 ms 84.1%
  triton_mm_365 0.0093 ms 83.2%
  triton_mm_363 0.0094 ms 82.9%
  triton_mm_362 0.0112 ms 69.6%
SingleProcess AUTOTUNE takes 4.0890 seconds
AUTOTUNE mm(6272x672, 672x112)
  triton_mm_377 0.0180 ms 100.0%
  triton_mm_378 0.0181 ms 99.1%
  mm 0.0181 ms 98.9%
  triton_mm_382 0.0187 ms 96.2%
  triton_mm_375 0.0206 ms 87.0%
  triton_mm_376 0.0207 ms 86.6%
  triton_mm_379 0.0250 ms 71.8%
  triton_mm_380 0.0260 ms 69.2%
  triton_mm_383 0.0270 ms 66.5%
  triton_mm_374 0.0298 ms 60.3%
SingleProcess AUTOTUNE takes 4.6208 seconds
AUTOTUNE mm(1568x672, 672x160)
  mm 0.0122 ms 100.0%
  triton_mm_430 0.0139 ms 87.4%
  triton_mm_431 0.0152 ms 79.8%
  triton_mm_428 0.0153 ms 79.5%
  triton_mm_425 0.0155 ms 78.6%
  triton_mm_426 0.0155 ms 78.6%
  triton_mm_427 0.0156 ms 77.9%
  triton_mm_423 0.0174 ms 69.7%
  triton_mm_424 0.0176 ms 69.1%
  triton_mm_422 0.0257 ms 47.4%
SingleProcess AUTOTUNE takes 4.5823 seconds
AUTOTUNE mm(1568x160, 160x960)
  triton_mm_436 0.0113 ms 100.0%
  triton_mm_435 0.0116 ms 97.2%
  triton_mm_437 0.0118 ms 95.1%
  triton_mm_438 0.0124 ms 90.5%
  mm 0.0125 ms 90.0%
  triton_mm_434 0.0128 ms 88.2%
  triton_mm_441 0.0142 ms 79.1%
  triton_mm_442 0.0150 ms 75.2%
  triton_mm_444 0.0154 ms 73.3%
  triton_mm_440 0.0172 ms 65.3%
SingleProcess AUTOTUNE takes 4.6646 seconds
AUTOTUNE addmm(32x240, 32x960, 960x240)
  bias_addmm 0.0127 ms 100.0%
  triton_mm_451 0.0128 ms 99.2%
  triton_mm_452 0.0139 ms 91.0%
  triton_mm_454 0.0146 ms 86.7%
  triton_mm_455 0.0148 ms 85.7%
  triton_mm_450 0.0156 ms 81.3%
  addmm 0.0160 ms 79.4%
  triton_mm_449 0.0169 ms 75.1%
  triton_mm_448 0.0192 ms 65.9%
  triton_mm_447 0.0208 ms 60.9%
SingleProcess AUTOTUNE takes 4.7262 seconds
AUTOTUNE addmm(32x960, 32x240, 240x960)
  triton_mm_467 0.0087 ms 100.0%
  triton_mm_466 0.0089 ms 97.5%
  triton_mm_463 0.0090 ms 96.8%
  triton_mm_464 0.0097 ms 90.1%
  bias_addmm 0.0097 ms 89.5%
  triton_mm_462 0.0099 ms 87.7%
  triton_mm_461 0.0100 ms 86.6%
  triton_mm_460 0.0101 ms 85.8%
  triton_mm_459 0.0105 ms 83.2%
  triton_mm_458 0.0130 ms 66.8%
SingleProcess AUTOTUNE takes 4.3418 seconds
AUTOTUNE mm(1568x960, 960x160)
  mm 0.0130 ms 100.0%
  triton_mm_478 0.0164 ms 79.2%
  triton_mm_479 0.0182 ms 71.3%
  triton_mm_474 0.0183 ms 70.8%
  triton_mm_473 0.0186 ms 69.7%
  triton_mm_475 0.0194 ms 66.8%
  triton_mm_476 0.0196 ms 66.1%
  triton_mm_472 0.0227 ms 57.2%
  triton_mm_471 0.0227 ms 57.2%
  triton_mm_470 0.0342 ms 37.9%
SingleProcess AUTOTUNE takes 4.9167 seconds
AUTOTUNE int_mm(32x960, 960x1280, 32x1280)
  triton_mm_552 0.0125 ms 100.0%
  triton_mm_547 0.0138 ms 90.6%
  triton_mm_550 0.0140 ms 89.7%
  triton_mm_548 0.0147 ms 85.4%
  triton_mm_546 0.0153 ms 82.2%
  triton_mm_551 0.0158 ms 79.4%
  triton_mm_545 0.0176 ms 71.3%
  triton_mm_544 0.0199 ms 62.9%
  triton_mm_543 0.0220 ms 57.1%
  triton_mm_542 0.0282 ms 44.4%
SingleProcess AUTOTUNE takes 4.0825 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 67.04it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 74.03it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 76.39it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 75.94it/s]
5.874x
loading model: 0it [00:00, ?it/s]NCCL version 2.19.3+cuda12.0
loading model: 0it [00:02, ?it/s]
cuda eval  moco                               
ERROR:common:Backend eager failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/moco/moco/builder.py", line 130, in forward
    self._momentum_update_key_encoder()  # update the key encoder
  File "/home/cdhernandez/local/pytorch/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/moco/moco/builder.py", line 50, in _momentum_update_key_encoder
    param_k.mul_(self.m).add_(param_q.mul(1. - self.m))
TypeError: add_(): argument 'other' (position 1) must be Tensor, not NoneType
Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
number of parameters: 123.69M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: True
cuda eval  nanogpt                            
AUTOTUNE int_mm(64x768, 768x2304, 64x2304)
  triton_mm_10 0.0129 ms 100.0%
  triton_mm_8 0.0138 ms 93.5%
  triton_mm_6 0.0139 ms 92.8%
  triton_mm_5 0.0147 ms 87.4%
  triton_mm_4 0.0158 ms 81.4%
  triton_mm_9 0.0160 ms 80.6%
  triton_mm_3 0.0175 ms 73.4%
  triton_mm_2 0.0180 ms 71.5%
  triton_mm_1 0.0200 ms 64.3%
  triton_mm_0 0.0238 ms 54.0%
SingleProcess AUTOTUNE takes 5.1985 seconds
AUTOTUNE int_mm(64x768, 768x768, 64x768)
  triton_mm_16 0.0126 ms 100.0%
  triton_mm_21 0.0127 ms 99.2%
  triton_mm_19 0.0128 ms 98.5%
  triton_mm_17 0.0132 ms 95.4%
  triton_mm_15 0.0153 ms 82.5%
  triton_mm_20 0.0163 ms 77.5%
  triton_mm_14 0.0173 ms 73.0%
  triton_mm_13 0.0177 ms 71.6%
  triton_mm_12 0.0192 ms 65.9%
  triton_mm_11 0.0238 ms 53.2%
SingleProcess AUTOTUNE takes 5.2859 seconds
AUTOTUNE int_mm(64x768, 768x3072, 64x3072)
  triton_mm_30 0.0136 ms 100.0%
  triton_mm_32 0.0137 ms 99.3%
  triton_mm_28 0.0146 ms 93.2%
  triton_mm_27 0.0156 ms 86.7%
  triton_mm_26 0.0161 ms 84.4%
  triton_mm_31 0.0162 ms 84.0%
  triton_mm_25 0.0176 ms 77.2%
  triton_mm_24 0.0182 ms 74.4%
  triton_mm_23 0.0206 ms 65.8%
  triton_mm_22 0.0238 ms 57.0%
SingleProcess AUTOTUNE takes 5.5870 seconds
AUTOTUNE int_mm(64x3072, 3072x768, 64x768)
  triton_mm_43 0.0245 ms 100.0%
  triton_mm_38 0.0295 ms 83.0%
  triton_mm_39 0.0301 ms 81.2%
  triton_mm_41 0.0306 ms 79.9%
  triton_mm_42 0.0313 ms 78.3%
  triton_mm_37 0.0374 ms 65.4%
  triton_mm_36 0.0424 ms 57.8%
  triton_mm_35 0.0498 ms 49.2%
  triton_mm_34 0.0544 ms 45.0%
  triton_mm_33 0.0786 ms 31.2%
SingleProcess AUTOTUNE takes 5.0175 seconds
AUTOTUNE int_mm(1x768, 768x50304, 1x50304)
  triton_mm_538 0.0448 ms 100.0%
  triton_mm_537 0.0454 ms 98.7%
  triton_mm_536 0.0471 ms 95.1%
  triton_mm_533 0.0531 ms 84.4%
  triton_mm_534 0.0536 ms 83.5%
  triton_mm_532 0.0544 ms 82.4%
  triton_mm_529 0.0545 ms 82.3%
  triton_mm_531 0.0558 ms 80.3%
  triton_mm_530 0.0559 ms 80.1%
  triton_mm_528 0.0560 ms 80.0%
SingleProcess AUTOTUNE takes 3.8895 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:05,  5.54it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:04,  5.72it/s]running benchmark:  10%|█         | 3/30 [00:00<00:04,  5.87it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:04,  5.91it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:04,  6.01it/s]running benchmark:  20%|██        | 6/30 [00:01<00:03,  6.07it/s]running benchmark:  23%|██▎       | 7/30 [00:01<00:03,  6.04it/s]running benchmark:  27%|██▋       | 8/30 [00:01<00:03,  6.06it/s]running benchmark:  30%|███       | 9/30 [00:01<00:03,  6.08it/s]running benchmark:  33%|███▎      | 10/30 [00:01<00:03,  6.11it/s]running benchmark:  37%|███▋      | 11/30 [00:01<00:03,  6.11it/s]running benchmark:  40%|████      | 12/30 [00:01<00:02,  6.14it/s]running benchmark:  43%|████▎     | 13/30 [00:02<00:02,  6.13it/s]running benchmark:  47%|████▋     | 14/30 [00:02<00:02,  6.10it/s]running benchmark:  50%|█████     | 15/30 [00:02<00:02,  6.12it/s]running benchmark:  53%|█████▎    | 16/30 [00:02<00:02,  6.15it/s]running benchmark:  57%|█████▋    | 17/30 [00:02<00:02,  6.13it/s]running benchmark:  60%|██████    | 18/30 [00:02<00:01,  6.15it/s]running benchmark:  63%|██████▎   | 19/30 [00:03<00:01,  6.16it/s]running benchmark:  67%|██████▋   | 20/30 [00:03<00:01,  6.17it/s]running benchmark:  70%|███████   | 21/30 [00:03<00:01,  6.19it/s]running benchmark:  73%|███████▎  | 22/30 [00:03<00:01,  6.20it/s]running benchmark:  77%|███████▋  | 23/30 [00:03<00:01,  6.20it/s]running benchmark:  80%|████████  | 24/30 [00:03<00:00,  6.19it/s]running benchmark:  83%|████████▎ | 25/30 [00:04<00:00,  6.16it/s]running benchmark:  87%|████████▋ | 26/30 [00:04<00:00,  6.19it/s]running benchmark:  90%|█████████ | 27/30 [00:04<00:00,  6.17it/s]running benchmark:  93%|█████████▎| 28/30 [00:04<00:00,  6.16it/s]running benchmark:  97%|█████████▋| 29/30 [00:04<00:00,  6.18it/s]running benchmark: 100%|██████████| 30/30 [00:04<00:00,  6.19it/s]running benchmark: 100%|██████████| 30/30 [00:04<00:00,  6.11it/s]
98.857x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  nvidia_deeprecommender             
AUTOTUNE mm(256x197951, 197951x512)
  mm 1.1545 ms 100.0%
  triton_mm_9 2.5703 ms 44.9%
  triton_mm_8 5.6784 ms 20.3%
  triton_mm_0 6.8956 ms 16.7%
  triton_mm_5 7.6999 ms 15.0%
  triton_mm_6 7.8170 ms 14.8%
  triton_mm_4 7.8947 ms 14.6%
  triton_mm_3 8.1641 ms 14.1%
  triton_mm_1 8.7186 ms 13.2%
  triton_mm_2 8.7473 ms 13.2%
SingleProcess AUTOTUNE takes 5.7766 seconds
AUTOTUNE mm(256x512, 512x512)
  mm 0.0104 ms 100.0%
  triton_mm_18 0.0105 ms 99.7%
  triton_mm_21 0.0108 ms 96.7%
  triton_mm_20 0.0109 ms 95.3%
  triton_mm_17 0.0110 ms 95.0%
  triton_mm_15 0.0124 ms 83.8%
  triton_mm_16 0.0127 ms 82.1%
  triton_mm_14 0.0147 ms 70.9%
  triton_mm_13 0.0151 ms 69.2%
  triton_mm_12 0.0202 ms 51.7%
SingleProcess AUTOTUNE takes 4.7233 seconds
AUTOTUNE mm(256x512, 512x1024)
  mm 0.0108 ms 100.0%
  triton_mm_32 0.0118 ms 91.3%
  triton_mm_33 0.0124 ms 87.3%
  triton_mm_27 0.0133 ms 81.2%
  triton_mm_30 0.0133 ms 80.8%
  triton_mm_28 0.0135 ms 80.0%
  triton_mm_29 0.0142 ms 76.1%
  triton_mm_25 0.0151 ms 71.2%
  triton_mm_26 0.0156 ms 68.9%
  triton_mm_24 0.0202 ms 53.4%
SingleProcess AUTOTUNE takes 4.4760 seconds
AUTOTUNE mm(256x1024, 1024x512)
  mm 0.0122 ms 100.0%
  triton_mm_42 0.0140 ms 87.0%
  triton_mm_41 0.0140 ms 86.8%
  triton_mm_44 0.0152 ms 80.0%
  triton_mm_45 0.0158 ms 77.3%
  triton_mm_39 0.0184 ms 66.3%
  triton_mm_40 0.0188 ms 64.7%
  triton_mm_37 0.0228 ms 53.4%
  triton_mm_38 0.0232 ms 52.6%
  triton_mm_36 0.0334 ms 36.5%
SingleProcess AUTOTUNE takes 5.1605 seconds
AUTOTUNE mm(256x512, 512x197951)
  triton_mm_61 0.3730 ms 100.0%
  triton_mm_62 0.3801 ms 98.1%
  triton_mm_63 0.4246 ms 87.8%
  triton_mm_64 0.4314 ms 86.5%
  triton_mm_67 0.4440 ms 84.0%
  triton_mm_60 0.4716 ms 79.1%
  triton_mm_70 0.6932 ms 53.8%
  triton_mm_68 0.7620 ms 48.9%
  triton_mm_69 0.8037 ms 46.4%
  mm 0.8211 ms 45.4%
SingleProcess AUTOTUNE takes 5.7385 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 196.71it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 201.82it/s]
0.923x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/opacus_cifar10/__init__.py", line 6, in <module>
    from opacus import PrivacyEngine
ModuleNotFoundError: No module named 'opacus'
Failed to import user benchmark module dynamo, error: No module named 'opacus'
loading model: 0it [00:00, ?it/s]
Downloading config.json:   0%|          | 0.00/727 [00:00<?, ?B/s][ADownloading config.json: 100%|██████████| 727/727 [00:00<00:00, 3.92MB/s]

Downloading configuration_phi.py:   0%|          | 0.00/2.03k [00:00<?, ?B/s][ADownloading configuration_phi.py: 100%|██████████| 2.03k/2.03k [00:00<00:00, 23.8MB/s]
A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-1_5:
- configuration_phi.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.

Downloading modeling_phi.py:   0%|          | 0.00/33.8k [00:00<?, ?B/s][ADownloading modeling_phi.py: 100%|██████████| 33.8k/33.8k [00:00<00:00, 50.6MB/s]
A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-1_5:
- modeling_phi.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
loading model: 0it [00:02, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/phi_1_5/__init__.py", line 11, in __init__
    super().__init__(name="phi_1_5", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 437, in from_config
    model_class = get_class_from_dynamic_module(class_ref, repo_id, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 497, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module.replace(".py", ""))
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 199, in get_class_in_module
    module = importlib.import_module(module_path)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/5fd430c7bcd28140560faee2014d1228338e19a0/modeling_phi.py", line 16, in <module>
    from transformers import PretrainedConfig, PreTrainedModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  phlippe_densenet                   
AUTOTUNE convolution(128x3x32x32, 32x3x3x3)
  convolution 0.0417 ms 100.0%
  triton_convolution_4 0.0441 ms 94.6%
  triton_convolution_0 0.0443 ms 94.0%
  triton_convolution_3 0.0461 ms 90.4%
  triton_convolution_2 0.0502 ms 83.0%
  triton_convolution_5 0.0552 ms 75.5%
  triton_convolution_1 0.0645 ms 64.6%
SingleProcess AUTOTUNE takes 2.7085 seconds
AUTOTUNE mm(131072x32, 32x32)
  triton_mm_8 0.0176 ms 100.0%
  triton_mm_10 0.0182 ms 97.0%
  triton_mm_13 0.0183 ms 96.5%
  triton_mm_6 0.0183 ms 96.3%
  triton_mm_7 0.0183 ms 96.2%
  triton_mm_9 0.0189 ms 93.2%
  triton_mm_11 0.0190 ms 92.9%
  triton_mm_14 0.0190 ms 92.9%
  triton_mm_12 0.0192 ms 92.0%
  triton_mm_15 0.0192 ms 92.0%
SingleProcess AUTOTUNE takes 3.3016 seconds
AUTOTUNE convolution(128x32x32x32, 16x32x3x3)
  convolution 0.0396 ms 100.0%
  triton_convolution_21 0.0737 ms 53.8%
  triton_convolution_22 0.0930 ms 42.6%
  triton_convolution_23 0.0960 ms 41.3%
  triton_convolution_18 0.0980 ms 40.4%
  triton_convolution_19 0.1040 ms 38.1%
  triton_convolution_20 0.1511 ms 26.2%
SingleProcess AUTOTUNE takes 2.4886 seconds
AUTOTUNE mm(131072x48, 48x32)
  triton_mm_32 0.0234 ms 100.0%
  triton_mm_33 0.0237 ms 98.8%
  triton_mm_25 0.0245 ms 95.4%
  triton_mm_26 0.0251 ms 93.1%
  triton_mm_28 0.0256 ms 91.3%
  triton_mm_31 0.0257 ms 91.1%
  triton_mm_34 0.0257 ms 91.1%
  triton_mm_24 0.0259 ms 90.4%
  triton_mm_27 0.0259 ms 90.4%
  triton_mm_35 0.0259 ms 90.2%
SingleProcess AUTOTUNE takes 3.6523 seconds
AUTOTUNE mm(131072x64, 64x32)
  triton_mm_50 0.0266 ms 100.0%
  triton_mm_51 0.0271 ms 98.1%
  triton_mm_44 0.0284 ms 93.8%
  triton_mm_43 0.0286 ms 93.1%
  triton_mm_46 0.0291 ms 91.4%
  triton_mm_45 0.0294 ms 90.4%
  triton_mm_52 0.0295 ms 90.4%
  triton_mm_49 0.0296 ms 89.9%
  triton_mm_47 0.0296 ms 89.8%
  triton_mm_53 0.0298 ms 89.3%
SingleProcess AUTOTUNE takes 3.7471 seconds
AUTOTUNE mm(131072x80, 80x32)
  triton_mm_63 0.0339 ms 100.0%
  triton_mm_70 0.0340 ms 99.7%
  triton_mm_68 0.0343 ms 98.9%
  triton_mm_65 0.0344 ms 98.7%
  triton_mm_62 0.0344 ms 98.7%
  triton_mm_64 0.0346 ms 98.1%
  triton_mm_61 0.0353 ms 96.2%
  triton_mm_67 0.0357 ms 95.1%
  triton_mm_71 0.0360 ms 94.2%
  triton_mm_60 0.0380 ms 89.3%
SingleProcess AUTOTUNE takes 3.5238 seconds
AUTOTUNE mm(131072x96, 96x32)
  triton_mm_85 0.0365 ms 100.0%
  triton_mm_78 0.0367 ms 99.4%
  triton_mm_82 0.0370 ms 98.6%
  triton_mm_81 0.0371 ms 98.3%
  triton_mm_80 0.0373 ms 97.7%
  triton_mm_83 0.0376 ms 97.0%
  triton_mm_86 0.0377 ms 96.8%
  triton_mm_79 0.0380 ms 96.0%
  triton_mm_88 0.0386 ms 94.4%
  triton_mm_89 0.0413 ms 88.2%
SingleProcess AUTOTUNE takes 3.5329 seconds
AUTOTUNE mm(131072x112, 112x32)
  triton_mm_104 0.0416 ms 100.0%
  triton_mm_101 0.0416 ms 99.9%
  triton_mm_98 0.0426 ms 97.7%
  triton_mm_103 0.0426 ms 97.5%
  triton_mm_100 0.0428 ms 97.1%
  triton_mm_97 0.0428 ms 97.0%
  triton_mm_106 0.0431 ms 96.4%
  triton_mm_96 0.0441 ms 94.2%
  triton_mm_99 0.0445 ms 93.3%
  triton_mm_107 0.0464 ms 89.6%
SingleProcess AUTOTUNE takes 3.6473 seconds
AUTOTUNE mm(131072x128, 128x64)
  triton_mm_122 0.0467 ms 100.0%
  triton_mm_116 0.0468 ms 99.7%
  triton_mm_115 0.0470 ms 99.4%
  triton_mm_114 0.0472 ms 99.1%
  triton_mm_117 0.0478 ms 97.7%
  triton_mm_121 0.0481 ms 97.1%
  triton_mm_118 0.0492 ms 95.1%
  mm 0.0518 ms 90.1%
  triton_mm_120 0.0553 ms 84.5%
  triton_mm_123 0.0572 ms 81.7%
SingleProcess AUTOTUNE takes 4.2347 seconds
AUTOTUNE mm(32768x64, 64x32)
  triton_mm_128 0.0118 ms 100.0%
  triton_mm_134 0.0119 ms 99.2%
  triton_mm_135 0.0121 ms 97.1%
  triton_mm_127 0.0124 ms 94.6%
  triton_mm_131 0.0126 ms 93.2%
  triton_mm_136 0.0126 ms 93.2%
  triton_mm_126 0.0128 ms 92.0%
  triton_mm_129 0.0129 ms 91.1%
  triton_mm_133 0.0130 ms 90.6%
  triton_mm_137 0.0130 ms 90.4%
SingleProcess AUTOTUNE takes 3.6083 seconds
AUTOTUNE convolution(128x32x16x16, 16x32x3x3)
  convolution 0.0174 ms 100.0%
  triton_convolution_141 0.0295 ms 59.0%
  triton_convolution_138 0.0305 ms 57.0%
  triton_convolution_142 0.0324 ms 53.8%
  triton_convolution_143 0.0419 ms 41.6%
  triton_convolution_139 0.0424 ms 41.1%
  triton_convolution_140 0.0788 ms 22.1%
SingleProcess AUTOTUNE takes 2.3754 seconds
AUTOTUNE mm(32768x80, 80x32)
  triton_mm_155 0.0128 ms 100.0%
  triton_mm_154 0.0136 ms 94.6%
  triton_mm_146 0.0137 ms 93.9%
  triton_mm_147 0.0139 ms 92.6%
  triton_mm_148 0.0139 ms 92.6%
  triton_mm_151 0.0139 ms 92.4%
  triton_mm_149 0.0142 ms 90.3%
  triton_mm_144 0.0144 ms 88.9%
  triton_mm_145 0.0146 ms 87.7%
  triton_mm_152 0.0150 ms 85.3%
SingleProcess AUTOTUNE takes 3.5548 seconds
AUTOTUNE mm(32768x96, 96x32)
  triton_mm_162 0.0131 ms 100.0%
  triton_mm_164 0.0139 ms 94.5%
  triton_mm_169 0.0139 ms 94.2%
  triton_mm_165 0.0140 ms 93.4%
  triton_mm_167 0.0140 ms 93.2%
  triton_mm_172 0.0140 ms 93.2%
  triton_mm_163 0.0145 ms 90.5%
  triton_mm_166 0.0148 ms 88.7%
  triton_mm_173 0.0148 ms 88.5%
  mm 0.0155 ms 84.5%
SingleProcess AUTOTUNE takes 3.7850 seconds
AUTOTUNE mm(32768x112, 112x32)
  triton_mm_191 0.0156 ms 100.0%
  triton_mm_180 0.0156 ms 99.6%
  triton_mm_187 0.0157 ms 99.0%
  triton_mm_190 0.0158 ms 98.8%
  triton_mm_185 0.0159 ms 98.2%
  triton_mm_188 0.0159 ms 98.2%
  triton_mm_182 0.0160 ms 97.2%
  triton_mm_181 0.0162 ms 96.4%
  triton_mm_184 0.0170 ms 91.7%
  triton_mm_189 0.0175 ms 89.2%
SingleProcess AUTOTUNE takes 3.6996 seconds
AUTOTUNE mm(32768x128, 128x32)
  triton_mm_198 0.0150 ms 100.0%
  triton_mm_205 0.0157 ms 95.1%
  triton_mm_199 0.0158 ms 94.5%
  triton_mm_200 0.0160 ms 93.4%
  triton_mm_206 0.0164 ms 91.1%
  triton_mm_203 0.0165 ms 90.5%
  triton_mm_208 0.0165 ms 90.5%
  triton_mm_209 0.0167 ms 89.7%
  triton_mm_202 0.0172 ms 87.3%
  triton_mm_207 0.0174 ms 86.2%
SingleProcess AUTOTUNE takes 3.5970 seconds
AUTOTUNE mm(32768x144, 144x32)
  triton_mm_216 0.0168 ms 100.0%
  triton_mm_218 0.0174 ms 96.8%
  triton_mm_223 0.0178 ms 94.6%
  triton_mm_227 0.0180 ms 93.3%
  triton_mm_217 0.0183 ms 91.9%
  triton_mm_226 0.0183 ms 91.6%
  triton_mm_221 0.0187 ms 89.9%
  triton_mm_219 0.0193 ms 86.9%
  triton_mm_224 0.0194 ms 86.6%
  triton_mm_220 0.0195 ms 86.2%
SingleProcess AUTOTUNE takes 3.7635 seconds
AUTOTUNE mm(32768x160, 160x80)
  triton_mm_235 0.0233 ms 100.0%
  triton_mm_236 0.0237 ms 98.2%
  triton_mm_241 0.0256 ms 91.0%
  triton_mm_237 0.0263 ms 88.6%
  triton_mm_234 0.0265 ms 87.9%
  triton_mm_238 0.0267 ms 87.3%
  triton_mm_242 0.0274 ms 84.8%
  triton_mm_239 0.0287 ms 81.0%
  mm 0.0293 ms 79.5%
  triton_mm_245 0.0347 ms 67.0%
SingleProcess AUTOTUNE takes 4.9996 seconds
AUTOTUNE mm(8192x80, 80x32)
  triton_mm_247 0.0086 ms 100.0%
  triton_mm_250 0.0086 ms 100.0%
  triton_mm_248 0.0090 ms 95.4%
  triton_mm_246 0.0091 ms 94.7%
  triton_mm_254 0.0091 ms 94.7%
  triton_mm_251 0.0093 ms 92.8%
  triton_mm_257 0.0097 ms 88.5%
  triton_mm_249 0.0098 ms 88.2%
  triton_mm_255 0.0098 ms 87.9%
  triton_mm_253 0.0099 ms 87.3%
SingleProcess AUTOTUNE takes 4.0265 seconds
AUTOTUNE convolution(128x32x8x8, 16x32x3x3)
  convolution 0.0115 ms 100.0%
  triton_convolution_262 0.0173 ms 66.1%
  triton_convolution_261 0.0184 ms 62.4%
  triton_convolution_258 0.0188 ms 61.1%
  triton_convolution_263 0.0259 ms 44.3%
  triton_convolution_259 0.0333 ms 34.4%
  triton_convolution_260 0.0780 ms 14.7%
SingleProcess AUTOTUNE takes 2.3942 seconds
AUTOTUNE mm(8192x96, 96x32)
  triton_mm_266 0.0086 ms 100.0%
  triton_mm_268 0.0086 ms 99.6%
  triton_mm_265 0.0088 ms 97.8%
  triton_mm_267 0.0091 ms 94.4%
  triton_mm_269 0.0091 ms 94.4%
  triton_mm_264 0.0095 ms 90.6%
  triton_mm_271 0.0097 ms 88.8%
  triton_mm_272 0.0099 ms 87.1%
  triton_mm_273 0.0099 ms 87.1%
  triton_mm_270 0.0099 ms 86.8%
SingleProcess AUTOTUNE takes 3.9514 seconds
AUTOTUNE mm(8192x112, 112x32)
  triton_mm_284 0.0091 ms 100.0%
  triton_mm_286 0.0095 ms 95.3%
  triton_mm_290 0.0098 ms 93.1%
  triton_mm_283 0.0101 ms 89.9%
  triton_mm_291 0.0101 ms 89.6%
  triton_mm_287 0.0103 ms 88.5%
  triton_mm_288 0.0104 ms 87.4%
  triton_mm_282 0.0104 ms 87.1%
  triton_mm_285 0.0108 ms 83.8%
  mm 0.0110 ms 82.3%
SingleProcess AUTOTUNE takes 3.8407 seconds
AUTOTUNE mm(8192x128, 128x32)
  triton_mm_301 0.0093 ms 100.0%
  triton_mm_302 0.0095 ms 98.0%
  triton_mm_304 0.0095 ms 97.7%
  triton_mm_308 0.0098 ms 95.4%
  triton_mm_309 0.0098 ms 95.4%
  triton_mm_306 0.0102 ms 91.5%
  triton_mm_300 0.0104 ms 89.3%
  triton_mm_305 0.0107 ms 87.4%
  triton_mm_303 0.0108 ms 86.0%
  triton_mm_307 0.0109 ms 85.6%
SingleProcess AUTOTUNE takes 4.0867 seconds
AUTOTUNE mm(8192x144, 144x32)
  triton_mm_320 0.0106 ms 100.0%
  triton_mm_327 0.0107 ms 98.8%
  triton_mm_322 0.0108 ms 97.9%
  triton_mm_326 0.0109 ms 97.4%
  triton_mm_324 0.0110 ms 96.5%
  triton_mm_319 0.0110 ms 96.2%
  triton_mm_318 0.0114 ms 93.0%
  triton_mm_321 0.0115 ms 91.9%
  triton_mm_323 0.0116 ms 91.2%
  mm 0.0121 ms 87.6%
SingleProcess AUTOTUNE takes 3.9764 seconds
AUTOTUNE mm(8192x160, 160x32)
  triton_mm_338 0.0102 ms 100.0%
  triton_mm_339 0.0108 ms 94.6%
  triton_mm_337 0.0108 ms 94.1%
  triton_mm_340 0.0108 ms 94.1%
  triton_mm_342 0.0110 ms 92.4%
  triton_mm_345 0.0110 ms 92.2%
  triton_mm_341 0.0111 ms 91.6%
  triton_mm_344 0.0117 ms 86.6%
  triton_mm_336 0.0118 ms 86.2%
  triton_mm_343 0.0126 ms 80.7%
SingleProcess AUTOTUNE takes 3.9756 seconds
AUTOTUNE mm(8192x176, 176x88)
  triton_mm_355 0.0136 ms 100.0%
  triton_mm_356 0.0137 ms 99.5%
  mm 0.0142 ms 96.2%
  triton_mm_358 0.0142 ms 95.7%
  triton_mm_362 0.0144 ms 94.9%
  triton_mm_354 0.0146 ms 93.4%
  triton_mm_357 0.0148 ms 91.8%
  triton_mm_359 0.0152 ms 89.7%
  triton_mm_361 0.0160 ms 85.1%
  triton_mm_365 0.0160 ms 85.0%
SingleProcess AUTOTUNE takes 5.2676 seconds
AUTOTUNE mm(2048x88, 88x32)
  triton_mm_369 0.0074 ms 100.0%
  triton_mm_368 0.0076 ms 96.7%
  triton_mm_374 0.0076 ms 96.7%
  triton_mm_367 0.0079 ms 94.1%
  mm 0.0079 ms 93.5%
  triton_mm_371 0.0080 ms 92.8%
  triton_mm_366 0.0081 ms 91.3%
  triton_mm_372 0.0082 ms 90.2%
  triton_mm_370 0.0084 ms 87.8%
  triton_mm_375 0.0085 ms 87.2%
SingleProcess AUTOTUNE takes 4.1334 seconds
AUTOTUNE convolution(128x32x4x4, 16x32x3x3)
  convolution 0.0112 ms 100.0%
  triton_convolution_378 0.0171 ms 65.5%
  triton_convolution_382 0.0188 ms 59.4%
  triton_convolution_381 0.0190 ms 59.0%
  triton_convolution_383 0.0277 ms 40.5%
  triton_convolution_379 0.0323 ms 34.7%
  triton_convolution_380 0.0742 ms 15.1%
SingleProcess AUTOTUNE takes 2.2942 seconds
AUTOTUNE mm(2048x104, 104x32)
  triton_mm_385 0.0074 ms 100.0%
  triton_mm_387 0.0076 ms 97.1%
  triton_mm_389 0.0076 ms 97.1%
  triton_mm_393 0.0079 ms 94.3%
  triton_mm_392 0.0081 ms 92.1%
  mm 0.0081 ms 91.3%
  triton_mm_390 0.0082 ms 90.3%
  triton_mm_386 0.0084 ms 88.2%
  triton_mm_388 0.0086 ms 85.9%
  triton_mm_384 0.0096 ms 77.5%
SingleProcess AUTOTUNE takes 3.9466 seconds
AUTOTUNE mm(2048x120, 120x32)
  triton_mm_403 0.0076 ms 100.0%
  mm 0.0079 ms 96.4%
  triton_mm_404 0.0081 ms 94.4%
  triton_mm_410 0.0082 ms 93.0%
  triton_mm_407 0.0082 ms 92.6%
  triton_mm_408 0.0083 ms 92.2%
  triton_mm_405 0.0083 ms 91.5%
  triton_mm_411 0.0086 ms 88.1%
  triton_mm_406 0.0089 ms 85.9%
  triton_mm_402 0.0090 ms 84.4%
SingleProcess AUTOTUNE takes 3.6268 seconds
AUTOTUNE mm(2048x136, 136x32)
  triton_mm_423 0.0080 ms 100.0%
  triton_mm_426 0.0084 ms 95.1%
  triton_mm_428 0.0084 ms 94.7%
  triton_mm_429 0.0086 ms 92.9%
  triton_mm_425 0.0086 ms 92.6%
  triton_mm_421 0.0087 ms 92.3%
  triton_mm_422 0.0091 ms 87.7%
  triton_mm_424 0.0091 ms 87.7%
  mm 0.0098 ms 82.0%
  triton_mm_420 0.0105 ms 76.2%
SingleProcess AUTOTUNE takes 3.8572 seconds
AUTOTUNE mm(2048x152, 152x32)
  triton_mm_444 0.0081 ms 100.0%
  triton_mm_443 0.0081 ms 99.6%
  triton_mm_446 0.0081 ms 99.6%
  triton_mm_441 0.0087 ms 93.0%
  triton_mm_442 0.0088 ms 91.7%
  triton_mm_439 0.0089 ms 90.7%
  triton_mm_447 0.0089 ms 90.7%
  mm 0.0095 ms 85.2%
  triton_mm_440 0.0096 ms 84.3%
  triton_mm_438 0.0107 ms 76.0%
SingleProcess AUTOTUNE takes 4.0470 seconds
AUTOTUNE mm(2048x168, 168x32)
  triton_mm_464 0.0084 ms 100.0%
  triton_mm_462 0.0087 ms 96.0%
  triton_mm_457 0.0088 ms 94.9%
  triton_mm_465 0.0089 ms 93.5%
  triton_mm_461 0.0090 ms 92.9%
  triton_mm_458 0.0091 ms 92.2%
  triton_mm_459 0.0092 ms 91.3%
  mm 0.0095 ms 87.9%
  triton_mm_460 0.0096 ms 86.7%
  triton_mm_456 0.0115 ms 72.7%
SingleProcess AUTOTUNE takes 3.8634 seconds
AUTOTUNE int_mm(128x184, 184x10, 128x10)
  triton_mm_480 0.0074 ms 100.0%
  triton_mm_479 0.0076 ms 97.1%
  triton_mm_482 0.0080 ms 92.4%
  triton_mm_483 0.0083 ms 88.8%
  triton_mm_477 0.0084 ms 88.2%
  triton_mm_475 0.0087 ms 85.2%
  triton_mm_476 0.0090 ms 82.2%
  triton_mm_474 0.0097 ms 76.2%
  triton_mm_478 0.0108 ms 68.1%
  triton_mm_481 0.0132 ms 55.8%
SingleProcess AUTOTUNE takes 3.3073 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 71.99it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 78.45it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 81.14it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 80.33it/s]
5.577x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  phlippe_resnet                     
AUTOTUNE convolution(128x3x32x32, 16x3x3x3)
  triton_convolution_1 0.0187 ms 100.0%
  triton_convolution_3 0.0203 ms 92.4%
  triton_convolution_4 0.0205 ms 91.3%
  triton_convolution_0 0.0218 ms 85.8%
  triton_convolution_2 0.0375 ms 49.9%
  convolution 0.0399 ms 46.9%
SingleProcess AUTOTUNE takes 1.8185 seconds
AUTOTUNE convolution(128x16x32x32, 16x16x3x3)
  triton_convolution_6 0.0330 ms 100.0%
  triton_convolution_8 0.0348 ms 94.8%
  triton_convolution_5 0.0360 ms 91.6%
  convolution 0.0366 ms 90.1%
  triton_convolution_9 0.0371 ms 88.8%
  triton_convolution_7 0.0527 ms 62.5%
SingleProcess AUTOTUNE takes 1.8338 seconds
AUTOTUNE convolution(128x16x32x32, 32x16x3x3)
  convolution 0.0179 ms 100.0%
  triton_convolution_39 0.0185 ms 96.7%
  triton_convolution_35 0.0215 ms 83.3%
  triton_convolution_38 0.0275 ms 65.2%
  triton_convolution_40 0.0386 ms 46.4%
  triton_convolution_36 0.0388 ms 46.0%
  triton_convolution_37 0.0436 ms 41.0%
SingleProcess AUTOTUNE takes 2.4263 seconds
AUTOTUNE convolution(128x32x16x16, 32x32x3x3)
  convolution 0.0180 ms 100.0%
  triton_convolution_45 0.0242 ms 74.2%
  triton_convolution_44 0.0252 ms 71.4%
  triton_convolution_41 0.0262 ms 68.6%
  triton_convolution_46 0.0286 ms 62.8%
  triton_convolution_47 0.0294 ms 61.0%
  triton_convolution_42 0.0439 ms 40.9%
  triton_convolution_43 0.0784 ms 22.9%
SingleProcess AUTOTUNE takes 2.8783 seconds
AUTOTUNE convolution(128x16x32x32, 32x16x1x1)
  triton_convolution_48 0.0099 ms 100.0%
  triton_convolution_51 0.0100 ms 99.0%
  triton_convolution_52 0.0101 ms 98.1%
  triton_convolution_53 0.0111 ms 89.6%
  convolution 0.0124 ms 79.9%
  triton_convolution_49 0.0126 ms 78.9%
  triton_convolution_50 0.0132 ms 75.4%
SingleProcess AUTOTUNE takes 2.5649 seconds
AUTOTUNE convolution(128x32x16x16, 64x32x3x3)
  convolution 0.0141 ms 100.0%
  triton_convolution_87 0.0365 ms 38.7%
  triton_convolution_82 0.0479 ms 29.5%
  triton_convolution_86 0.0536 ms 26.4%
  triton_convolution_88 0.0557 ms 25.4%
  triton_convolution_85 0.0573 ms 24.7%
  triton_convolution_83 0.0905 ms 15.6%
  triton_convolution_84 0.1282 ms 11.0%
SingleProcess AUTOTUNE takes 3.9878 seconds
AUTOTUNE convolution(128x64x8x8, 64x64x3x3)
  convolution 0.0208 ms 100.0%
  triton_convolution_94 0.0416 ms 49.9%
  triton_convolution_93 0.0516 ms 40.3%
  triton_convolution_89 0.0544 ms 38.2%
  triton_convolution_92 0.0594 ms 35.0%
  triton_convolution_95 0.0710 ms 29.3%
  triton_convolution_90 0.1108 ms 18.7%
  triton_convolution_91 0.2200 ms 9.4%
SingleProcess AUTOTUNE takes 3.7305 seconds
AUTOTUNE convolution(128x32x16x16, 64x32x1x1)
  triton_convolution_96 0.0088 ms 100.0%
  triton_convolution_101 0.0088 ms 99.6%
  triton_convolution_99 0.0103 ms 85.1%
  triton_convolution_100 0.0103 ms 84.8%
  convolution 0.0117 ms 74.9%
  triton_convolution_102 0.0131 ms 67.0%
  triton_convolution_97 0.0171 ms 51.2%
  triton_convolution_98 0.0218 ms 40.3%
SingleProcess AUTOTUNE takes 3.9225 seconds
AUTOTUNE int_mm(128x64, 64x10, 128x10)
  triton_mm_137 0.0068 ms 100.0%
  triton_mm_139 0.0069 ms 98.1%
  triton_mm_132 0.0069 ms 97.7%
  triton_mm_136 0.0069 ms 97.7%
  triton_mm_140 0.0073 ms 92.6%
  triton_mm_134 0.0075 ms 90.6%
  triton_mm_131 0.0077 ms 87.6%
  triton_mm_133 0.0080 ms 84.8%
  triton_mm_135 0.0083 ms 81.9%
  triton_mm_138 0.0085 ms 79.4%
SingleProcess AUTOTUNE takes 3.4036 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 162.37it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 168.81it/s]
5.999x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
cuda eval  pyhpc_equation_of_state            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 139.39it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 175.01it/s]
16.582x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pyhpc_isoneutral_mixing            
skipping cudagraphs due to ['mutated inputs']
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 86.30it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 87.45it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 87.72it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 87.06it/s]
6.112x
loading model: 0it [00:00, ?it/s]WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
loading model: 0it [00:01, ?it/s]
WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
cuda eval  pyhpc_turbulent_kinetic_energy     
WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 73.57it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 77.97it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 79.35it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 78.70it/s]
4.617x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/__init__.py", line 13, in <module>
    from .train_cyclegan import prepare_training_loop
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/train_cyclegan.py", line 27, in <module>
    from .util.visualizer import Visualizer
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/util/visualizer.py", line 6, in <module>
    from . import util, html
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/util/html.py", line 1, in <module>
    import dominate
ModuleNotFoundError: No module named 'dominate'
Failed to import user benchmark module dynamo, error: No module named 'dominate'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/__init__.py", line 52, in __init__
    self.data_loader = self.get_data_loader(config)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/__init__.py", line 68, in get_data_loader
    celeba_loader = get_loader(config.celeba_image_dir, config.attr_path, config.selected_attrs,
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 82, in get_loader
    dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 24, in __init__
    self.preprocess()
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 33, in preprocess
    lines = [line.rstrip() for line in open(self.attr_path, 'r')]
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/data/.data/pytorch_stargan_inputs/data/celeba/list_attr_celeba.txt'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pytorch_unet                       
AUTOTUNE convolution(1x3x640x959, 64x3x3x3)
  triton_convolution_4 0.2404 ms 100.0%
  triton_convolution_3 0.2485 ms 96.7%
  convolution 0.2967 ms 81.0%
  triton_convolution_5 0.3289 ms 73.1%
  triton_convolution_0 0.3372 ms 71.3%
  triton_convolution_2 0.4540 ms 53.0%
  triton_convolution_1 0.5532 ms 43.5%
SingleProcess AUTOTUNE takes 3.2173 seconds
AUTOTUNE convolution(1x64x640x959, 64x64x3x3)
  convolution 0.2784 ms 100.0%
  triton_convolution_12 1.3447 ms 20.7%
  triton_convolution_6 1.3520 ms 20.6%
  triton_convolution_11 1.5423 ms 18.1%
  triton_convolution_7 2.0158 ms 13.8%
  triton_convolution_9 2.2738 ms 12.2%
  triton_convolution_10 2.3690 ms 11.8%
  triton_convolution_8 5.5407 ms 5.0%
SingleProcess AUTOTUNE takes 3.8439 seconds
AUTOTUNE convolution(1x64x320x479, 128x64x3x3)
  convolution 0.1238 ms 100.0%
  triton_convolution_13 0.6810 ms 18.2%
  triton_convolution_16 0.6955 ms 17.8%
  triton_convolution_19 0.6995 ms 17.7%
  triton_convolution_18 0.7728 ms 16.0%
  triton_convolution_14 1.0336 ms 12.0%
  triton_convolution_17 1.1999 ms 10.3%
  triton_convolution_15 2.8704 ms 4.3%
SingleProcess AUTOTUNE takes 4.2499 seconds
AUTOTUNE convolution(1x128x320x479, 128x128x3x3)
  convolution 0.2156 ms 100.0%
  triton_convolution_23 1.3722 ms 15.7%
  triton_convolution_26 1.3891 ms 15.5%
  triton_convolution_20 1.4541 ms 14.8%
  triton_convolution_25 1.5248 ms 14.1%
  triton_convolution_21 2.1373 ms 10.1%
  triton_convolution_24 2.3364 ms 9.2%
  triton_convolution_22 5.7308 ms 3.8%
SingleProcess AUTOTUNE takes 4.1554 seconds
AUTOTUNE convolution(1x128x160x239, 256x128x3x3)
  convolution 0.1095 ms 100.0%
  triton_convolution_32 0.5942 ms 18.4%
  triton_convolution_30 0.6738 ms 16.2%
  triton_convolution_27 0.8149 ms 13.4%
  triton_convolution_33 0.9660 ms 11.3%
  triton_convolution_31 0.9727 ms 11.3%
  triton_convolution_28 1.2426 ms 8.8%
  triton_convolution_29 2.8978 ms 3.8%
SingleProcess AUTOTUNE takes 5.0828 seconds
AUTOTUNE convolution(1x256x160x239, 256x256x3x3)
  convolution 0.2013 ms 100.0%
  triton_convolution_39 1.3126 ms 15.3%
  triton_convolution_37 1.5037 ms 13.4%
  triton_convolution_34 1.6485 ms 12.2%
  triton_convolution_40 2.2057 ms 9.1%
  triton_convolution_38 2.7119 ms 7.4%
  triton_convolution_35 3.0624 ms 6.6%
  triton_convolution_36 5.7767 ms 3.5%
SingleProcess AUTOTUNE takes 4.9776 seconds
AUTOTUNE convolution(1x256x80x119, 512x256x3x3)
  convolution 0.0993 ms 100.0%
  triton_convolution_46 0.6067 ms 16.4%
  triton_convolution_44 0.6594 ms 15.1%
  triton_convolution_47 0.7390 ms 13.4%
  triton_convolution_41 1.0655 ms 9.3%
  triton_convolution_45 1.2047 ms 8.2%
  triton_convolution_42 1.2279 ms 8.1%
  triton_convolution_43 2.8606 ms 3.5%
SingleProcess AUTOTUNE takes 4.7999 seconds
AUTOTUNE convolution(1x512x80x119, 512x512x3x3)
  convolution 0.1904 ms 100.0%
  triton_convolution_53 1.3121 ms 14.5%
  triton_convolution_54 1.4981 ms 12.7%
  triton_convolution_51 1.7946 ms 10.6%
  triton_convolution_48 2.1313 ms 8.9%
  triton_convolution_49 2.4922 ms 7.6%
  triton_convolution_52 2.9946 ms 6.4%
  triton_convolution_50 5.7678 ms 3.3%
SingleProcess AUTOTUNE takes 4.9304 seconds
AUTOTUNE convolution(1x512x40x59, 512x512x3x3)
  convolution 0.0677 ms 100.0%
  triton_convolution_60 0.4826 ms 14.0%
  triton_convolution_58 0.6689 ms 10.1%
  triton_convolution_61 0.7633 ms 8.9%
  triton_convolution_59 1.0423 ms 6.5%
  triton_convolution_55 1.1519 ms 5.9%
  triton_convolution_56 1.2347 ms 5.5%
  triton_convolution_57 1.9236 ms 3.5%
SingleProcess AUTOTUNE takes 4.8068 seconds
AUTOTUNE convolution(1x1024x80x119, 512x1024x3x3)
  convolution 0.3755 ms 100.0%
  triton_convolution_74 2.7385 ms 13.7%
  triton_convolution_75 3.7858 ms 9.9%
  triton_convolution_72 4.0945 ms 9.2%
  triton_convolution_69 4.4789 ms 8.4%
  triton_convolution_73 6.5787 ms 5.7%
  triton_convolution_70 6.9056 ms 5.4%
  triton_convolution_71 11.5261 ms 3.3%
SingleProcess AUTOTUNE takes 5.0119 seconds
AUTOTUNE convolution(1x512x80x119, 256x512x3x3)
  convolution 0.1217 ms 100.0%
  triton_convolution_81 0.9054 ms 13.4%
  triton_convolution_82 0.9977 ms 12.2%
  triton_convolution_76 1.1423 ms 10.7%
  triton_convolution_79 1.1964 ms 10.2%
  triton_convolution_77 1.3056 ms 9.3%
  triton_convolution_80 1.5958 ms 7.6%
  triton_convolution_78 3.8496 ms 3.2%
SingleProcess AUTOTUNE takes 4.7539 seconds
AUTOTUNE convolution(1x512x160x239, 256x512x3x3)
  convolution 0.3852 ms 100.0%
  triton_convolution_88 2.7501 ms 14.0%
  triton_convolution_83 3.9467 ms 9.8%
  triton_convolution_86 3.9931 ms 9.6%
  triton_convolution_89 6.6220 ms 5.8%
  triton_convolution_87 7.1726 ms 5.4%
  triton_convolution_84 7.5557 ms 5.1%
  triton_convolution_85 11.7898 ms 3.3%
SingleProcess AUTOTUNE takes 5.4468 seconds
AUTOTUNE convolution(1x256x160x239, 128x256x3x3)
  convolution 0.1044 ms 100.0%
  triton_convolution_93 0.7715 ms 13.5%
  triton_convolution_95 0.8472 ms 12.3%
  triton_convolution_90 0.9362 ms 11.2%
  triton_convolution_96 1.1362 ms 9.2%
  triton_convolution_94 1.3906 ms 7.5%
  triton_convolution_91 1.7886 ms 5.8%
  triton_convolution_92 2.8961 ms 3.6%
SingleProcess AUTOTUNE takes 4.4732 seconds
AUTOTUNE convolution(1x256x320x479, 128x256x3x3)
  convolution 0.4060 ms 100.0%
  triton_convolution_100 2.8781 ms 14.1%
  triton_convolution_102 3.0601 ms 13.3%
  triton_convolution_103 3.2244 ms 12.6%
  triton_convolution_97 3.5152 ms 11.6%
  triton_convolution_101 6.0426 ms 6.7%
  triton_convolution_98 6.1132 ms 6.6%
  triton_convolution_99 11.6213 ms 3.5%
SingleProcess AUTOTUNE takes 5.0109 seconds
AUTOTUNE convolution(1x128x320x479, 64x128x3x3)
  convolution 0.1241 ms 100.0%
  triton_convolution_110 0.7143 ms 17.4%
  triton_convolution_104 0.7789 ms 15.9%
  triton_convolution_109 0.8021 ms 15.5%
  triton_convolution_105 1.0789 ms 11.5%
  triton_convolution_108 1.1973 ms 10.4%
  triton_convolution_107 1.2022 ms 10.3%
  triton_convolution_106 2.8908 ms 4.3%
SingleProcess AUTOTUNE takes 6.1587 seconds
AUTOTUNE convolution(1x128x640x959, 64x128x3x3)
  convolution 0.4643 ms 100.0%
  triton_convolution_117 2.7053 ms 17.2%
  triton_convolution_116 3.0865 ms 15.0%
  triton_convolution_111 3.3708 ms 13.8%
  triton_convolution_112 4.2247 ms 11.0%
  triton_convolution_114 4.6381 ms 10.0%
  triton_convolution_115 4.6384 ms 10.0%
  triton_convolution_113 11.0468 ms 4.2%
SingleProcess AUTOTUNE takes 5.7611 seconds
AUTOTUNE addmm(613760x2, 613760x64, 64x2)
  triton_mm_127 0.0761 ms 100.0%
  triton_mm_132 0.0792 ms 96.1%
  triton_mm_129 0.0796 ms 95.5%
  triton_mm_126 0.0815 ms 93.3%
  triton_mm_125 0.0816 ms 93.2%
  triton_mm_128 0.0828 ms 91.9%
  triton_mm_130 0.0845 ms 90.0%
  triton_mm_135 0.0903 ms 84.3%
  triton_mm_134 0.0997 ms 76.3%
  bias_addmm 0.1200 ms 63.4%
SingleProcess AUTOTUNE takes 5.6271 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:00, 27.82it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 38.34it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 41.65it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 43.25it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 44.06it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 44.65it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 42.88it/s]
1.778x
loading model: 0it [00:00, ?it/s]Downloading: "https://download.pytorch.org/models/resnet152-394f9c45.pth" to /home/cdhernandez/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth

  0%|          | 0.00/230M [00:00<?, ?B/s][A
  3%|▎         | 6.16M/230M [00:00<00:03, 64.6MB/s][A
  7%|▋         | 16.4M/230M [00:00<00:02, 86.9MB/s][A
 14%|█▎        | 31.2M/230M [00:00<00:02, 87.6MB/s][A
 17%|█▋        | 39.5M/230M [00:00<00:03, 51.6MB/s][A
 21%|██▏       | 49.1M/230M [00:00<00:03, 58.5MB/s][A
 24%|██▍       | 55.7M/230M [00:00<00:03, 54.6MB/s][A
 27%|██▋       | 61.5M/230M [00:01<00:03, 53.5MB/s][A
 29%|██▉       | 67.0M/230M [00:01<00:03, 47.3MB/s][A
 35%|███▍      | 80.3M/230M [00:01<00:02, 55.9MB/s][A
 37%|███▋      | 85.8M/230M [00:01<00:02, 51.0MB/s][A
 43%|████▎     | 98.3M/230M [00:01<00:02, 61.4MB/s][A
 49%|████▉     | 114M/230M [00:01<00:01, 65.5MB/s] [A
 52%|█████▏    | 120M/230M [00:02<00:01, 61.8MB/s][A
 57%|█████▋    | 131M/230M [00:02<00:01, 59.6MB/s][A
 64%|██████▎   | 147M/230M [00:02<00:01, 76.4MB/s][A
 67%|██████▋   | 154M/230M [00:02<00:01, 62.3MB/s][A
 71%|███████   | 164M/230M [00:02<00:01, 59.2MB/s][A
 78%|███████▊  | 179M/230M [00:02<00:00, 78.0MB/s][A
 81%|████████▏ | 188M/230M [00:03<00:00, 60.3MB/s][A
 85%|████████▌ | 197M/230M [00:03<00:00, 56.8MB/s][A
 90%|█████████ | 208M/230M [00:03<00:00, 68.3MB/s][A
 94%|█████████▎| 216M/230M [00:03<00:00, 64.1MB/s][A
 99%|█████████▉| 229M/230M [00:03<00:00, 60.1MB/s][A100%|██████████| 230M/230M [00:03<00:00, 60.5MB/s]
loading model: 0it [00:07, ?it/s]
cuda eval  resnet152                          
AUTOTUNE convolution(32x3x224x224, 64x3x7x7)
  convolution 0.4262 ms 100.0%
  triton_convolution_3 0.7128 ms 59.8%
  triton_convolution_4 0.7720 ms 55.2%
  triton_convolution_5 0.8300 ms 51.3%
  triton_convolution_0 0.9445 ms 45.1%
  triton_convolution_2 1.0364 ms 41.1%
  triton_convolution_1 3.0047 ms 14.2%
SingleProcess AUTOTUNE takes 3.2956 seconds
AUTOTUNE mm(100352x64, 64x64)
  triton_mm_14 0.0255 ms 100.0%
  triton_mm_8 0.0271 ms 94.1%
  triton_mm_9 0.0275 ms 92.8%
  triton_mm_7 0.0276 ms 92.2%
  triton_mm_10 0.0284 ms 89.8%
  triton_mm_6 0.0284 ms 89.5%
  triton_mm_15 0.0285 ms 89.3%
  triton_mm_13 0.0305 ms 83.5%
  mm 0.0306 ms 83.3%
  triton_mm_17 0.0347 ms 73.4%
SingleProcess AUTOTUNE takes 3.8656 seconds
AUTOTUNE convolution(32x64x56x56, 64x64x3x3)
  convolution 0.0592 ms 100.0%
  triton_convolution_18 0.2490 ms 23.8%
  triton_convolution_23 0.2714 ms 21.8%
  triton_convolution_24 0.3397 ms 17.4%
  triton_convolution_21 0.3698 ms 16.0%
  triton_convolution_22 0.4022 ms 14.7%
  triton_convolution_19 0.4694 ms 12.6%
  triton_convolution_20 0.9551 ms 6.2%
SingleProcess AUTOTUNE takes 4.2208 seconds
AUTOTUNE mm(100352x64, 64x256)
  triton_mm_27 0.0524 ms 100.0%
  triton_mm_26 0.0526 ms 99.6%
  triton_mm_29 0.0581 ms 90.3%
  triton_mm_33 0.0584 ms 89.8%
  triton_mm_28 0.0588 ms 89.1%
  mm 0.0649 ms 80.8%
  triton_mm_25 0.0684 ms 76.7%
  triton_mm_32 0.0716 ms 73.2%
  triton_mm_35 0.0764 ms 68.7%
  triton_mm_34 0.0865 ms 60.7%
SingleProcess AUTOTUNE takes 4.7220 seconds
AUTOTUNE mm(100352x256, 256x64)
  triton_mm_52 0.0581 ms 100.0%
  triton_mm_50 0.0583 ms 99.7%
  triton_mm_51 0.0591 ms 98.4%
  triton_mm_56 0.0592 ms 98.2%
  triton_mm_57 0.0593 ms 98.1%
  triton_mm_53 0.0609 ms 95.5%
  mm 0.0616 ms 94.3%
  triton_mm_49 0.0625 ms 92.9%
  triton_mm_55 0.0693 ms 83.8%
  triton_mm_54 0.0708 ms 82.1%
SingleProcess AUTOTUNE takes 4.3063 seconds
AUTOTUNE mm(100352x256, 256x128)
  mm 0.0676 ms 100.0%
  triton_mm_113 0.0692 ms 97.7%
  triton_mm_112 0.0712 ms 95.1%
  triton_mm_115 0.0734 ms 92.2%
  triton_mm_114 0.0737 ms 91.8%
  triton_mm_118 0.0758 ms 89.2%
  triton_mm_119 0.0792 ms 85.4%
  triton_mm_111 0.0852 ms 79.4%
  triton_mm_117 0.1243 ms 54.4%
  triton_mm_116 0.1250 ms 54.1%
SingleProcess AUTOTUNE takes 4.9347 seconds
AUTOTUNE convolution(32x128x56x56, 128x128x3x3)
  convolution 0.0539 ms 100.0%
  triton_convolution_123 0.4163 ms 12.9%
  triton_convolution_126 0.4420 ms 12.2%
  triton_convolution_128 0.4764 ms 11.3%
  triton_convolution_129 0.5021 ms 10.7%
  triton_convolution_124 0.5396 ms 10.0%
  triton_convolution_127 0.7020 ms 7.7%
  triton_convolution_125 1.0200 ms 5.3%
SingleProcess AUTOTUNE takes 4.2997 seconds
AUTOTUNE mm(25088x128, 128x512)
  triton_mm_132 0.0358 ms 100.0%
  triton_mm_131 0.0363 ms 98.7%
  triton_mm_134 0.0420 ms 85.4%
  triton_mm_130 0.0422 ms 85.0%
  triton_mm_137 0.0422 ms 85.0%
  triton_mm_133 0.0427 ms 83.9%
  mm 0.0453 ms 79.0%
  triton_mm_138 0.0469 ms 76.3%
  triton_mm_140 0.0567 ms 63.2%
  triton_mm_139 0.0739 ms 48.5%
SingleProcess AUTOTUNE takes 4.8031 seconds
AUTOTUNE convolution(32x256x56x56, 512x256x1x1)
  convolution 0.0542 ms 100.0%
  triton_convolution_145 0.1490 ms 36.4%
  triton_convolution_142 0.1566 ms 34.6%
  triton_convolution_143 0.1592 ms 34.1%
  triton_convolution_148 0.1685 ms 32.2%
  triton_convolution_146 0.1730 ms 31.4%
  triton_convolution_147 0.1765 ms 30.7%
  triton_convolution_144 0.8830 ms 6.1%
SingleProcess AUTOTUNE takes 5.0415 seconds
AUTOTUNE mm(25088x512, 512x128)
  mm 0.0377 ms 100.0%
  triton_mm_152 0.0380 ms 99.3%
  triton_mm_153 0.0398 ms 94.6%
  triton_mm_150 0.0406 ms 92.8%
  triton_mm_157 0.0413 ms 91.2%
  triton_mm_151 0.0415 ms 90.8%
  triton_mm_156 0.0444 ms 84.8%
  triton_mm_149 0.0543 ms 69.4%
  triton_mm_154 0.0591 ms 63.7%
  triton_mm_155 0.0595 ms 63.4%
SingleProcess AUTOTUNE takes 5.0290 seconds
AUTOTUNE convolution(32x128x28x28, 128x128x3x3)
  convolution 0.0416 ms 100.0%
  triton_convolution_164 0.2270 ms 18.3%
  triton_convolution_161 0.2483 ms 16.7%
  triton_convolution_166 0.2743 ms 15.2%
  triton_convolution_165 0.3319 ms 12.5%
  triton_convolution_167 0.3350 ms 12.4%
  triton_convolution_162 0.4458 ms 9.3%
  triton_convolution_163 0.9323 ms 4.5%
SingleProcess AUTOTUNE takes 4.2346 seconds
AUTOTUNE mm(25088x512, 512x256)
  mm 0.0471 ms 100.0%
  triton_mm_368 0.0509 ms 92.6%
  triton_mm_367 0.0537 ms 87.8%
  triton_mm_369 0.0566 ms 83.2%
  triton_mm_370 0.0569 ms 82.8%
  triton_mm_374 0.0619 ms 76.2%
  triton_mm_373 0.0631 ms 74.7%
  triton_mm_366 0.0683 ms 69.0%
  triton_mm_376 0.1044 ms 45.1%
  triton_mm_375 0.1078 ms 43.7%
SingleProcess AUTOTUNE takes 5.6581 seconds
AUTOTUNE convolution(32x256x28x28, 256x256x3x3)
  convolution 0.0440 ms 100.0%
  triton_convolution_383 0.3099 ms 14.2%
  triton_convolution_384 0.4268 ms 10.3%
  triton_convolution_381 0.4673 ms 9.4%
  triton_convolution_378 0.6311 ms 7.0%
  triton_convolution_382 0.6873 ms 6.4%
  triton_convolution_379 0.8759 ms 5.0%
  triton_convolution_380 1.3232 ms 3.3%
SingleProcess AUTOTUNE takes 5.3311 seconds
AUTOTUNE mm(6272x256, 256x1024)
  mm 0.0292 ms 100.0%
  triton_mm_387 0.0308 ms 94.9%
  triton_mm_386 0.0312 ms 93.4%
  triton_mm_392 0.0338 ms 86.3%
  triton_mm_389 0.0352 ms 82.8%
  triton_mm_388 0.0356 ms 82.0%
  triton_mm_385 0.0384 ms 76.0%
  triton_mm_393 0.0386 ms 75.6%
  triton_mm_395 0.0494 ms 59.0%
  triton_mm_391 0.0654 ms 44.6%
SingleProcess AUTOTUNE takes 4.4004 seconds
AUTOTUNE convolution(32x512x28x28, 1024x512x1x1)
  convolution 0.0434 ms 100.0%
  triton_convolution_398 0.1368 ms 31.7%
  triton_convolution_400 0.1403 ms 30.9%
  triton_convolution_397 0.1423 ms 30.5%
  triton_convolution_402 0.1518 ms 28.6%
  triton_convolution_401 0.1570 ms 27.6%
  triton_convolution_403 0.1592 ms 27.3%
  triton_convolution_399 0.9321 ms 4.7%
SingleProcess AUTOTUNE takes 4.7038 seconds
AUTOTUNE mm(6272x1024, 1024x256)
  mm 0.0271 ms 100.0%
  triton_mm_407 0.0295 ms 91.8%
  triton_mm_405 0.0299 ms 90.5%
  triton_mm_406 0.0302 ms 89.8%
  triton_mm_408 0.0304 ms 89.1%
  triton_mm_412 0.0355 ms 76.3%
  triton_mm_411 0.0415 ms 65.3%
  triton_mm_404 0.0429 ms 63.1%
  triton_mm_413 0.0563 ms 48.2%
  triton_mm_414 0.0571 ms 47.4%
SingleProcess AUTOTUNE takes 4.8419 seconds
AUTOTUNE convolution(32x256x14x14, 256x256x3x3)
  convolution 0.0393 ms 100.0%
  triton_convolution_421 0.2276 ms 17.2%
  triton_convolution_419 0.2406 ms 16.3%
  triton_convolution_422 0.3823 ms 10.3%
  triton_convolution_420 0.4155 ms 9.4%
  triton_convolution_416 0.5348 ms 7.3%
  triton_convolution_417 0.7353 ms 5.3%
  triton_convolution_418 1.1621 ms 3.4%
SingleProcess AUTOTUNE takes 4.6721 seconds
AUTOTUNE mm(6272x1024, 1024x512)
  mm 0.0374 ms 100.0%
  triton_mm_1492 0.0507 ms 73.8%
  triton_mm_1491 0.0512 ms 73.1%
  triton_mm_1493 0.0513 ms 72.8%
  triton_mm_1490 0.0516 ms 72.5%
  triton_mm_1497 0.0600 ms 62.3%
  triton_mm_1496 0.0731 ms 51.1%
  triton_mm_1489 0.0746 ms 50.2%
  triton_mm_1495 0.1003 ms 37.3%
  triton_mm_1494 0.1005 ms 37.2%
SingleProcess AUTOTUNE takes 5.3068 seconds
AUTOTUNE convolution(32x512x14x14, 512x512x3x3)
  convolution 0.0492 ms 100.0%
  triton_convolution_1506 0.5782 ms 8.5%
  triton_convolution_1505 0.7365 ms 6.7%
  triton_convolution_1507 0.8118 ms 6.1%
  triton_convolution_1504 0.8345 ms 5.9%
  triton_convolution_1501 1.2055 ms 4.1%
  triton_convolution_1502 1.6834 ms 2.9%
  triton_convolution_1503 1.9237 ms 2.6%
SingleProcess AUTOTUNE takes 4.8256 seconds
AUTOTUNE mm(1568x512, 512x2048)
  mm 0.0244 ms 100.0%
  triton_mm_1509 0.0298 ms 81.9%
  triton_mm_1510 0.0301 ms 81.2%
  triton_mm_1512 0.0308 ms 79.3%
  triton_mm_1511 0.0310 ms 78.8%
  triton_mm_1515 0.0311 ms 78.5%
  triton_mm_1516 0.0361 ms 67.7%
  triton_mm_1508 0.0397 ms 61.5%
  triton_mm_1514 0.0580 ms 42.1%
  triton_mm_1518 0.0581 ms 42.0%
SingleProcess AUTOTUNE takes 5.2896 seconds
AUTOTUNE convolution(32x1024x14x14, 2048x1024x1x1)
  convolution 0.0394 ms 100.0%
  triton_convolution_1520 0.1453 ms 27.1%
  triton_convolution_1523 0.1463 ms 27.0%
  triton_convolution_1525 0.1581 ms 24.9%
  triton_convolution_1524 0.1730 ms 22.8%
  triton_convolution_1526 0.2092 ms 18.9%
  triton_convolution_1521 0.2148 ms 18.4%
  triton_convolution_1522 1.1277 ms 3.5%
SingleProcess AUTOTUNE takes 4.6679 seconds
AUTOTUNE mm(1568x2048, 2048x512)
  mm 0.0274 ms 100.0%
  triton_mm_1530 0.0311 ms 87.9%
  triton_mm_1531 0.0313 ms 87.4%
  triton_mm_1535 0.0333 ms 82.2%
  triton_mm_1529 0.0405 ms 67.6%
  triton_mm_1528 0.0405 ms 67.5%
  triton_mm_1527 0.0509 ms 53.7%
  triton_mm_1532 0.0557 ms 49.2%
  triton_mm_1533 0.0560 ms 48.8%
  triton_mm_1536 0.0575 ms 47.6%
SingleProcess AUTOTUNE takes 4.8928 seconds
AUTOTUNE convolution(32x512x7x7, 512x512x3x3)
  convolution 0.0465 ms 100.0%
  triton_convolution_1544 0.4763 ms 9.8%
  triton_convolution_1543 0.5404 ms 8.6%
  triton_convolution_1542 0.6379 ms 7.3%
  triton_convolution_1545 0.7989 ms 5.8%
  triton_convolution_1539 1.1723 ms 4.0%
  triton_convolution_1540 1.5774 ms 2.9%
  triton_convolution_1541 1.6383 ms 2.8%
SingleProcess AUTOTUNE takes 5.0683 seconds
AUTOTUNE int_mm(32x2048, 2048x1000, 32x1000)
  triton_mm_1599 0.0173 ms 100.0%
  triton_mm_1598 0.0209 ms 82.6%
  triton_mm_1594 0.0211 ms 81.8%
  triton_mm_1597 0.0219 ms 78.9%
  triton_mm_1595 0.0233 ms 74.3%
  triton_mm_1593 0.0255 ms 67.7%
  triton_mm_1592 0.0290 ms 59.7%
  triton_mm_1591 0.0344 ms 50.3%
  triton_mm_1590 0.0387 ms 44.7%
  triton_mm_1589 0.0535 ms 32.3%
SingleProcess AUTOTUNE takes 3.9281 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 23.37it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 28.33it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 29.94it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 30.35it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 30.53it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 30.58it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 30.76it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 30.26it/s]
2.652x
loading model: 0it [00:00, ?it/s]Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /home/cdhernandez/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth

  0%|          | 0.00/44.7M [00:00<?, ?B/s][A
 12%|█▏        | 5.31M/44.7M [00:00<00:00, 55.7MB/s][A
 36%|███▌      | 15.9M/44.7M [00:00<00:00, 88.2MB/s][A
 64%|██████▎   | 28.4M/44.7M [00:00<00:00, 108MB/s] [A
 87%|████████▋ | 38.6M/44.7M [00:00<00:00, 86.4MB/s][A100%|██████████| 44.7M/44.7M [00:00<00:00, 85.7MB/s]
loading model: 0it [00:04, ?it/s]
cuda eval  resnet18                           
AUTOTUNE convolution(8x3x224x224, 64x3x7x7)
  convolution 0.1340 ms 100.0%
  triton_convolution_3 0.2020 ms 66.3%
  triton_convolution_4 0.2093 ms 64.0%
  triton_convolution_5 0.2337 ms 57.3%
  triton_convolution_0 0.2548 ms 52.6%
  triton_convolution_2 0.2759 ms 48.6%
  triton_convolution_1 0.7990 ms 16.8%
SingleProcess AUTOTUNE takes 3.3227 seconds
AUTOTUNE convolution(8x64x56x56, 64x64x3x3)
  convolution 0.0219 ms 100.0%
  triton_convolution_6 0.0685 ms 31.9%
  triton_convolution_11 0.0755 ms 28.9%
  triton_convolution_9 0.1018 ms 21.5%
  triton_convolution_10 0.1119 ms 19.5%
  triton_convolution_12 0.1121 ms 19.5%
  triton_convolution_7 0.1524 ms 14.3%
  triton_convolution_8 0.2449 ms 8.9%
SingleProcess AUTOTUNE takes 3.8311 seconds
AUTOTUNE convolution(8x64x56x56, 128x64x3x3)
  convolution 0.0172 ms 100.0%
  triton_convolution_39 0.0641 ms 26.9%
  triton_convolution_34 0.0892 ms 19.3%
  triton_convolution_38 0.0956 ms 18.0%
  triton_convolution_37 0.1060 ms 16.2%
  triton_convolution_40 0.1135 ms 15.2%
  triton_convolution_35 0.2160 ms 8.0%
  triton_convolution_36 0.2605 ms 6.6%
SingleProcess AUTOTUNE takes 4.7311 seconds
AUTOTUNE convolution(8x128x28x28, 128x128x3x3)
  convolution 0.0193 ms 100.0%
  triton_convolution_46 0.0795 ms 24.2%
  triton_convolution_45 0.1031 ms 18.7%
  triton_convolution_41 0.1123 ms 17.2%
  triton_convolution_44 0.1163 ms 16.6%
  triton_convolution_47 0.1916 ms 10.1%
  triton_convolution_42 0.3787 ms 5.1%
  triton_convolution_43 0.4696 ms 4.1%
SingleProcess AUTOTUNE takes 4.5960 seconds
AUTOTUNE convolution(8x64x56x56, 128x64x1x1)
  triton_convolution_52 0.0100 ms 100.0%
  convolution 0.0105 ms 95.1%
  triton_convolution_53 0.0116 ms 86.7%
  triton_convolution_48 0.0117 ms 85.3%
  triton_convolution_51 0.0120 ms 83.5%
  triton_convolution_54 0.0138 ms 72.5%
  triton_convolution_49 0.0159 ms 63.1%
  triton_convolution_50 0.0356 ms 28.1%
SingleProcess AUTOTUNE takes 4.8558 seconds
AUTOTUNE convolution(8x128x28x28, 256x128x3x3)
  convolution 0.0180 ms 100.0%
  triton_convolution_74 0.1518 ms 11.9%
  triton_convolution_73 0.1686 ms 10.7%
  triton_convolution_72 0.2062 ms 8.8%
  triton_convolution_75 0.2071 ms 8.7%
  triton_convolution_69 0.3077 ms 5.9%
  triton_convolution_70 0.4068 ms 4.4%
  triton_convolution_71 0.5005 ms 3.6%
SingleProcess AUTOTUNE takes 4.9158 seconds
AUTOTUNE convolution(8x256x14x14, 256x256x3x3)
  convolution 0.0197 ms 100.0%
  triton_convolution_80 0.1703 ms 11.6%
  triton_convolution_81 0.2296 ms 8.6%
  triton_convolution_79 0.2379 ms 8.3%
  triton_convolution_82 0.4130 ms 4.8%
  triton_convolution_76 0.5161 ms 3.8%
  triton_convolution_77 0.7707 ms 2.6%
  triton_convolution_78 0.8861 ms 2.2%
SingleProcess AUTOTUNE takes 4.5619 seconds
AUTOTUNE convolution(8x128x28x28, 256x128x1x1)
  convolution 0.0098 ms 100.0%
  triton_convolution_87 0.0121 ms 80.9%
  triton_convolution_86 0.0169 ms 57.9%
  triton_convolution_88 0.0182 ms 53.7%
  triton_convolution_89 0.0182 ms 53.5%
  triton_convolution_84 0.0220 ms 44.3%
  triton_convolution_83 0.0224 ms 43.5%
  triton_convolution_85 0.0626 ms 15.6%
SingleProcess AUTOTUNE takes 4.5870 seconds
AUTOTUNE convolution(8x256x14x14, 512x256x3x3)
  convolution 0.0187 ms 100.0%
  triton_convolution_109 0.2881 ms 6.5%
  triton_convolution_108 0.3360 ms 5.6%
  triton_convolution_110 0.4020 ms 4.6%
  triton_convolution_107 0.4075 ms 4.6%
  triton_convolution_106 0.5352 ms 3.5%
  triton_convolution_104 0.6179 ms 3.0%
  triton_convolution_105 0.8338 ms 2.2%
SingleProcess AUTOTUNE takes 4.6203 seconds
AUTOTUNE convolution(8x512x7x7, 512x512x3x3)
  convolution 0.0286 ms 100.0%
  triton_convolution_115 0.4249 ms 6.7%
  triton_convolution_116 0.4908 ms 5.8%
  triton_convolution_114 0.6492 ms 4.4%
  triton_convolution_113 0.7597 ms 3.8%
  triton_convolution_117 0.7855 ms 3.6%
  triton_convolution_111 1.2086 ms 2.4%
  triton_convolution_112 1.6592 ms 1.7%
SingleProcess AUTOTUNE takes 4.8271 seconds
AUTOTUNE convolution(8x256x14x14, 512x256x1x1)
  convolution 0.0111 ms 100.0%
  triton_convolution_122 0.0164 ms 67.9%
  triton_convolution_121 0.0254 ms 43.7%
  triton_convolution_124 0.0261 ms 42.6%
  triton_convolution_123 0.0270 ms 41.2%
  triton_convolution_119 0.0353 ms 31.4%
  triton_convolution_118 0.0356 ms 31.2%
  triton_convolution_120 0.0645 ms 17.2%
SingleProcess AUTOTUNE takes 4.9928 seconds
AUTOTUNE int_mm(8x512, 512x1000, 8x1000)
  triton_mm_149 0.0092 ms 100.0%
  triton_mm_144 0.0094 ms 97.6%
  triton_mm_147 0.0097 ms 94.7%
  triton_mm_143 0.0102 ms 90.0%
  triton_mm_145 0.0104 ms 88.5%
  triton_mm_148 0.0107 ms 86.0%
  triton_mm_141 0.0131 ms 70.4%
  triton_mm_140 0.0138 ms 66.7%
  triton_mm_139 0.0159 ms 57.9%
  triton_mm_142 0.0169 ms 54.4%
SingleProcess AUTOTUNE takes 4.2148 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 123.28it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 128.52it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 128.12it/s]
6.718x
loading model: 0it [00:00, ?it/s]Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /home/cdhernandez/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth

  0%|          | 0.00/97.8M [00:00<?, ?B/s][A
  8%|▊         | 8.27M/97.8M [00:00<00:01, 86.7MB/s][A
 29%|██▊       | 27.9M/97.8M [00:00<00:00, 157MB/s] [A
 49%|████▉     | 47.7M/97.8M [00:00<00:00, 180MB/s][A
 69%|██████▉   | 67.5M/97.8M [00:00<00:00, 191MB/s][A
 89%|████████▉ | 87.4M/97.8M [00:00<00:00, 197MB/s][A100%|██████████| 97.8M/97.8M [00:00<00:00, 186MB/s]
loading model: 0it [00:02, ?it/s]
cuda eval  resnet50                           
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 62.21it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 69.31it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 72.38it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 71.90it/s]
2.171x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:resnet50_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/resnet50_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

loading model: 0it [00:00, ?it/s]Downloading: "https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth" to /home/cdhernandez/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth

  0%|          | 0.00/95.8M [00:00<?, ?B/s][A
  8%|▊         | 7.33M/95.8M [00:00<00:01, 75.9MB/s][A
 22%|██▏       | 20.7M/95.8M [00:00<00:00, 113MB/s] [A
 38%|███▊      | 36.3M/95.8M [00:00<00:00, 136MB/s][A
 52%|█████▏    | 49.8M/95.8M [00:00<00:00, 138MB/s][A
 67%|██████▋   | 64.6M/95.8M [00:00<00:00, 144MB/s][A
 82%|████████▏ | 78.3M/95.8M [00:00<00:00, 109MB/s][A
 94%|█████████▍| 89.8M/95.8M [00:00<00:00, 106MB/s][A100%|██████████| 95.8M/95.8M [00:00<00:00, 108MB/s]
loading model: 0it [00:03, ?it/s]
cuda eval  resnext50_32x4d                    
AUTOTUNE mm(25088x64, 64x128)
  triton_mm_14 0.0140 ms 100.0%
  triton_mm_8 0.0156 ms 90.3%
  triton_mm_10 0.0156 ms 90.3%
  triton_mm_7 0.0161 ms 87.3%
  mm 0.0164 ms 85.9%
  triton_mm_6 0.0169 ms 83.3%
  triton_mm_9 0.0174 ms 80.8%
  triton_mm_13 0.0175 ms 80.4%
  triton_mm_15 0.0179 ms 78.5%
  triton_mm_16 0.0202 ms 69.5%
SingleProcess AUTOTUNE takes 4.6774 seconds
AUTOTUNE mm(25088x128, 128x256)
  triton_mm_20 0.0243 ms 100.0%
  triton_mm_19 0.0249 ms 97.6%
  triton_mm_25 0.0258 ms 94.2%
  triton_mm_22 0.0268 ms 90.6%
  triton_mm_18 0.0276 ms 87.8%
  triton_mm_21 0.0276 ms 87.8%
  mm 0.0280 ms 86.5%
  triton_mm_26 0.0283 ms 85.8%
  triton_mm_28 0.0379 ms 64.1%
  triton_mm_27 0.0419 ms 58.0%
SingleProcess AUTOTUNE takes 4.2400 seconds
AUTOTUNE mm(25088x64, 64x256)
  triton_mm_31 0.0200 ms 100.0%
  triton_mm_32 0.0202 ms 99.4%
  triton_mm_38 0.0202 ms 99.2%
  triton_mm_34 0.0220 ms 91.0%
  triton_mm_33 0.0224 ms 89.4%
  mm 0.0230 ms 86.9%
  triton_mm_30 0.0231 ms 86.8%
  triton_mm_37 0.0244 ms 82.0%
  triton_mm_39 0.0273 ms 73.4%
  triton_mm_40 0.0276 ms 72.5%
SingleProcess AUTOTUNE takes 4.9304 seconds
AUTOTUNE mm(25088x256, 256x128)
  triton_mm_49 0.0241 ms 100.0%
  triton_mm_45 0.0250 ms 96.7%
  triton_mm_44 0.0255 ms 94.7%
  triton_mm_43 0.0256 ms 94.2%
  triton_mm_46 0.0259 ms 93.1%
  mm 0.0263 ms 91.6%
  triton_mm_50 0.0268 ms 89.9%
  triton_mm_42 0.0306 ms 78.9%
  triton_mm_48 0.0379 ms 63.7%
  triton_mm_47 0.0382 ms 63.2%
SingleProcess AUTOTUNE takes 4.4691 seconds
AUTOTUNE mm(25088x256, 256x256)
  mm 0.0327 ms 100.0%
  triton_mm_92 0.0356 ms 92.1%
  triton_mm_97 0.0371 ms 88.2%
  triton_mm_94 0.0372 ms 88.0%
  triton_mm_91 0.0395 ms 83.0%
  triton_mm_93 0.0410 ms 79.9%
  triton_mm_98 0.0433 ms 75.6%
  triton_mm_90 0.0453 ms 72.3%
  triton_mm_95 0.0668 ms 49.0%
  triton_mm_96 0.0669 ms 48.9%
SingleProcess AUTOTUNE takes 4.9960 seconds
AUTOTUNE mm(6272x256, 256x512)
  mm 0.0181 ms 100.0%
  triton_mm_109 0.0212 ms 85.3%
  triton_mm_105 0.0212 ms 85.2%
  triton_mm_106 0.0213 ms 84.8%
  triton_mm_104 0.0213 ms 84.7%
  triton_mm_103 0.0218 ms 82.8%
  triton_mm_110 0.0241 ms 75.1%
  triton_mm_102 0.0266 ms 68.0%
  triton_mm_112 0.0317 ms 57.0%
  triton_mm_108 0.0366 ms 49.4%
SingleProcess AUTOTUNE takes 4.8090 seconds
AUTOTUNE convolution(8x256x56x56, 512x256x1x1)
  convolution 0.0188 ms 100.0%
  triton_convolution_115 0.0437 ms 43.0%
  triton_convolution_114 0.0452 ms 41.6%
  triton_convolution_117 0.0486 ms 38.7%
  triton_convolution_119 0.0489 ms 38.5%
  triton_convolution_120 0.0493 ms 38.2%
  triton_convolution_118 0.0514 ms 36.6%
  triton_convolution_116 0.2577 ms 7.3%
SingleProcess AUTOTUNE takes 4.9259 seconds
AUTOTUNE mm(6272x512, 512x256)
  mm 0.0183 ms 100.0%
  triton_mm_123 0.0191 ms 96.0%
  triton_mm_124 0.0191 ms 95.8%
  triton_mm_122 0.0192 ms 95.5%
  triton_mm_125 0.0192 ms 95.3%
  triton_mm_129 0.0233 ms 78.6%
  triton_mm_121 0.0249 ms 73.6%
  triton_mm_128 0.0293 ms 62.4%
  triton_mm_131 0.0324 ms 56.5%
  triton_mm_130 0.0340 ms 53.8%
SingleProcess AUTOTUNE takes 4.7998 seconds
AUTOTUNE mm(6272x512, 512x512)
  mm 0.0250 ms 100.0%
  triton_mm_196 0.0310 ms 80.7%
  triton_mm_197 0.0312 ms 80.2%
  triton_mm_195 0.0315 ms 79.5%
  triton_mm_194 0.0319 ms 78.3%
  triton_mm_200 0.0336 ms 74.5%
  triton_mm_201 0.0363 ms 68.9%
  triton_mm_193 0.0417 ms 60.0%
  triton_mm_199 0.0581 ms 43.0%
  triton_mm_198 0.0581 ms 43.0%
SingleProcess AUTOTUNE takes 4.9213 seconds
AUTOTUNE mm(1568x512, 512x1024)
  mm 0.0174 ms 100.0%
  triton_mm_206 0.0178 ms 97.5%
  triton_mm_207 0.0180 ms 96.6%
  triton_mm_208 0.0191 ms 90.8%
  triton_mm_209 0.0192 ms 90.5%
  triton_mm_213 0.0224 ms 77.6%
  triton_mm_205 0.0236 ms 73.5%
  triton_mm_212 0.0275 ms 63.3%
  triton_mm_215 0.0310 ms 56.1%
  triton_mm_214 0.0336 ms 51.7%
SingleProcess AUTOTUNE takes 4.6437 seconds
AUTOTUNE convolution(8x512x28x28, 1024x512x1x1)
  convolution 0.0193 ms 100.0%
  triton_convolution_220 0.0439 ms 43.9%
  triton_convolution_222 0.0464 ms 41.5%
  triton_convolution_221 0.0513 ms 37.6%
  triton_convolution_218 0.0679 ms 28.4%
  triton_convolution_217 0.0692 ms 27.8%
  triton_convolution_223 0.0749 ms 25.7%
  triton_convolution_219 0.3566 ms 5.4%
SingleProcess AUTOTUNE takes 4.9621 seconds
AUTOTUNE mm(1568x1024, 1024x512)
  mm 0.0181 ms 100.0%
  triton_mm_228 0.0192 ms 94.2%
  triton_mm_227 0.0196 ms 92.4%
  triton_mm_232 0.0211 ms 85.8%
  triton_mm_225 0.0236 ms 76.7%
  triton_mm_226 0.0238 ms 76.1%
  triton_mm_230 0.0324 ms 55.9%
  triton_mm_233 0.0324 ms 55.8%
  triton_mm_229 0.0327 ms 55.3%
  triton_mm_224 0.0353 ms 51.4%
SingleProcess AUTOTUNE takes 5.1370 seconds
AUTOTUNE mm(1568x1024, 1024x1024)
  mm 0.0236 ms 100.0%
  triton_mm_345 0.0272 ms 86.6%
  triton_mm_346 0.0273 ms 86.3%
  triton_mm_347 0.0284 ms 82.8%
  triton_mm_348 0.0291 ms 81.1%
  triton_mm_352 0.0339 ms 69.6%
  triton_mm_344 0.0379 ms 62.2%
  triton_mm_351 0.0387 ms 60.9%
  triton_mm_354 0.0521 ms 45.2%
  triton_mm_353 0.0558 ms 42.2%
SingleProcess AUTOTUNE takes 5.1618 seconds
AUTOTUNE mm(392x1024, 1024x2048)
  mm 0.0234 ms 100.0%
  triton_mm_357 0.0251 ms 93.2%
  triton_mm_358 0.0260 ms 90.1%
  triton_mm_364 0.0273 ms 85.7%
  triton_mm_360 0.0280 ms 83.6%
  triton_mm_359 0.0282 ms 83.1%
  triton_mm_362 0.0331 ms 70.7%
  triton_mm_365 0.0335 ms 69.8%
  triton_mm_356 0.0368 ms 63.7%
  triton_mm_363 0.0386 ms 60.7%
SingleProcess AUTOTUNE takes 5.4786 seconds
AUTOTUNE convolution(8x1024x14x14, 2048x1024x1x1)
  convolution 0.0286 ms 100.0%
  triton_convolution_371 0.0765 ms 37.3%
  triton_convolution_374 0.0780 ms 36.7%
  triton_convolution_373 0.0812 ms 35.2%
  triton_convolution_372 0.0851 ms 33.6%
  triton_convolution_369 0.1143 ms 25.0%
  triton_convolution_368 0.1198 ms 23.8%
  triton_convolution_370 0.4623 ms 6.2%
SingleProcess AUTOTUNE takes 4.6565 seconds
AUTOTUNE mm(392x2048, 2048x1024)
  mm 0.0223 ms 100.0%
  triton_mm_378 0.0296 ms 75.5%
  triton_mm_379 0.0297 ms 75.2%
  triton_mm_381 0.0320 ms 69.7%
  triton_mm_383 0.0320 ms 69.7%
  triton_mm_384 0.0341 ms 65.4%
  triton_mm_377 0.0393 ms 56.8%
  triton_mm_376 0.0394 ms 56.7%
  triton_mm_380 0.0431 ms 51.9%
  triton_mm_375 0.0482 ms 46.4%
SingleProcess AUTOTUNE takes 4.9729 seconds
AUTOTUNE int_mm(8x2048, 2048x1000, 8x1000)
  triton_mm_433 0.0164 ms 100.0%
  triton_mm_432 0.0181 ms 90.1%
  triton_mm_428 0.0195 ms 84.0%
  triton_mm_431 0.0202 ms 80.9%
  triton_mm_429 0.0208 ms 78.6%
  triton_mm_427 0.0222 ms 73.5%
  triton_mm_425 0.0322 ms 50.7%
  triton_mm_424 0.0364 ms 44.9%
  triton_mm_423 0.0499 ms 32.8%
  triton_mm_426 0.0539 ms 30.3%
SingleProcess AUTOTUNE takes 4.0758 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:00, 48.03it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 54.53it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 56.84it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 58.04it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 58.67it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 57.31it/s]
6.891x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/__init__.py", line 27, in __init__
    self.model = sam_model_registry[model_type](checkpoint=sam_checkpoint)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/build_sam.py", line 19, in build_sam_vit_h
    return _build_sam(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/build_sam.py", line 108, in _build_sam
    with open(checkpoint, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/.data/sam_vit_h_4b8939.pth'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]Downloading: "https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth" to /home/cdhernandez/.cache/torch/hub/checkpoints/shufflenetv2_x1-5666bf0f80.pth

  0%|          | 0.00/8.79M [00:00<?, ?B/s][A
 66%|██████▌   | 5.77M/8.79M [00:00<00:00, 60.5MB/s][A100%|██████████| 8.79M/8.79M [00:00<00:00, 78.6MB/s]
loading model: 0it [00:02, ?it/s]
cuda eval  shufflenet_v2_x1_0                 
AUTOTUNE convolution(64x3x224x224, 24x3x3x3)
  convolution 0.2110 ms 100.0%
  triton_convolution_4 0.2375 ms 88.8%
  triton_convolution_0 0.2588 ms 81.5%
  triton_convolution_3 0.2611 ms 80.8%
  triton_convolution_2 0.3054 ms 69.1%
  triton_convolution_5 0.3624 ms 58.2%
  triton_convolution_1 0.4222 ms 50.0%
SingleProcess AUTOTUNE takes 2.9557 seconds
AUTOTUNE mm(50176x24, 24x58)
  triton_mm_17 0.0135 ms 100.0%
  triton_mm_14 0.0135 ms 99.8%
  triton_mm_12 0.0140 ms 96.6%
  triton_mm_9 0.0141 ms 95.9%
  triton_mm_8 0.0142 ms 95.3%
  triton_mm_10 0.0142 ms 95.3%
  triton_mm_7 0.0145 ms 93.0%
  triton_mm_6 0.0146 ms 92.7%
  triton_mm_13 0.0146 ms 92.5%
  mm 0.0175 ms 77.0%
SingleProcess AUTOTUNE takes 4.2806 seconds
AUTOTUNE mm(200704x24, 24x58)
  triton_mm_25 0.0295 ms 100.0%
  triton_mm_20 0.0296 ms 99.8%
  triton_mm_18 0.0301 ms 98.1%
  triton_mm_19 0.0301 ms 98.1%
  triton_mm_22 0.0303 ms 97.6%
  triton_mm_21 0.0314 ms 94.1%
  triton_mm_26 0.0320 ms 92.3%
  triton_mm_29 0.0327 ms 90.3%
  triton_mm_24 0.0341 ms 86.6%
  triton_mm_27 0.0459 ms 64.4%
SingleProcess AUTOTUNE takes 3.7540 seconds
AUTOTUNE mm(50176x58, 58x58)
  triton_mm_38 0.0173 ms 100.0%
  triton_mm_31 0.0190 ms 90.8%
  triton_mm_32 0.0192 ms 90.2%
  triton_mm_40 0.0195 ms 88.8%
  triton_mm_37 0.0196 ms 88.2%
  triton_mm_39 0.0196 ms 87.9%
  triton_mm_34 0.0198 ms 87.4%
  triton_mm_30 0.0198 ms 87.1%
  mm 0.0205 ms 84.1%
  triton_mm_41 0.0216 ms 79.9%
SingleProcess AUTOTUNE takes 4.7525 seconds
AUTOTUNE mm(12544x116, 116x116)
  triton_mm_115 0.0134 ms 100.0%
  triton_mm_114 0.0142 ms 94.6%
  triton_mm_121 0.0148 ms 90.5%
  triton_mm_116 0.0149 ms 90.1%
  triton_mm_117 0.0149 ms 89.9%
  triton_mm_118 0.0154 ms 87.1%
  triton_mm_124 0.0162 ms 82.8%
  triton_mm_122 0.0162 ms 82.6%
  mm 0.0167 ms 80.4%
  triton_mm_123 0.0182 ms 73.6%
SingleProcess AUTOTUNE takes 5.1148 seconds
AUTOTUNE mm(50176x116, 116x116)
  triton_mm_133 0.0308 ms 100.0%
  triton_mm_127 0.0322 ms 95.7%
  triton_mm_129 0.0339 ms 91.0%
  triton_mm_128 0.0348 ms 88.7%
  triton_mm_126 0.0357 ms 86.3%
  triton_mm_130 0.0359 ms 86.0%
  mm 0.0369 ms 83.7%
  triton_mm_136 0.0380 ms 81.1%
  triton_mm_134 0.0395 ms 78.1%
  triton_mm_135 0.0443 ms 69.7%
SingleProcess AUTOTUNE takes 4.9607 seconds
AUTOTUNE mm(3136x232, 232x232)
  triton_mm_322 0.0122 ms 100.0%
  triton_mm_320 0.0122 ms 99.7%
  mm 0.0129 ms 94.3%
  triton_mm_319 0.0139 ms 87.7%
  triton_mm_326 0.0140 ms 86.8%
  triton_mm_321 0.0141 ms 86.0%
  triton_mm_318 0.0149 ms 81.6%
  triton_mm_323 0.0169 ms 72.0%
  triton_mm_325 0.0175 ms 69.3%
  triton_mm_327 0.0187 ms 65.1%
SingleProcess AUTOTUNE takes 4.7782 seconds
AUTOTUNE mm(12544x232, 232x232)
  mm 0.0217 ms 100.0%
  triton_mm_337 0.0260 ms 83.4%
  triton_mm_334 0.0287 ms 75.6%
  triton_mm_332 0.0288 ms 75.4%
  triton_mm_338 0.0310 ms 70.2%
  triton_mm_331 0.0319 ms 68.0%
  triton_mm_333 0.0323 ms 67.3%
  triton_mm_330 0.0376 ms 57.7%
  triton_mm_335 0.0390 ms 55.7%
  triton_mm_336 0.0467 ms 46.6%
SingleProcess AUTOTUNE takes 4.5880 seconds
AUTOTUNE mm(3136x464, 464x1024)
  mm 0.0238 ms 100.0%
  triton_mm_428 0.0290 ms 82.0%
  triton_mm_433 0.0291 ms 81.8%
  triton_mm_427 0.0295 ms 80.6%
  triton_mm_429 0.0312 ms 76.1%
  triton_mm_430 0.0320 ms 74.3%
  triton_mm_434 0.0377 ms 63.1%
  triton_mm_426 0.0401 ms 59.3%
  triton_mm_435 0.0555 ms 42.8%
  triton_mm_432 0.0564 ms 42.2%
SingleProcess AUTOTUNE takes 4.5650 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 75.67it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 79.42it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 80.57it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 80.33it/s]
4.278x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/soft_actor_critic/__init__.py", line 13, in <module>
    from .envs import load_gym
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/soft_actor_critic/envs.py", line 6, in <module>
    import gym
ModuleNotFoundError: No module named 'gym'
Failed to import user benchmark module dynamo, error: No module named 'gym'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/speech_transformer/__init__.py", line 15, in <module>
    from .config import SpeechTransformerTrainConfig, SpeechTransformerEvalConfig
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/speech_transformer/config.py", line 4, in <module>
    import kaldi_io
ModuleNotFoundError: No module named 'kaldi_io'
Failed to import user benchmark module dynamo, error: No module named 'kaldi_io'
loading model: 0it [00:00, ?it/s]Downloading: "https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth" to /home/cdhernandez/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth

  0%|          | 0.00/4.73M [00:00<?, ?B/s][A
 69%|██████▊   | 3.24M/4.73M [00:00<00:00, 34.0MB/s][A100%|██████████| 4.73M/4.73M [00:00<00:00, 45.2MB/s]
loading model: 0it [00:01, ?it/s]
cuda eval  squeezenet1_1                      
AUTOTUNE convolution(16x3x224x224, 64x3x3x3)
  triton_convolution_4 0.0878 ms 100.0%
  convolution 0.0930 ms 94.4%
  triton_convolution_3 0.0949 ms 92.5%
  triton_convolution_5 0.1338 ms 65.6%
  triton_convolution_2 0.1400 ms 62.7%
  triton_convolution_0 0.1452 ms 60.5%
  triton_convolution_1 0.3592 ms 24.4%
SingleProcess AUTOTUNE takes 3.2045 seconds
AUTOTUNE addmm(48400x16, 48400x64, 64x16)
  triton_mm_6 0.0137 ms 100.0%
  triton_mm_8 0.0139 ms 99.2%
  triton_mm_16 0.0139 ms 99.2%
  triton_mm_13 0.0140 ms 98.5%
  triton_mm_14 0.0140 ms 98.5%
  triton_mm_7 0.0140 ms 97.8%
  triton_mm_17 0.0140 ms 97.8%
  triton_mm_9 0.0142 ms 96.7%
  triton_mm_15 0.0142 ms 96.5%
  triton_mm_10 0.0147 ms 93.4%
SingleProcess AUTOTUNE takes 4.1254 seconds
AUTOTUNE addmm(48400x64, 48400x16, 16x64)
  triton_mm_21 0.0116 ms 100.0%
  triton_mm_18 0.0117 ms 99.5%
  triton_mm_26 0.0122 ms 95.3%
  triton_mm_19 0.0122 ms 95.0%
  triton_mm_20 0.0124 ms 93.6%
  triton_mm_24 0.0125 ms 92.6%
  triton_mm_22 0.0126 ms 91.9%
  triton_mm_23 0.0127 ms 91.4%
  triton_mm_25 0.0131 ms 88.8%
  triton_mm_27 0.0134 ms 86.9%
SingleProcess AUTOTUNE takes 3.6361 seconds
AUTOTUNE convolution(16x16x55x55, 64x16x3x3)
  convolution 0.0205 ms 100.0%
  triton_convolution_33 0.0291 ms 70.4%
  triton_convolution_32 0.0302 ms 67.8%
  triton_convolution_34 0.0361 ms 56.8%
  triton_convolution_29 0.0388 ms 52.8%
  triton_convolution_31 0.0549 ms 37.3%
  triton_convolution_30 0.0663 ms 31.0%
SingleProcess AUTOTUNE takes 2.9960 seconds
AUTOTUNE addmm(48400x16, 48400x128, 128x16)
  triton_mm_35 0.0199 ms 100.0%
  triton_mm_42 0.0200 ms 99.7%
  triton_mm_45 0.0205 ms 97.2%
  triton_mm_43 0.0209 ms 95.4%
  triton_mm_37 0.0212 ms 94.3%
  triton_mm_46 0.0212 ms 94.0%
  triton_mm_36 0.0213 ms 93.4%
  triton_mm_38 0.0217 ms 91.9%
  triton_mm_40 0.0218 ms 91.5%
  triton_mm_41 0.0219 ms 90.9%
SingleProcess AUTOTUNE takes 3.9894 seconds
AUTOTUNE addmm(11664x32, 11664x128, 128x32)
  triton_mm_65 0.0104 ms 100.0%
  triton_mm_72 0.0108 ms 97.0%
  triton_mm_73 0.0111 ms 93.9%
  triton_mm_67 0.0112 ms 93.4%
  triton_mm_66 0.0113 ms 92.6%
  triton_mm_68 0.0115 ms 91.1%
  triton_mm_69 0.0116 ms 90.1%
  triton_mm_70 0.0116 ms 89.9%
  bias_addmm 0.0117 ms 89.1%
  triton_mm_64 0.0118 ms 88.6%
SingleProcess AUTOTUNE takes 4.0023 seconds
AUTOTUNE addmm(11664x128, 11664x32, 32x128)
  triton_mm_82 0.0099 ms 100.0%
  triton_mm_76 0.0101 ms 97.8%
  triton_mm_78 0.0101 ms 97.2%
  triton_mm_85 0.0105 ms 93.8%
  triton_mm_81 0.0105 ms 93.6%
  triton_mm_84 0.0105 ms 93.6%
  triton_mm_77 0.0108 ms 91.3%
  triton_mm_80 0.0109 ms 90.5%
  triton_mm_83 0.0110 ms 89.3%
  triton_mm_87 0.0111 ms 88.5%
SingleProcess AUTOTUNE takes 5.0065 seconds
AUTOTUNE convolution(16x32x27x27, 128x32x3x3)
  convolution 0.0152 ms 100.0%
  triton_convolution_91 0.0342 ms 44.5%
  triton_convolution_88 0.0351 ms 43.5%
  triton_convolution_94 0.0372 ms 40.9%
  triton_convolution_93 0.0402 ms 37.9%
  triton_convolution_92 0.0483 ms 31.6%
  triton_convolution_89 0.0554 ms 27.5%
  triton_convolution_90 0.0751 ms 20.3%
SingleProcess AUTOTUNE takes 4.8164 seconds
AUTOTUNE addmm(11664x32, 11664x256, 256x32)
  triton_mm_99 0.0130 ms 100.0%
  triton_mm_97 0.0132 ms 98.5%
  triton_mm_104 0.0135 ms 96.2%
  triton_mm_96 0.0136 ms 95.5%
  triton_mm_103 0.0137 ms 95.1%
  triton_mm_101 0.0138 ms 94.4%
  triton_mm_98 0.0140 ms 93.3%
  triton_mm_100 0.0141 ms 92.5%
  bias_addmm 0.0144 ms 90.4%
  triton_mm_95 0.0156 ms 83.6%
SingleProcess AUTOTUNE takes 4.9186 seconds
AUTOTUNE addmm(2704x48, 2704x256, 256x48)
  triton_mm_131 0.0096 ms 100.0%
  triton_mm_129 0.0100 ms 96.5%
  triton_mm_132 0.0100 ms 96.5%
  triton_mm_134 0.0104 ms 92.6%
  triton_mm_135 0.0108 ms 89.6%
  triton_mm_127 0.0109 ms 88.3%
  bias_addmm 0.0114 ms 84.6%
  triton_mm_130 0.0114 ms 84.6%
  triton_mm_128 0.0118 ms 81.4%
  triton_mm_137 0.0144 ms 66.7%
SingleProcess AUTOTUNE takes 4.8999 seconds
AUTOTUNE addmm(2704x192, 2704x48, 48x192)
  triton_mm_147 0.0084 ms 100.0%
  triton_mm_149 0.0085 ms 99.2%
  triton_mm_138 0.0087 ms 96.7%
  triton_mm_141 0.0091 ms 92.3%
  triton_mm_139 0.0092 ms 92.0%
  triton_mm_140 0.0092 ms 91.8%
  triton_mm_144 0.0092 ms 91.6%
  triton_mm_142 0.0092 ms 91.0%
  triton_mm_146 0.0093 ms 90.5%
  triton_mm_148 0.0095 ms 88.3%
SingleProcess AUTOTUNE takes 5.1593 seconds
AUTOTUNE convolution(16x48x13x13, 192x48x3x3)
  convolution 0.0141 ms 100.0%
  triton_convolution_154 0.0432 ms 32.7%
  triton_convolution_153 0.0553 ms 25.5%
  triton_convolution_155 0.0610 ms 23.1%
  triton_convolution_156 0.0614 ms 23.0%
  triton_convolution_150 0.0709 ms 19.9%
  triton_convolution_152 0.0735 ms 19.2%
  triton_convolution_151 0.0883 ms 16.0%
SingleProcess AUTOTUNE takes 5.0269 seconds
AUTOTUNE addmm(2704x48, 2704x384, 384x48)
  triton_mm_162 0.0109 ms 100.0%
  triton_mm_160 0.0115 ms 94.7%
  triton_mm_165 0.0117 ms 92.9%
  triton_mm_166 0.0118 ms 91.9%
  triton_mm_163 0.0119 ms 91.2%
  bias_addmm 0.0123 ms 88.8%
  triton_mm_161 0.0129 ms 84.4%
  triton_mm_158 0.0134 ms 81.1%
  triton_mm_159 0.0140 ms 77.8%
  addmm 0.0156 ms 69.5%
SingleProcess AUTOTUNE takes 4.7646 seconds
AUTOTUNE addmm(2704x64, 2704x384, 384x64)
  triton_mm_197 0.0111 ms 100.0%
  triton_mm_193 0.0113 ms 98.3%
  triton_mm_194 0.0116 ms 95.1%
  triton_mm_196 0.0119 ms 93.0%
  triton_mm_191 0.0120 ms 92.5%
  bias_addmm 0.0123 ms 90.3%
  triton_mm_192 0.0126 ms 87.8%
  triton_mm_189 0.0133 ms 83.4%
  triton_mm_190 0.0147 ms 75.5%
  addmm 0.0156 ms 70.8%
SingleProcess AUTOTUNE takes 5.3371 seconds
AUTOTUNE addmm(2704x256, 2704x64, 64x256)
  triton_mm_202 0.0088 ms 100.0%
  triton_mm_200 0.0089 ms 99.6%
  triton_mm_204 0.0089 ms 99.6%
  triton_mm_201 0.0091 ms 97.2%
  triton_mm_208 0.0096 ms 92.3%
  triton_mm_209 0.0096 ms 91.7%
  triton_mm_203 0.0099 ms 89.6%
  triton_mm_210 0.0099 ms 89.6%
  triton_mm_211 0.0106 ms 83.1%
  triton_mm_205 0.0108 ms 81.8%
SingleProcess AUTOTUNE takes 5.1944 seconds
AUTOTUNE convolution(16x64x13x13, 256x64x3x3)
  convolution 0.0135 ms 100.0%
  triton_convolution_216 0.0475 ms 28.5%
  triton_convolution_217 0.0541 ms 25.0%
  triton_convolution_215 0.0581 ms 23.3%
  triton_convolution_218 0.0634 ms 21.4%
  triton_convolution_213 0.1096 ms 12.4%
  triton_convolution_212 0.1222 ms 11.1%
  triton_convolution_214 0.2278 ms 5.9%
SingleProcess AUTOTUNE takes 4.5803 seconds
AUTOTUNE addmm(2704x64, 2704x512, 512x64)
  triton_mm_224 0.0118 ms 100.0%
  triton_mm_227 0.0124 ms 95.6%
  triton_mm_225 0.0124 ms 95.1%
  triton_mm_228 0.0127 ms 93.4%
  bias_addmm 0.0128 ms 92.7%
  triton_mm_222 0.0132 ms 89.6%
  triton_mm_223 0.0141 ms 84.1%
  triton_mm_220 0.0152 ms 77.9%
  addmm 0.0161 ms 73.7%
  triton_mm_221 0.0169 ms 70.1%
SingleProcess AUTOTUNE takes 4.9891 seconds
AUTOTUNE addmm(2704x1000, 2704x512, 512x1000)
  bias_addmm 0.0251 ms 100.0%
  triton_mm_251 0.0292 ms 86.0%
  triton_mm_252 0.0299 ms 84.0%
  triton_mm_253 0.0324 ms 77.4%
  triton_mm_257 0.0324 ms 77.4%
  triton_mm_254 0.0331 ms 75.9%
  triton_mm_258 0.0340 ms 73.8%
  addmm 0.0352 ms 71.4%
  triton_mm_250 0.0363 ms 69.3%
  triton_mm_260 0.0470 ms 53.5%
SingleProcess AUTOTUNE takes 5.1710 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 278.72it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 279.62it/s]
3.601x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/stable_diffusion_text_encoder/__init__.py", line 11, in <module>
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
ModuleNotFoundError: No module named 'diffusers'
Failed to import user benchmark module dynamo, error: No module named 'diffusers'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/stable_diffusion_unet/__init__.py", line 11, in <module>
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
ModuleNotFoundError: No module named 'diffusers'
Failed to import user benchmark module dynamo, error: No module named 'diffusers'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/timm_efficientdet/__init__.py", line 12, in <module>
    from effdet import create_model, create_loader
ModuleNotFoundError: No module named 'effdet'
Failed to import user benchmark module dynamo, error: No module named 'effdet'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  timm_efficientnet                  
AUTOTUNE convolution(64x3x224x224, 32x3x3x3)
  convolution 0.1983 ms 100.0%
  triton_convolution_4 0.2393 ms 82.9%
  triton_convolution_0 0.2597 ms 76.4%
  triton_convolution_3 0.2632 ms 75.3%
  triton_convolution_2 0.2792 ms 71.0%
  triton_convolution_5 0.3666 ms 54.1%
  triton_convolution_1 0.4287 ms 46.3%
SingleProcess AUTOTUNE takes 2.7386 seconds
AUTOTUNE addmm(64x8, 64x32, 32x8)
  triton_mm_11 0.0065 ms 100.0%
  triton_mm_7 0.0067 ms 97.1%
  triton_mm_8 0.0067 ms 97.1%
  triton_mm_13 0.0067 ms 97.1%
  triton_mm_12 0.0069 ms 94.0%
  triton_mm_10 0.0071 ms 91.4%
  triton_mm_9 0.0071 ms 91.0%
  triton_mm_6 0.0071 ms 90.6%
  bias_addmm 0.0078 ms 82.6%
  addmm 0.0124 ms 51.9%
SingleProcess AUTOTUNE takes 2.3263 seconds
AUTOTUNE addmm(64x32, 64x8, 8x32)
  triton_mm_18 0.0060 ms 100.0%
  triton_mm_20 0.0062 ms 96.9%
  triton_mm_17 0.0063 ms 96.4%
  triton_mm_14 0.0065 ms 93.6%
  triton_mm_21 0.0067 ms 90.9%
  triton_mm_22 0.0067 ms 90.4%
  triton_mm_19 0.0068 ms 88.7%
  triton_mm_16 0.0069 ms 87.5%
  triton_mm_15 0.0071 ms 85.3%
  bias_addmm 0.0080 ms 75.4%
SingleProcess AUTOTUNE takes 2.5503 seconds
AUTOTUNE mm(802816x32, 32x16)
  triton_mm_25 0.0586 ms 100.0%
  triton_mm_30 0.0588 ms 99.7%
  triton_mm_23 0.0588 ms 99.6%
  triton_mm_24 0.0588 ms 99.6%
  triton_mm_27 0.0588 ms 99.6%
  triton_mm_28 0.0591 ms 99.1%
  triton_mm_26 0.0591 ms 99.0%
  triton_mm_29 0.0595 ms 98.5%
  triton_mm_31 0.0595 ms 98.4%
  triton_mm_33 0.0636 ms 92.1%
SingleProcess AUTOTUNE takes 2.8233 seconds
AUTOTUNE mm(802816x16, 16x96)
  triton_mm_43 0.1227 ms 100.0%
  triton_mm_39 0.1236 ms 99.2%
  triton_mm_44 0.1261 ms 97.3%
  triton_mm_34 0.1293 ms 94.9%
  triton_mm_38 0.1298 ms 94.5%
  triton_mm_41 0.1299 ms 94.4%
  triton_mm_35 0.1300 ms 94.3%
  triton_mm_42 0.1306 ms 93.9%
  triton_mm_36 0.1311 ms 93.6%
  triton_mm_37 0.1312 ms 93.5%
SingleProcess AUTOTUNE takes 3.2885 seconds
AUTOTUNE addmm(64x4, 64x96, 96x4)
  triton_mm_47 0.0071 ms 100.0%
  triton_mm_46 0.0071 ms 99.5%
  triton_mm_49 0.0073 ms 96.9%
  triton_mm_48 0.0075 ms 94.4%
  triton_mm_50 0.0075 ms 94.4%
  bias_addmm 0.0080 ms 88.0%
  triton_mm_45 0.0082 ms 86.7%
  triton_mm_51 0.0083 ms 85.3%
  triton_mm_53 0.0089 ms 79.8%
  triton_mm_52 0.0092 ms 77.3%
SingleProcess AUTOTUNE takes 3.1581 seconds
AUTOTUNE addmm(64x96, 64x4, 4x96)
  triton_mm_63 0.0061 ms 100.0%
  triton_mm_58 0.0065 ms 94.6%
  triton_mm_62 0.0065 ms 94.1%
  triton_mm_59 0.0068 ms 90.1%
  triton_mm_60 0.0069 ms 88.9%
  triton_mm_64 0.0071 ms 86.1%
  triton_mm_54 0.0073 ms 83.8%
  triton_mm_56 0.0073 ms 83.8%
  triton_mm_55 0.0075 ms 81.7%
  bias_addmm 0.0080 ms 76.8%
SingleProcess AUTOTUNE takes 3.6951 seconds
AUTOTUNE mm(200704x96, 96x24)
  triton_mm_68 0.0476 ms 100.0%
  triton_mm_70 0.0477 ms 99.7%
  triton_mm_69 0.0483 ms 98.5%
  triton_mm_72 0.0492 ms 96.7%
  triton_mm_73 0.0492 ms 96.7%
  triton_mm_67 0.0499 ms 95.3%
  triton_mm_65 0.0499 ms 95.3%
  triton_mm_66 0.0502 ms 94.8%
  triton_mm_75 0.0512 ms 92.8%
  triton_mm_76 0.0535 ms 88.9%
SingleProcess AUTOTUNE takes 3.7818 seconds
AUTOTUNE mm(200704x24, 24x144)
  triton_mm_79 0.0556 ms 100.0%
  triton_mm_77 0.0593 ms 93.9%
  triton_mm_81 0.0602 ms 92.5%
  triton_mm_85 0.0626 ms 88.9%
  triton_mm_88 0.0628 ms 88.6%
  triton_mm_82 0.0644 ms 86.4%
  triton_mm_78 0.0670 ms 83.1%
  triton_mm_86 0.0681 ms 81.7%
  triton_mm_84 0.0694 ms 80.2%
  triton_mm_83 0.0716 ms 77.8%
SingleProcess AUTOTUNE takes 4.1180 seconds
AUTOTUNE addmm(64x6, 64x144, 144x6)
  triton_mm_93 0.0071 ms 100.0%
  triton_mm_92 0.0073 ms 96.5%
  triton_mm_91 0.0075 ms 94.0%
  triton_mm_94 0.0078 ms 90.2%
  triton_mm_95 0.0079 ms 89.3%
  triton_mm_90 0.0079 ms 89.1%
  triton_mm_89 0.0096 ms 73.9%
  bias_addmm 0.0096 ms 73.4%
  triton_mm_97 0.0112 ms 63.0%
  triton_mm_96 0.0114 ms 61.9%
SingleProcess AUTOTUNE takes 3.1814 seconds
AUTOTUNE addmm(64x144, 64x6, 6x144)
  triton_mm_107 0.0062 ms 100.0%
  triton_mm_104 0.0064 ms 97.5%
  triton_mm_103 0.0065 ms 96.5%
  triton_mm_106 0.0067 ms 93.3%
  triton_mm_108 0.0068 ms 91.3%
  triton_mm_98 0.0069 ms 90.7%
  triton_mm_102 0.0072 ms 86.7%
  triton_mm_100 0.0074 ms 84.8%
  triton_mm_101 0.0077 ms 80.6%
  triton_mm_105 0.0077 ms 80.6%
SingleProcess AUTOTUNE takes 4.3382 seconds
AUTOTUNE mm(200704x144, 144x24)
  triton_mm_114 0.0634 ms 100.0%
  triton_mm_113 0.0638 ms 99.3%
  triton_mm_111 0.0641 ms 98.8%
  triton_mm_112 0.0642 ms 98.7%
  triton_mm_116 0.0647 ms 98.0%
  triton_mm_117 0.0657 ms 96.5%
  triton_mm_119 0.0666 ms 95.1%
  triton_mm_120 0.0704 ms 90.0%
  triton_mm_110 0.0722 ms 87.7%
  mm 0.0725 ms 87.4%
SingleProcess AUTOTUNE takes 3.7532 seconds
AUTOTUNE mm(50176x144, 144x40)
  triton_mm_160 0.0241 ms 100.0%
  triton_mm_155 0.0266 ms 90.6%
  triton_mm_161 0.0273 ms 88.1%
  mm 0.0277 ms 86.8%
  triton_mm_157 0.0278 ms 86.5%
  triton_mm_156 0.0289 ms 83.2%
  triton_mm_153 0.0293 ms 82.1%
  triton_mm_154 0.0300 ms 80.3%
  triton_mm_158 0.0305 ms 78.9%
  triton_mm_164 0.0312 ms 77.2%
SingleProcess AUTOTUNE takes 4.5228 seconds
AUTOTUNE mm(50176x40, 40x240)
  triton_mm_173 0.0334 ms 100.0%
  triton_mm_167 0.0337 ms 99.2%
  mm 0.0364 ms 91.7%
  triton_mm_169 0.0367 ms 91.0%
  triton_mm_176 0.0382 ms 87.4%
  triton_mm_166 0.0406 ms 82.3%
  triton_mm_172 0.0419 ms 79.8%
  triton_mm_168 0.0423 ms 79.0%
  triton_mm_165 0.0440 ms 76.0%
  triton_mm_174 0.0442 ms 75.7%
SingleProcess AUTOTUNE takes 4.4837 seconds
AUTOTUNE addmm(64x10, 64x240, 240x10)
  triton_mm_181 0.0077 ms 100.0%
  triton_mm_183 0.0080 ms 96.8%
  triton_mm_180 0.0086 ms 89.3%
  triton_mm_182 0.0086 ms 89.3%
  triton_mm_179 0.0089 ms 87.0%
  triton_mm_178 0.0092 ms 83.4%
  bias_addmm 0.0093 ms 83.1%
  triton_mm_177 0.0118 ms 65.3%
  addmm 0.0130 ms 59.2%
  triton_mm_185 0.0151 ms 51.1%
SingleProcess AUTOTUNE takes 3.2449 seconds
AUTOTUNE addmm(64x240, 64x10, 10x240)
  triton_mm_195 0.0062 ms 100.0%
  triton_mm_192 0.0065 ms 96.5%
  triton_mm_196 0.0065 ms 96.5%
  triton_mm_190 0.0069 ms 90.7%
  triton_mm_194 0.0069 ms 90.7%
  triton_mm_186 0.0069 ms 90.3%
  triton_mm_188 0.0069 ms 90.3%
  triton_mm_191 0.0069 ms 90.3%
  triton_mm_187 0.0077 ms 80.6%
  triton_mm_189 0.0077 ms 80.6%
SingleProcess AUTOTUNE takes 4.2190 seconds
AUTOTUNE mm(50176x240, 240x40)
  triton_mm_204 0.0369 ms 100.0%
  triton_mm_205 0.0374 ms 98.8%
  triton_mm_201 0.0375 ms 98.5%
  triton_mm_199 0.0380 ms 97.2%
  triton_mm_200 0.0386 ms 95.7%
  mm 0.0388 ms 95.1%
  triton_mm_198 0.0391 ms 94.4%
  triton_mm_202 0.0420 ms 87.9%
  triton_mm_197 0.0440 ms 83.9%
  triton_mm_206 0.0472 ms 78.3%
SingleProcess AUTOTUNE takes 4.5883 seconds
AUTOTUNE mm(12544x240, 240x80)
  mm 0.0157 ms 100.0%
  triton_mm_243 0.0160 ms 98.2%
  triton_mm_242 0.0165 ms 95.0%
  triton_mm_245 0.0166 ms 94.6%
  triton_mm_244 0.0168 ms 93.3%
  triton_mm_241 0.0183 ms 85.8%
  triton_mm_249 0.0183 ms 85.8%
  triton_mm_248 0.0187 ms 83.8%
  triton_mm_250 0.0199 ms 78.9%
  triton_mm_246 0.0204 ms 76.9%
SingleProcess AUTOTUNE takes 4.7791 seconds
AUTOTUNE mm(12544x80, 80x480)
  triton_mm_255 0.0214 ms 100.0%
  triton_mm_254 0.0218 ms 98.2%
  triton_mm_260 0.0223 ms 96.2%
  triton_mm_256 0.0226 ms 95.0%
  triton_mm_257 0.0229 ms 93.6%
  triton_mm_253 0.0231 ms 92.8%
  mm 0.0241 ms 88.9%
  triton_mm_263 0.0268 ms 79.9%
  triton_mm_261 0.0283 ms 75.9%
  triton_mm_264 0.0316 ms 67.9%
SingleProcess AUTOTUNE takes 4.6002 seconds
AUTOTUNE addmm(64x20, 64x480, 480x20)
  triton_mm_269 0.0092 ms 100.0%
  triton_mm_272 0.0101 ms 91.2%
  triton_mm_271 0.0103 ms 89.5%
  triton_mm_268 0.0106 ms 87.4%
  triton_mm_267 0.0108 ms 85.3%
  bias_addmm 0.0119 ms 77.9%
  triton_mm_266 0.0121 ms 76.7%
  addmm 0.0156 ms 59.2%
  triton_mm_270 0.0188 ms 49.2%
  triton_mm_265 0.0190 ms 48.7%
SingleProcess AUTOTUNE takes 3.5603 seconds
AUTOTUNE addmm(64x480, 64x20, 20x480)
  triton_mm_280 0.0067 ms 100.0%
  triton_mm_286 0.0069 ms 96.7%
  triton_mm_279 0.0069 ms 96.3%
  triton_mm_284 0.0069 ms 96.3%
  triton_mm_281 0.0071 ms 93.3%
  triton_mm_275 0.0073 ms 91.2%
  triton_mm_283 0.0074 ms 89.5%
  triton_mm_285 0.0076 ms 88.1%
  triton_mm_277 0.0078 ms 85.6%
  triton_mm_276 0.0080 ms 83.5%
SingleProcess AUTOTUNE takes 4.3122 seconds
AUTOTUNE mm(12544x480, 480x80)
  triton_mm_288 0.0210 ms 100.0%
  triton_mm_290 0.0211 ms 99.5%
  mm 0.0216 ms 97.0%
  triton_mm_289 0.0218 ms 96.1%
  triton_mm_291 0.0222 ms 94.7%
  triton_mm_295 0.0254 ms 82.7%
  triton_mm_287 0.0276 ms 76.1%
  triton_mm_292 0.0278 ms 75.5%
  triton_mm_296 0.0288 ms 72.8%
  triton_mm_294 0.0304 ms 69.1%
SingleProcess AUTOTUNE takes 5.2700 seconds
AUTOTUNE mm(12544x480, 480x112)
  mm 0.0214 ms 100.0%
  triton_mm_381 0.0221 ms 96.5%
  triton_mm_383 0.0229 ms 93.2%
  triton_mm_380 0.0235 ms 90.9%
  triton_mm_382 0.0240 ms 89.2%
  triton_mm_379 0.0280 ms 76.3%
  triton_mm_387 0.0282 ms 75.7%
  triton_mm_386 0.0304 ms 70.2%
  triton_mm_384 0.0330 ms 64.8%
  triton_mm_385 0.0359 ms 59.6%
SingleProcess AUTOTUNE takes 5.1172 seconds
AUTOTUNE mm(12544x112, 112x672)
  triton_mm_393 0.0299 ms 100.0%
  triton_mm_398 0.0323 ms 92.6%
  triton_mm_392 0.0345 ms 86.7%
  triton_mm_395 0.0345 ms 86.7%
  mm 0.0353 ms 84.8%
  triton_mm_399 0.0370 ms 80.9%
  triton_mm_394 0.0371 ms 80.7%
  triton_mm_391 0.0374 ms 79.9%
  triton_mm_401 0.0475 ms 62.9%
  triton_mm_400 0.0540 ms 55.4%
SingleProcess AUTOTUNE takes 4.4192 seconds
AUTOTUNE addmm(64x28, 64x672, 672x28)
  triton_mm_407 0.0108 ms 100.0%
  triton_mm_409 0.0118 ms 91.4%
  triton_mm_406 0.0119 ms 91.1%
  bias_addmm 0.0122 ms 88.7%
  triton_mm_410 0.0124 ms 86.9%
  triton_mm_405 0.0125 ms 86.4%
  triton_mm_404 0.0144 ms 74.9%
  addmm 0.0153 ms 70.6%
  triton_mm_403 0.0231 ms 46.9%
  triton_mm_408 0.0239 ms 45.2%
SingleProcess AUTOTUNE takes 3.9335 seconds
AUTOTUNE addmm(64x672, 64x28, 28x672)
  triton_mm_422 0.0065 ms 100.0%
  triton_mm_418 0.0072 ms 90.2%
  triton_mm_419 0.0072 ms 90.2%
  triton_mm_415 0.0073 ms 88.2%
  triton_mm_421 0.0075 ms 86.3%
  triton_mm_424 0.0075 ms 86.3%
  triton_mm_417 0.0076 ms 85.2%
  triton_mm_413 0.0078 ms 83.1%
  triton_mm_420 0.0082 ms 78.9%
  triton_mm_423 0.0082 ms 78.6%
SingleProcess AUTOTUNE takes 4.6770 seconds
AUTOTUNE mm(12544x672, 672x112)
  mm 0.0268 ms 100.0%
  triton_mm_429 0.0287 ms 93.6%
  triton_mm_427 0.0291 ms 92.2%
  triton_mm_428 0.0294 ms 91.3%
  triton_mm_426 0.0308 ms 87.0%
  triton_mm_433 0.0352 ms 76.2%
  triton_mm_425 0.0386 ms 69.6%
  triton_mm_432 0.0396 ms 67.8%
  triton_mm_430 0.0421 ms 63.8%
  triton_mm_431 0.0444 ms 60.4%
SingleProcess AUTOTUNE takes 5.4007 seconds
AUTOTUNE mm(3136x672, 672x192)
  mm 0.0146 ms 100.0%
  triton_mm_521 0.0158 ms 92.1%
  triton_mm_520 0.0171 ms 85.4%
  triton_mm_525 0.0175 ms 83.2%
  triton_mm_519 0.0182 ms 80.1%
  triton_mm_518 0.0191 ms 76.5%
  triton_mm_523 0.0201 ms 72.7%
  triton_mm_522 0.0203 ms 71.9%
  triton_mm_526 0.0246 ms 59.2%
  triton_mm_517 0.0273 ms 53.5%
SingleProcess AUTOTUNE takes 4.9377 seconds
AUTOTUNE mm(3136x192, 192x1152)
  triton_mm_530 0.0188 ms 100.0%
  triton_mm_531 0.0191 ms 98.7%
  triton_mm_533 0.0213 ms 88.6%
  triton_mm_532 0.0218 ms 86.3%
  triton_mm_529 0.0221 ms 85.1%
  triton_mm_537 0.0226 ms 83.5%
  mm 0.0246 ms 76.6%
  triton_mm_536 0.0249 ms 75.8%
  triton_mm_539 0.0287 ms 65.7%
  triton_mm_535 0.0346 ms 54.5%
SingleProcess AUTOTUNE takes 4.6820 seconds
AUTOTUNE addmm(64x48, 64x1152, 1152x48)
  bias_addmm 0.0117 ms 100.0%
  addmm 0.0148 ms 79.6%
  triton_mm_544 0.0151 ms 77.8%
  triton_mm_545 0.0151 ms 77.6%
  triton_mm_548 0.0154 ms 76.5%
  triton_mm_547 0.0168 ms 69.8%
  triton_mm_543 0.0172 ms 68.1%
  triton_mm_542 0.0220 ms 53.4%
  triton_mm_541 0.0377 ms 31.2%
  triton_mm_550 0.0380 ms 30.9%
SingleProcess AUTOTUNE takes 4.1691 seconds
AUTOTUNE addmm(64x1152, 64x48, 48x1152)
  triton_mm_559 0.0073 ms 100.0%
  triton_mm_560 0.0075 ms 97.4%
  bias_addmm 0.0076 ms 96.6%
  triton_mm_556 0.0076 ms 96.6%
  triton_mm_553 0.0077 ms 94.6%
  triton_mm_557 0.0078 ms 93.5%
  triton_mm_562 0.0082 ms 89.1%
  triton_mm_555 0.0083 ms 88.4%
  triton_mm_552 0.0084 ms 87.4%
  triton_mm_554 0.0084 ms 87.1%
SingleProcess AUTOTUNE takes 4.6222 seconds
AUTOTUNE mm(3136x1152, 1152x192)
  mm 0.0186 ms 100.0%
  triton_mm_567 0.0217 ms 85.8%
  triton_mm_566 0.0222 ms 84.0%
  triton_mm_571 0.0228 ms 81.9%
  triton_mm_565 0.0270 ms 69.0%
  triton_mm_564 0.0272 ms 68.4%
  triton_mm_568 0.0291 ms 64.0%
  triton_mm_569 0.0301 ms 61.9%
  triton_mm_572 0.0348 ms 53.6%
  triton_mm_563 0.0418 ms 44.5%
SingleProcess AUTOTUNE takes 4.7881 seconds
AUTOTUNE mm(3136x1152, 1152x320)
  mm 0.0245 ms 100.0%
  triton_mm_703 0.0285 ms 86.0%
  triton_mm_702 0.0293 ms 83.6%
  triton_mm_709 0.0301 ms 81.5%
  triton_mm_704 0.0314 ms 78.2%
  triton_mm_705 0.0315 ms 77.9%
  triton_mm_706 0.0425 ms 57.6%
  triton_mm_701 0.0427 ms 57.5%
  triton_mm_708 0.0433 ms 56.6%
  triton_mm_707 0.0439 ms 55.8%
SingleProcess AUTOTUNE takes 4.9516 seconds
AUTOTUNE mm(3136x320, 320x1280)
  triton_mm_715 0.0252 ms 100.0%
  triton_mm_714 0.0254 ms 99.1%
  triton_mm_717 0.0284 ms 88.4%
  triton_mm_716 0.0287 ms 87.5%
  triton_mm_713 0.0301 ms 83.5%
  mm 0.0316 ms 79.6%
  triton_mm_721 0.0322 ms 78.1%
  triton_mm_720 0.0355 ms 70.9%
  triton_mm_723 0.0467 ms 53.9%
  triton_mm_719 0.0523 ms 48.1%
SingleProcess AUTOTUNE takes 5.2978 seconds
AUTOTUNE int_mm(64x1280, 1280x1000, 64x1000)
  triton_mm_735 0.0156 ms 100.0%
  triton_mm_730 0.0169 ms 92.4%
  triton_mm_731 0.0172 ms 91.0%
  triton_mm_733 0.0175 ms 89.1%
  triton_mm_734 0.0196 ms 79.5%
  triton_mm_729 0.0204 ms 76.7%
  triton_mm_728 0.0234 ms 66.8%
  triton_mm_727 0.0253 ms 61.8%
  triton_mm_726 0.0279 ms 56.0%
  triton_mm_725 0.0362 ms 43.1%
SingleProcess AUTOTUNE takes 5.5262 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:00, 45.46it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 51.94it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 54.10it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 55.10it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 55.69it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 54.43it/s]
2.452x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  timm_nfnet                         
AUTOTUNE convolution(128x3x193x193, 16x3x3x3)
  triton_convolution_1 0.2017 ms 100.0%
  convolution 0.2368 ms 85.2%
  triton_convolution_4 0.3282 ms 61.4%
  triton_convolution_2 0.3356 ms 60.1%
  triton_convolution_0 0.3941 ms 51.2%
  triton_convolution_3 0.4892 ms 41.2%
SingleProcess AUTOTUNE takes 2.1200 seconds
AUTOTUNE convolution(128x16x96x96, 32x16x3x3)
  triton_convolution_6 0.3228 ms 100.0%
  triton_convolution_5 0.3741 ms 86.3%
  triton_convolution_8 0.4002 ms 80.7%
  convolution 0.4014 ms 80.4%
  triton_convolution_10 0.4052 ms 79.7%
  triton_convolution_9 0.4449 ms 72.6%
  triton_convolution_7 0.6988 ms 46.2%
SingleProcess AUTOTUNE takes 2.6804 seconds
AUTOTUNE convolution(128x32x96x96, 64x32x3x3)
  convolution 0.6391 ms 100.0%
  triton_convolution_12 0.9437 ms 67.7%
  triton_convolution_14 0.9444 ms 67.7%
  triton_convolution_11 0.9971 ms 64.1%
  triton_convolution_15 1.0318 ms 61.9%
  triton_convolution_17 1.0810 ms 59.1%
  triton_convolution_16 1.1087 ms 57.6%
  triton_convolution_13 2.1753 ms 29.4%
SingleProcess AUTOTUNE takes 4.0623 seconds
AUTOTUNE convolution(128x64x97x97, 128x64x3x3)
  convolution 0.5877 ms 100.0%
  triton_convolution_22 1.0599 ms 55.4%
  triton_convolution_24 1.0626 ms 55.3%
  triton_convolution_19 1.1156 ms 52.7%
  triton_convolution_21 1.1999 ms 49.0%
  triton_convolution_18 1.3137 ms 44.7%
  triton_convolution_23 1.5089 ms 38.9%
  triton_convolution_20 4.7445 ms 12.4%
SingleProcess AUTOTUNE takes 5.0693 seconds
AUTOTUNE convolution(128x128x48x48, 128x128x1x1)
  convolution 0.1182 ms 100.0%
  triton_convolution_29 0.3623 ms 32.6%
  triton_convolution_26 0.3811 ms 31.0%
  triton_convolution_25 0.3825 ms 30.9%
  triton_convolution_28 0.4253 ms 27.8%
  triton_convolution_31 0.4538 ms 26.1%
  triton_convolution_30 0.4649 ms 25.4%
  triton_convolution_27 0.7422 ms 15.9%
  conv1x1_via_mm 1.2731 ms 9.3%
SingleProcess AUTOTUNE takes 4.5909 seconds
AUTOTUNE convolution(128x128x48x48, 128x128x3x3)
  convolution 0.6094 ms 100.0%
  triton_convolution_33 1.5675 ms 38.9%
  triton_convolution_38 1.7219 ms 35.4%
  triton_convolution_36 1.9804 ms 30.8%
  triton_convolution_35 2.2517 ms 27.1%
  triton_convolution_32 2.2693 ms 26.9%
  triton_convolution_37 2.7964 ms 21.8%
  triton_convolution_34 4.3839 ms 13.9%
SingleProcess AUTOTUNE takes 4.4846 seconds
AUTOTUNE convolution(128x128x48x48, 256x128x1x1)
  convolution 0.2009 ms 100.0%
  triton_convolution_50 0.6970 ms 28.8%
  triton_convolution_47 0.7189 ms 27.9%
  triton_convolution_49 0.8164 ms 24.6%
  triton_convolution_52 0.8817 ms 22.8%
  triton_convolution_51 0.9009 ms 22.3%
  triton_convolution_46 0.9408 ms 21.4%
  triton_convolution_48 1.4195 ms 14.2%
  conv1x1_via_mm 2.2164 ms 9.1%
SingleProcess AUTOTUNE takes 5.2475 seconds
AUTOTUNE convolution(128x256x1x1, 128x256x1x1)
  convolution 0.0093 ms 100.0%
  triton_convolution_57 0.0161 ms 57.8%
  triton_convolution_58 0.0168 ms 55.2%
  triton_convolution_53 0.0201 ms 46.2%
  triton_convolution_59 0.0204 ms 45.7%
  triton_convolution_54 0.0220 ms 42.3%
  triton_convolution_56 0.0252 ms 37.0%
  triton_convolution_55 0.0284 ms 32.7%
  conv1x1_via_mm 0.1272 ms 7.3%
SingleProcess AUTOTUNE takes 3.1126 seconds
AUTOTUNE convolution(128x128x1x1, 256x128x1x1)
  convolution 0.0076 ms 100.0%
  triton_convolution_64 0.0108 ms 70.1%
  triton_convolution_66 0.0131 ms 57.9%
  triton_convolution_61 0.0144 ms 52.8%
  triton_convolution_62 0.0148 ms 51.1%
  triton_convolution_63 0.0156 ms 48.6%
  triton_convolution_65 0.0171 ms 44.3%
  triton_convolution_60 0.0363 ms 20.9%
  conv1x1_via_mm 0.1184 ms 6.4%
SingleProcess AUTOTUNE takes 3.3458 seconds
AUTOTUNE convolution(128x256x48x48, 256x256x1x1)
  convolution 0.3042 ms 100.0%
  triton_convolution_75 0.9486 ms 32.1%
  triton_convolution_78 1.0901 ms 27.9%
  triton_convolution_77 1.0980 ms 27.7%
  triton_convolution_80 1.1806 ms 25.8%
  triton_convolution_79 1.2932 ms 23.5%
  triton_convolution_74 1.3048 ms 23.3%
  triton_convolution_76 2.3883 ms 12.7%
  conv1x1_via_mm 2.7982 ms 10.9%
SingleProcess AUTOTUNE takes 5.5636 seconds
AUTOTUNE convolution(128x256x24x24, 512x256x1x1)
  convolution 0.1386 ms 100.0%
  triton_convolution_82 0.5218 ms 26.6%
  triton_convolution_85 0.5715 ms 24.3%
  triton_convolution_84 0.5846 ms 23.7%
  triton_convolution_87 0.6223 ms 22.3%
  triton_convolution_86 0.6755 ms 20.5%
  triton_convolution_81 0.7086 ms 19.6%
  triton_convolution_83 1.2624 ms 11.0%
  conv1x1_via_mm 1.5210 ms 9.1%
SingleProcess AUTOTUNE takes 5.6625 seconds
AUTOTUNE convolution(128x512x1x1, 256x512x1x1)
  convolution 0.0106 ms 100.0%
  triton_convolution_92 0.0261 ms 40.6%
  triton_convolution_94 0.0357 ms 29.7%
  triton_convolution_89 0.0385 ms 27.5%
  triton_convolution_91 0.0431 ms 24.6%
  triton_convolution_93 0.0445 ms 23.8%
  triton_convolution_90 0.0525 ms 20.2%
  conv1x1_via_mm 0.1239 ms 8.6%
  triton_convolution_88 0.1271 ms 8.3%
SingleProcess AUTOTUNE takes 3.4100 seconds
AUTOTUNE convolution(128x256x1x1, 512x256x1x1)
  convolution 0.0094 ms 100.0%
  triton_convolution_99 0.0161 ms 58.0%
  triton_convolution_101 0.0206 ms 45.3%
  triton_convolution_96 0.0224 ms 41.8%
  triton_convolution_98 0.0244 ms 38.4%
  triton_convolution_100 0.0264 ms 35.4%
  triton_convolution_97 0.0280 ms 33.5%
  triton_convolution_95 0.0653 ms 14.3%
  conv1x1_via_mm 0.1162 ms 8.1%
SingleProcess AUTOTUNE takes 3.6764 seconds
AUTOTUNE convolution(128x512x24x24, 256x512x1x1)
  convolution 0.2771 ms 100.0%
  triton_convolution_110 0.4426 ms 62.6%
  triton_convolution_112 0.4894 ms 56.6%
  triton_convolution_113 0.5101 ms 54.3%
  triton_convolution_115 0.5233 ms 52.9%
  triton_convolution_114 0.5797 ms 47.8%
  triton_convolution_109 0.6129 ms 45.2%
  conv1x1_via_mm 1.0971 ms 25.3%
  triton_convolution_111 1.1468 ms 24.2%
SingleProcess AUTOTUNE takes 5.3356 seconds
AUTOTUNE convolution(128x512x24x24, 768x512x1x1)
  convolution 0.5654 ms 100.0%
  triton_convolution_138 1.1680 ms 48.4%
  triton_convolution_140 1.3437 ms 42.1%
  triton_convolution_143 1.4245 ms 39.7%
  triton_convolution_141 1.4358 ms 39.4%
  triton_convolution_142 1.5794 ms 35.8%
  triton_convolution_137 1.6311 ms 34.7%
  conv1x1_via_mm 2.5221 ms 22.4%
  triton_convolution_139 3.3013 ms 17.1%
SingleProcess AUTOTUNE takes 5.2688 seconds
AUTOTUNE convolution(128x768x12x12, 1536x768x1x1)
  convolution 0.3381 ms 100.0%
  triton_convolution_145 0.6340 ms 53.3%
  triton_convolution_147 0.8649 ms 39.1%
  triton_convolution_150 0.8747 ms 38.7%
  triton_convolution_144 0.8806 ms 38.4%
  triton_convolution_148 0.9203 ms 36.7%
  triton_convolution_149 1.0995 ms 30.8%
  conv1x1_via_mm 1.7960 ms 18.8%
  triton_convolution_146 1.9995 ms 16.9%
SingleProcess AUTOTUNE takes 5.4685 seconds
AUTOTUNE convolution(128x1536x1x1, 768x1536x1x1)
  convolution 0.0146 ms 100.0%
  triton_convolution_155 0.0688 ms 21.3%
  triton_convolution_157 0.0751 ms 19.5%
  triton_convolution_152 0.1128 ms 13.0%
  triton_convolution_154 0.1130 ms 12.9%
  triton_convolution_156 0.1193 ms 12.3%
  conv1x1_via_mm 0.1393 ms 10.5%
  triton_convolution_153 0.1439 ms 10.2%
  triton_convolution_151 0.5349 ms 2.7%
SingleProcess AUTOTUNE takes 4.3555 seconds
AUTOTUNE convolution(128x768x1x1, 1536x768x1x1)
  convolution 0.0124 ms 100.0%
  triton_convolution_162 0.0385 ms 32.4%
  triton_convolution_164 0.0419 ms 29.7%
  triton_convolution_159 0.0595 ms 20.9%
  triton_convolution_161 0.0608 ms 20.5%
  triton_convolution_163 0.0647 ms 19.2%
  triton_convolution_160 0.0758 ms 16.4%
  conv1x1_via_mm 0.1424 ms 8.7%
  triton_convolution_158 0.2834 ms 4.4%
SingleProcess AUTOTUNE takes 3.4922 seconds
AUTOTUNE convolution(128x512x12x12, 1536x512x1x1)
  convolution 0.2654 ms 100.0%
  triton_convolution_166 0.4441 ms 59.8%
  triton_convolution_169 0.5658 ms 46.9%
  triton_convolution_168 0.5792 ms 45.8%
  triton_convolution_171 0.5794 ms 45.8%
  triton_convolution_165 0.6126 ms 43.3%
  triton_convolution_170 0.7651 ms 34.7%
  triton_convolution_167 1.2479 ms 21.3%
  conv1x1_via_mm 1.4189 ms 18.7%
SingleProcess AUTOTUNE takes 5.2801 seconds
AUTOTUNE convolution(128x1536x12x12, 768x1536x1x1)
  convolution 0.3121 ms 100.0%
  triton_convolution_173 0.5818 ms 53.7%
  triton_convolution_178 0.8118 ms 38.4%
  triton_convolution_172 0.8598 ms 36.3%
  triton_convolution_176 0.8891 ms 35.1%
  triton_convolution_175 0.9731 ms 32.1%
  triton_convolution_177 1.2347 ms 25.3%
  conv1x1_via_mm 1.5759 ms 19.8%
  triton_convolution_174 1.9274 ms 16.2%
SingleProcess AUTOTUNE takes 5.9882 seconds
AUTOTUNE convolution(128x768x6x6, 1536x768x1x1)
  convolution 0.0999 ms 100.0%
  triton_convolution_320 0.1575 ms 63.4%
  triton_convolution_322 0.2067 ms 48.3%
  triton_convolution_325 0.2179 ms 45.8%
  triton_convolution_323 0.2290 ms 43.6%
  triton_convolution_324 0.2310 ms 43.2%
  triton_convolution_319 0.2577 ms 38.8%
  triton_convolution_321 0.5268 ms 19.0%
  conv1x1_via_mm 0.6553 ms 15.2%
SingleProcess AUTOTUNE takes 5.9769 seconds
AUTOTUNE convolution(128x1536x6x6, 1536x1536x1x1)
  convolution 0.1518 ms 100.0%
  triton_convolution_341 0.2924 ms 51.9%
  triton_convolution_343 0.3836 ms 39.6%
  triton_convolution_346 0.4014 ms 37.8%
  triton_convolution_345 0.4340 ms 35.0%
  triton_convolution_344 0.4352 ms 34.9%
  triton_convolution_340 0.4982 ms 30.5%
  triton_convolution_342 1.0256 ms 14.8%
  conv1x1_via_mm 1.1205 ms 13.5%
SingleProcess AUTOTUNE takes 5.6579 seconds
AUTOTUNE convolution(128x1536x6x6, 768x1536x1x1)
  convolution 0.0957 ms 100.0%
  triton_convolution_348 0.1638 ms 58.4%
  triton_convolution_350 0.1994 ms 48.0%
  triton_convolution_353 0.2101 ms 45.5%
  triton_convolution_351 0.2293 ms 41.7%
  triton_convolution_352 0.2326 ms 41.1%
  triton_convolution_347 0.2647 ms 36.1%
  conv1x1_via_mm 0.5970 ms 16.0%
  triton_convolution_349 0.6209 ms 15.4%
SingleProcess AUTOTUNE takes 5.2617 seconds
AUTOTUNE convolution(128x1536x6x6, 3072x1536x1x1)
  convolution 0.2814 ms 100.0%
  triton_convolution_404 0.5624 ms 50.0%
  triton_convolution_406 0.7519 ms 37.4%
  triton_convolution_409 0.7828 ms 35.9%
  triton_convolution_408 0.8285 ms 34.0%
  triton_convolution_407 0.8446 ms 33.3%
  triton_convolution_403 0.9711 ms 29.0%
  triton_convolution_405 1.8334 ms 15.3%
  conv1x1_via_mm 2.1736 ms 12.9%
SingleProcess AUTOTUNE takes 5.2707 seconds
AUTOTUNE int_mm(128x3072, 3072x1000, 128x1000)
  triton_mm_415 0.0290 ms 100.0%
  triton_mm_416 0.0292 ms 99.3%
  triton_mm_418 0.0301 ms 96.3%
  triton_mm_420 0.0303 ms 95.7%
  triton_mm_413 0.0430 ms 67.5%
  triton_mm_414 0.0435 ms 66.7%
  triton_mm_419 0.0518 ms 56.1%
  triton_mm_412 0.0544 ms 53.4%
  triton_mm_411 0.0547 ms 53.1%
  triton_mm_410 0.0718 ms 40.4%
SingleProcess AUTOTUNE takes 6.6938 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:04,  7.02it/s]running benchmark:  10%|█         | 3/30 [00:00<00:02, 11.55it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:01, 13.06it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 13.81it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 14.22it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 14.47it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:01, 14.64it/s]running benchmark:  50%|█████     | 15/30 [00:01<00:01, 14.71it/s]running benchmark:  57%|█████▋    | 17/30 [00:01<00:00, 14.78it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 14.82it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 14.85it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 14.90it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 14.90it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 14.90it/s]running benchmark:  97%|█████████▋| 29/30 [00:02<00:00, 14.90it/s]running benchmark: 100%|██████████| 30/30 [00:02<00:00, 14.39it/s]
1.917x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
cuda eval  timm_regnet                        
AUTOTUNE convolution(32x3x224x224, 32x3x3x3)
  convolution 0.1011 ms 100.0%
  triton_convolution_1 0.1011 ms 99.9%
  triton_convolution_0 0.1122 ms 90.1%
  triton_convolution_5 0.1262 ms 80.1%
  triton_convolution_3 0.1276 ms 79.2%
  triton_convolution_4 0.1306 ms 77.4%
  triton_convolution_2 0.2293 ms 44.1%
SingleProcess AUTOTUNE takes 2.6103 seconds
AUTOTUNE convolution(32x32x112x112, 224x32x1x1)
  convolution 0.1612 ms 100.0%
  triton_convolution_10 0.5649 ms 28.5%
  triton_convolution_11 0.6793 ms 23.7%
  triton_convolution_6 0.6875 ms 23.5%
  triton_convolution_9 0.6944 ms 23.2%
  triton_convolution_7 0.6978 ms 23.1%
  triton_convolution_12 0.7351 ms 21.9%
  triton_convolution_8 0.8857 ms 18.2%
  conv1x1_via_mm 2.3204 ms 6.9%
SingleProcess AUTOTUNE takes 4.9666 seconds
AUTOTUNE convolution(32x224x1x1, 8x224x1x1)
  convolution 0.0104 ms 100.0%
  triton_convolution_15 0.0131 ms 79.3%
  triton_convolution_14 0.0179 ms 58.0%
  triton_convolution_13 0.0185 ms 56.2%
  conv1x1_via_mm 0.2198 ms 4.7%
SingleProcess AUTOTUNE takes 1.3159 seconds
AUTOTUNE convolution(32x8x1x1, 224x8x1x1)
  triton_convolution_21 0.0063 ms 100.0%
  triton_convolution_19 0.0069 ms 90.8%
  triton_convolution_18 0.0071 ms 88.3%
  triton_convolution_17 0.0074 ms 85.5%
  triton_convolution_20 0.0082 ms 76.7%
  convolution 0.0088 ms 71.9%
  triton_convolution_16 0.0099 ms 63.8%
  conv1x1_via_mm 0.2189 ms 2.9%
SingleProcess AUTOTUNE takes 2.2165 seconds
AUTOTUNE convolution(32x224x56x56, 224x224x1x1)
  convolution 0.2285 ms 100.0%
  triton_convolution_23 0.3390 ms 67.4%
  triton_convolution_26 0.3412 ms 67.0%
  triton_convolution_25 0.3645 ms 62.7%
  triton_convolution_28 0.4009 ms 57.0%
  triton_convolution_22 0.4121 ms 55.4%
  triton_convolution_27 0.4270 ms 53.5%
  triton_convolution_24 0.7033 ms 32.5%
  conv1x1_via_mm 0.8292 ms 27.6%
SingleProcess AUTOTUNE takes 5.6549 seconds
AUTOTUNE convolution(32x32x112x112, 224x32x1x1)
  triton_convolution_33 0.1644 ms 100.0%
  convolution 0.1675 ms 98.2%
  triton_convolution_34 0.1869 ms 88.0%
  triton_convolution_32 0.1963 ms 83.8%
  triton_convolution_29 0.1972 ms 83.4%
  triton_convolution_30 0.2085 ms 78.9%
  triton_convolution_35 0.2188 ms 75.1%
  triton_convolution_31 0.2955 ms 55.7%
SingleProcess AUTOTUNE takes 4.8726 seconds
AUTOTUNE convolution(32x224x1x1, 56x224x1x1)
  convolution 0.0100 ms 100.0%
  triton_convolution_46 0.0132 ms 76.0%
  triton_convolution_45 0.0166 ms 60.2%
  triton_convolution_44 0.0174 ms 57.4%
  triton_convolution_43 0.0190 ms 52.6%
  conv1x1_via_mm 0.1216 ms 8.2%
SingleProcess AUTOTUNE takes 1.4839 seconds
AUTOTUNE convolution(32x56x1x1, 224x56x1x1)
  triton_convolution_53 0.0078 ms 100.0%
  triton_convolution_50 0.0088 ms 88.8%
  convolution 0.0090 ms 87.0%
  triton_convolution_51 0.0091 ms 86.6%
  triton_convolution_49 0.0092 ms 85.7%
  triton_convolution_48 0.0098 ms 80.2%
  triton_convolution_52 0.0114 ms 69.0%
  triton_convolution_47 0.0146 ms 53.6%
  conv1x1_via_mm 0.1387 ms 5.7%
SingleProcess AUTOTUNE takes 2.5801 seconds
AUTOTUNE convolution(32x224x56x56, 448x224x1x1)
  convolution 0.2048 ms 100.0%
  triton_convolution_62 0.5831 ms 35.1%
  triton_convolution_65 0.6022 ms 34.0%
  triton_convolution_64 0.6969 ms 29.4%
  triton_convolution_67 0.6992 ms 29.3%
  triton_convolution_61 0.7928 ms 25.8%
  triton_convolution_66 0.8090 ms 25.3%
  triton_convolution_63 1.3440 ms 15.2%
  conv1x1_via_mm 1.4330 ms 14.3%
SingleProcess AUTOTUNE takes 5.3030 seconds
AUTOTUNE convolution(32x448x1x1, 56x448x1x1)
  convolution 0.0109 ms 100.0%
  triton_convolution_71 0.0202 ms 54.1%
  triton_convolution_70 0.0222 ms 49.2%
  triton_convolution_68 0.0322 ms 33.9%
  triton_convolution_69 0.0323 ms 33.8%
  conv1x1_via_mm 0.1270 ms 8.6%
SingleProcess AUTOTUNE takes 1.4469 seconds
AUTOTUNE convolution(32x56x1x1, 448x56x1x1)
  triton_convolution_78 0.0084 ms 100.0%
  triton_convolution_74 0.0086 ms 97.4%
  convolution 0.0088 ms 94.9%
  triton_convolution_73 0.0093 ms 89.4%
  triton_convolution_75 0.0094 ms 88.8%
  triton_convolution_76 0.0096 ms 87.0%
  triton_convolution_77 0.0122 ms 68.7%
  triton_convolution_72 0.0146 ms 57.1%
  conv1x1_via_mm 0.1181 ms 7.1%
SingleProcess AUTOTUNE takes 2.5215 seconds
AUTOTUNE convolution(32x448x28x28, 448x448x1x1)
  convolution 0.0722 ms 100.0%
  triton_convolution_80 0.1971 ms 36.6%
  triton_convolution_83 0.2310 ms 31.3%
  triton_convolution_85 0.2394 ms 30.2%
  triton_convolution_82 0.2687 ms 26.9%
  triton_convolution_79 0.2847 ms 25.4%
  triton_convolution_84 0.3385 ms 21.3%
  triton_convolution_81 0.4665 ms 15.5%
  conv1x1_via_mm 0.5163 ms 14.0%
SingleProcess AUTOTUNE takes 5.3588 seconds
AUTOTUNE convolution(32x224x56x56, 448x224x1x1)
  convolution 0.1562 ms 100.0%
  triton_convolution_86 0.1799 ms 86.8%
  triton_convolution_89 0.1985 ms 78.7%
  triton_convolution_87 0.2022 ms 77.3%
  triton_convolution_92 0.2131 ms 73.3%
  triton_convolution_90 0.2167 ms 72.1%
  triton_convolution_91 0.2248 ms 69.5%
  triton_convolution_88 0.4564 ms 34.2%
SingleProcess AUTOTUNE takes 5.1653 seconds
AUTOTUNE convolution(32x448x1x1, 112x448x1x1)
  convolution 0.0099 ms 100.0%
  triton_convolution_104 0.0203 ms 48.5%
  triton_convolution_105 0.0205 ms 48.2%
  triton_convolution_103 0.0236 ms 41.8%
  triton_convolution_100 0.0290 ms 34.0%
  triton_convolution_102 0.0323 ms 30.6%
  triton_convolution_101 0.0325 ms 30.3%
  conv1x1_via_mm 0.1195 ms 8.2%
SingleProcess AUTOTUNE takes 2.1845 seconds
AUTOTUNE convolution(32x112x1x1, 448x112x1x1)
  convolution 0.0094 ms 100.0%
  triton_convolution_112 0.0106 ms 88.5%
  triton_convolution_108 0.0122 ms 76.7%
  triton_convolution_110 0.0134 ms 69.8%
  triton_convolution_109 0.0136 ms 68.8%
  triton_convolution_107 0.0147 ms 63.8%
  triton_convolution_111 0.0194 ms 48.3%
  triton_convolution_106 0.0228 ms 41.1%
  conv1x1_via_mm 0.1227 ms 7.6%
SingleProcess AUTOTUNE takes 2.7654 seconds
AUTOTUNE convolution(32x448x28x28, 896x448x1x1)
  convolution 0.1162 ms 100.0%
  triton_convolution_202 0.3466 ms 33.5%
  triton_convolution_207 0.4320 ms 26.9%
  triton_convolution_205 0.4390 ms 26.5%
  triton_convolution_204 0.4417 ms 26.3%
  triton_convolution_201 0.5349 ms 21.7%
  triton_convolution_206 0.6307 ms 18.4%
  triton_convolution_203 0.8421 ms 13.8%
  conv1x1_via_mm 0.8974 ms 12.9%
SingleProcess AUTOTUNE takes 5.2855 seconds
AUTOTUNE convolution(32x896x1x1, 112x896x1x1)
  convolution 0.0127 ms 100.0%
  triton_convolution_212 0.0348 ms 36.5%
  triton_convolution_213 0.0356 ms 35.5%
  triton_convolution_211 0.0410 ms 30.9%
  triton_convolution_208 0.0526 ms 24.1%
  triton_convolution_210 0.0579 ms 21.9%
  triton_convolution_209 0.0612 ms 20.7%
  conv1x1_via_mm 0.1214 ms 10.4%
SingleProcess AUTOTUNE takes 2.3882 seconds
AUTOTUNE convolution(32x112x1x1, 896x112x1x1)
  convolution 0.0086 ms 100.0%
  triton_convolution_220 0.0108 ms 79.6%
  triton_convolution_216 0.0124 ms 69.4%
  triton_convolution_218 0.0137 ms 63.1%
  triton_convolution_217 0.0140 ms 61.9%
  triton_convolution_215 0.0144 ms 60.0%
  triton_convolution_219 0.0200 ms 43.3%
  triton_convolution_214 0.0235 ms 36.8%
  conv1x1_via_mm 0.1218 ms 7.1%
SingleProcess AUTOTUNE takes 2.6334 seconds
AUTOTUNE convolution(32x896x14x14, 896x896x1x1)
  convolution 0.0982 ms 100.0%
  triton_convolution_222 0.1637 ms 60.0%
  triton_convolution_224 0.2027 ms 48.5%
  triton_convolution_227 0.2154 ms 45.6%
  triton_convolution_225 0.2334 ms 42.1%
  triton_convolution_226 0.2418 ms 40.6%
  triton_convolution_221 0.2795 ms 35.1%
  triton_convolution_223 0.3846 ms 25.5%
  conv1x1_via_mm 0.4216 ms 23.3%
SingleProcess AUTOTUNE takes 5.1091 seconds
AUTOTUNE convolution(32x448x28x28, 896x448x1x1)
  convolution 0.0960 ms 100.0%
  triton_convolution_231 0.1322 ms 72.6%
  triton_convolution_232 0.1323 ms 72.6%
  triton_convolution_234 0.1461 ms 65.7%
  triton_convolution_229 0.1508 ms 63.7%
  triton_convolution_233 0.1596 ms 60.2%
  triton_convolution_228 0.2113 ms 45.4%
  triton_convolution_230 0.4535 ms 21.2%
SingleProcess AUTOTUNE takes 5.0410 seconds
AUTOTUNE convolution(32x896x1x1, 224x896x1x1)
  convolution 0.0115 ms 100.0%
  triton_convolution_246 0.0350 ms 32.9%
  triton_convolution_248 0.0358 ms 32.2%
  triton_convolution_245 0.0412 ms 27.9%
  triton_convolution_247 0.0546 ms 21.1%
  triton_convolution_244 0.0579 ms 19.9%
  triton_convolution_243 0.0657 ms 17.5%
  triton_convolution_242 0.1193 ms 9.7%
  conv1x1_via_mm 0.1419 ms 8.1%
SingleProcess AUTOTUNE takes 2.7488 seconds
AUTOTUNE convolution(32x224x1x1, 896x224x1x1)
  convolution 0.0094 ms 100.0%
  triton_convolution_253 0.0139 ms 67.4%
  triton_convolution_252 0.0151 ms 61.9%
  triton_convolution_255 0.0171 ms 55.0%
  triton_convolution_251 0.0181 ms 51.7%
  triton_convolution_254 0.0195 ms 48.2%
  triton_convolution_250 0.0219 ms 42.8%
  triton_convolution_249 0.0346 ms 27.1%
  conv1x1_via_mm 0.1373 ms 6.8%
SingleProcess AUTOTUNE takes 2.5945 seconds
AUTOTUNE convolution(32x896x14x14, 2240x896x1x1)
  convolution 0.1925 ms 100.0%
  triton_convolution_516 0.3659 ms 52.6%
  triton_convolution_518 0.4312 ms 44.6%
  triton_convolution_521 0.4493 ms 42.8%
  triton_convolution_519 0.4984 ms 38.6%
  triton_convolution_520 0.5086 ms 37.8%
  triton_convolution_515 0.6657 ms 28.9%
  conv1x1_via_mm 0.8976 ms 21.4%
  triton_convolution_517 0.9407 ms 20.5%
SingleProcess AUTOTUNE takes 5.2945 seconds
AUTOTUNE convolution(32x2240x1x1, 224x2240x1x1)
  convolution 0.0185 ms 100.0%
  triton_convolution_528 0.0793 ms 23.3%
  triton_convolution_526 0.0796 ms 23.2%
  triton_convolution_525 0.0947 ms 19.5%
  conv1x1_via_mm 0.1181 ms 15.6%
  triton_convolution_527 0.1272 ms 14.5%
  triton_convolution_524 0.1361 ms 13.6%
  triton_convolution_523 0.1609 ms 11.5%
  triton_convolution_522 0.2863 ms 6.4%
SingleProcess AUTOTUNE takes 2.8635 seconds
AUTOTUNE convolution(32x224x1x1, 2240x224x1x1)
  convolution 0.0109 ms 100.0%
  triton_convolution_533 0.0137 ms 79.4%
  triton_convolution_532 0.0162 ms 67.2%
  triton_convolution_535 0.0181 ms 60.0%
  triton_convolution_534 0.0200 ms 54.5%
  triton_convolution_531 0.0204 ms 53.3%
  triton_convolution_530 0.0225 ms 48.4%
  triton_convolution_529 0.0373 ms 29.2%
  conv1x1_via_mm 0.1372 ms 7.9%
SingleProcess AUTOTUNE takes 2.5870 seconds
AUTOTUNE convolution(32x2240x7x7, 2240x2240x1x1)
  convolution 0.1228 ms 100.0%
  triton_convolution_540 0.3129 ms 39.2%
  triton_convolution_537 0.3542 ms 34.7%
  triton_convolution_539 0.3577 ms 34.3%
  triton_convolution_542 0.3885 ms 31.6%
  triton_convolution_541 0.5523 ms 22.2%
  triton_convolution_536 0.6237 ms 19.7%
  conv1x1_via_mm 0.7071 ms 17.4%
  triton_convolution_538 0.8642 ms 14.2%
SingleProcess AUTOTUNE takes 5.4708 seconds
AUTOTUNE convolution(32x896x14x14, 2240x896x1x1)
  convolution 0.0806 ms 100.0%
  triton_convolution_546 0.1562 ms 51.6%
  triton_convolution_547 0.1880 ms 42.9%
  triton_convolution_549 0.1900 ms 42.4%
  triton_convolution_548 0.2005 ms 40.2%
  triton_convolution_544 0.3067 ms 26.3%
  triton_convolution_543 0.3529 ms 22.8%
  triton_convolution_545 0.6117 ms 13.2%
SingleProcess AUTOTUNE takes 5.2789 seconds
AUTOTUNE int_mm(32x2240, 2240x1000, 32x1000)
  triton_mm_560 0.0189 ms 100.0%
  triton_mm_558 0.0224 ms 84.2%
  triton_mm_555 0.0225 ms 83.9%
  triton_mm_559 0.0229 ms 82.5%
  triton_mm_556 0.0241 ms 78.4%
  triton_mm_554 0.0265 ms 71.3%
  triton_mm_553 0.0302 ms 62.4%
  triton_mm_552 0.0371 ms 50.9%
  triton_mm_551 0.0407 ms 46.4%
  triton_mm_550 0.0569 ms 33.2%
SingleProcess AUTOTUNE takes 5.0948 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 23.87it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 29.06it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 30.77it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 31.58it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 31.99it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 32.20it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 32.35it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 31.54it/s]
1.455x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_resnest                       
AUTOTUNE convolution(32x32x112x112, 32x32x3x3)
  convolution 0.1846 ms 100.0%
  triton_convolution_7 0.1904 ms 97.0%
  triton_convolution_6 0.2098 ms 88.0%
  triton_convolution_9 0.2878 ms 64.2%
  triton_convolution_12 0.3040 ms 60.7%
  triton_convolution_10 0.3106 ms 59.4%
  triton_convolution_11 0.3508 ms 52.6%
  triton_convolution_8 0.4272 ms 43.2%
SingleProcess AUTOTUNE takes 3.7743 seconds
AUTOTUNE convolution(32x32x112x112, 64x32x3x3)
  convolution 0.2327 ms 100.0%
  triton_convolution_16 0.3480 ms 66.9%
  triton_convolution_13 0.3496 ms 66.6%
  triton_convolution_14 0.3534 ms 65.8%
  triton_convolution_17 0.3577 ms 65.1%
  triton_convolution_18 0.3890 ms 59.8%
  triton_convolution_19 0.4044 ms 57.6%
  triton_convolution_15 0.7869 ms 29.6%
SingleProcess AUTOTUNE takes 3.9293 seconds
AUTOTUNE convolution(32x64x56x56, 64x64x1x1)
  convolution 0.0370 ms 100.0%
  triton_convolution_20 0.0584 ms 63.4%
  triton_convolution_24 0.0588 ms 62.9%
  triton_convolution_25 0.0654 ms 56.6%
  triton_convolution_23 0.0719 ms 51.5%
  triton_convolution_21 0.0803 ms 46.1%
  triton_convolution_26 0.0880 ms 42.1%
  triton_convolution_22 0.1073 ms 34.5%
  conv1x1_via_mm 0.2690 ms 13.8%
SingleProcess AUTOTUNE takes 4.2282 seconds
AUTOTUNE convolution(32x64x1x1, 32x64x1x1)
  triton_convolution_28 0.0075 ms 100.0%
  triton_convolution_27 0.0084 ms 89.3%
  convolution 0.0084 ms 89.0%
  triton_convolution_29 0.0089 ms 83.9%
  conv1x1_via_mm 0.1163 ms 6.4%
SingleProcess AUTOTUNE takes 1.0579 seconds
AUTOTUNE convolution(32x32x1x1, 128x32x1x1)
  triton_convolution_32 0.0066 ms 100.0%
  triton_convolution_35 0.0074 ms 89.6%
  triton_convolution_30 0.0076 ms 86.2%
  triton_convolution_34 0.0077 ms 85.5%
  triton_convolution_33 0.0084 ms 78.6%
  triton_convolution_31 0.0091 ms 72.8%
  convolution 0.0093 ms 70.8%
  conv1x1_via_mm 0.1179 ms 5.6%
SingleProcess AUTOTUNE takes 2.0840 seconds
AUTOTUNE convolution(32x64x56x56, 256x64x1x1)
  convolution 0.0757 ms 100.0%
  triton_convolution_40 0.1917 ms 39.5%
  triton_convolution_37 0.2465 ms 30.7%
  triton_convolution_41 0.2677 ms 28.3%
  triton_convolution_39 0.2684 ms 28.2%
  triton_convolution_42 0.2788 ms 27.2%
  triton_convolution_36 0.2842 ms 26.7%
  triton_convolution_38 0.3412 ms 22.2%
  conv1x1_via_mm 0.6764 ms 11.2%
SingleProcess AUTOTUNE takes 5.0366 seconds
AUTOTUNE convolution(32x256x56x56, 128x256x1x1)
  convolution 0.0781 ms 100.0%
  triton_convolution_51 0.2118 ms 36.9%
  triton_convolution_54 0.2128 ms 36.7%
  triton_convolution_50 0.2208 ms 35.4%
  triton_convolution_53 0.2271 ms 34.4%
  triton_convolution_56 0.2444 ms 31.9%
  triton_convolution_55 0.2560 ms 30.5%
  triton_convolution_52 0.4757 ms 16.4%
  conv1x1_via_mm 0.6185 ms 12.6%
SingleProcess AUTOTUNE takes 4.7291 seconds
AUTOTUNE convolution(32x128x1x1, 64x128x1x1)
  convolution 0.0094 ms 100.0%
  triton_convolution_60 0.0097 ms 96.7%
  triton_convolution_58 0.0125 ms 74.9%
  triton_convolution_57 0.0131 ms 71.5%
  triton_convolution_59 0.0137 ms 68.6%
  conv1x1_via_mm 0.1364 ms 6.9%
SingleProcess AUTOTUNE takes 1.5208 seconds
AUTOTUNE convolution(32x64x1x1, 256x64x1x1)
  triton_convolution_65 0.0082 ms 100.0%
  triton_convolution_63 0.0084 ms 97.7%
  convolution 0.0089 ms 91.8%
  triton_convolution_67 0.0091 ms 90.1%
  triton_convolution_62 0.0100 ms 81.8%
  triton_convolution_66 0.0107 ms 76.9%
  triton_convolution_61 0.0145 ms 56.6%
  triton_convolution_64 0.0203 ms 40.4%
  conv1x1_via_mm 0.1256 ms 6.5%
SingleProcess AUTOTUNE takes 2.5193 seconds
AUTOTUNE convolution(32x128x28x28, 512x128x1x1)
  convolution 0.0547 ms 100.0%
  triton_convolution_69 0.0866 ms 63.2%
  triton_convolution_72 0.1014 ms 53.9%
  triton_convolution_71 0.1182 ms 46.3%
  triton_convolution_74 0.1191 ms 45.9%
  triton_convolution_68 0.1295 ms 42.2%
  triton_convolution_73 0.1453 ms 37.6%
  triton_convolution_70 0.1726 ms 31.7%
  conv1x1_via_mm 0.3950 ms 13.8%
SingleProcess AUTOTUNE takes 5.7208 seconds
AUTOTUNE convolution(32x256x28x28, 512x256x1x1)
  convolution 0.0761 ms 100.0%
  triton_convolution_76 0.1285 ms 59.2%
  triton_convolution_79 0.1554 ms 48.9%
  triton_convolution_78 0.1687 ms 45.1%
  triton_convolution_81 0.1689 ms 45.0%
  triton_convolution_75 0.1896 ms 40.1%
  triton_convolution_80 0.2228 ms 34.1%
  triton_convolution_77 0.2916 ms 26.1%
  conv1x1_via_mm 0.4918 ms 15.5%
SingleProcess AUTOTUNE takes 5.5915 seconds
AUTOTUNE convolution(32x512x28x28, 256x512x1x1)
  convolution 0.0508 ms 100.0%
  triton_convolution_83 0.1259 ms 40.4%
  triton_convolution_86 0.1591 ms 32.0%
  triton_convolution_88 0.1698 ms 30.0%
  triton_convolution_82 0.1720 ms 29.6%
  triton_convolution_85 0.1724 ms 29.5%
  triton_convolution_87 0.2132 ms 23.8%
  triton_convolution_84 0.3188 ms 15.9%
  conv1x1_via_mm 0.3611 ms 14.1%
SingleProcess AUTOTUNE takes 5.1023 seconds
AUTOTUNE convolution(32x256x1x1, 128x256x1x1)
  convolution 0.0099 ms 100.0%
  triton_convolution_93 0.0140 ms 70.9%
  triton_convolution_94 0.0155 ms 63.9%
  triton_convolution_92 0.0169 ms 58.7%
  triton_convolution_89 0.0190 ms 52.1%
  triton_convolution_91 0.0203 ms 48.8%
  triton_convolution_90 0.0216 ms 46.0%
  conv1x1_via_mm 0.1393 ms 7.1%
SingleProcess AUTOTUNE takes 2.1301 seconds
AUTOTUNE convolution(32x128x1x1, 512x128x1x1)
  convolution 0.0075 ms 100.0%
  triton_convolution_99 0.0099 ms 75.8%
  triton_convolution_97 0.0122 ms 61.7%
  triton_convolution_101 0.0135 ms 55.7%
  triton_convolution_96 0.0137 ms 54.8%
  triton_convolution_100 0.0140 ms 53.7%
  triton_convolution_98 0.0182 ms 41.3%
  triton_convolution_95 0.0220 ms 34.2%
  conv1x1_via_mm 0.1200 ms 6.3%
SingleProcess AUTOTUNE takes 2.5827 seconds
AUTOTUNE convolution(32x256x14x14, 1024x256x1x1)
  convolution 0.0568 ms 100.0%
  triton_convolution_103 0.0625 ms 90.8%
  triton_convolution_105 0.0801 ms 70.9%
  triton_convolution_108 0.0846 ms 67.1%
  triton_convolution_106 0.0865 ms 65.7%
  triton_convolution_107 0.0914 ms 62.1%
  triton_convolution_102 0.1012 ms 56.1%
  triton_convolution_104 0.1595 ms 35.6%
  conv1x1_via_mm 0.2945 ms 19.3%
SingleProcess AUTOTUNE takes 5.2162 seconds
AUTOTUNE convolution(32x512x14x14, 1024x512x1x1)
  convolution 0.0751 ms 100.0%
  triton_convolution_110 0.1030 ms 72.9%
  triton_convolution_112 0.1340 ms 56.1%
  triton_convolution_115 0.1432 ms 52.4%
  triton_convolution_113 0.1520 ms 49.4%
  triton_convolution_114 0.1520 ms 49.4%
  triton_convolution_109 0.1766 ms 42.5%
  triton_convolution_111 0.2916 ms 25.8%
  conv1x1_via_mm 0.3241 ms 23.2%
SingleProcess AUTOTUNE takes 5.2262 seconds
AUTOTUNE convolution(32x1024x14x14, 512x1024x1x1)
  convolution 0.0727 ms 100.0%
  triton_convolution_117 0.1058 ms 68.8%
  triton_convolution_122 0.1371 ms 53.0%
  triton_convolution_119 0.1398 ms 52.0%
  triton_convolution_120 0.1536 ms 47.4%
  triton_convolution_121 0.1583 ms 46.0%
  triton_convolution_116 0.1686 ms 43.1%
  conv1x1_via_mm 0.2860 ms 25.4%
  triton_convolution_118 0.3266 ms 22.3%
SingleProcess AUTOTUNE takes 5.2105 seconds
AUTOTUNE convolution(32x512x1x1, 256x512x1x1)
  convolution 0.0104 ms 100.0%
  triton_convolution_127 0.0228 ms 45.8%
  triton_convolution_129 0.0252 ms 41.4%
  triton_convolution_126 0.0277 ms 37.7%
  triton_convolution_128 0.0350 ms 29.8%
  triton_convolution_125 0.0359 ms 29.1%
  triton_convolution_124 0.0412 ms 25.3%
  triton_convolution_123 0.0737 ms 14.2%
  conv1x1_via_mm 0.1292 ms 8.1%
SingleProcess AUTOTUNE takes 2.5541 seconds
AUTOTUNE convolution(32x256x1x1, 1024x256x1x1)
  convolution 0.0093 ms 100.0%
  triton_convolution_134 0.0149 ms 62.6%
  triton_convolution_136 0.0155 ms 60.2%
  triton_convolution_133 0.0170 ms 54.8%
  triton_convolution_132 0.0198 ms 47.0%
  triton_convolution_135 0.0208 ms 44.8%
  triton_convolution_131 0.0240 ms 38.7%
  triton_convolution_130 0.0398 ms 23.4%
  conv1x1_via_mm 0.1187 ms 7.8%
SingleProcess AUTOTUNE takes 2.6703 seconds
AUTOTUNE convolution(32x512x7x7, 2048x512x1x1)
  convolution 0.0428 ms 100.0%
  triton_convolution_140 0.0729 ms 58.7%
  triton_convolution_141 0.0758 ms 56.5%
  triton_convolution_138 0.0860 ms 49.8%
  triton_convolution_143 0.0990 ms 43.2%
  triton_convolution_137 0.1011 ms 42.4%
  triton_convolution_142 0.1227 ms 34.9%
  triton_convolution_139 0.1977 ms 21.7%
  conv1x1_via_mm 0.2735 ms 15.7%
SingleProcess AUTOTUNE takes 5.2872 seconds
AUTOTUNE convolution(32x1024x7x7, 2048x1024x1x1)
  convolution 0.0590 ms 100.0%
  triton_convolution_147 0.1291 ms 45.7%
  triton_convolution_148 0.1386 ms 42.6%
  triton_convolution_145 0.1554 ms 38.0%
  triton_convolution_150 0.1804 ms 32.7%
  triton_convolution_144 0.1848 ms 31.9%
  triton_convolution_149 0.2290 ms 25.8%
  conv1x1_via_mm 0.3322 ms 17.8%
  triton_convolution_146 0.3795 ms 15.6%
SingleProcess AUTOTUNE takes 5.7139 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 75.53it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 86.89it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 90.41it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 88.68it/s]
2.050x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_vision_transformer            
AUTOTUNE convolution(32x3x224x224, 384x3x16x16)
  convolution 0.2736 ms 100.0%
  triton_convolution_6 0.3196 ms 85.6%
  triton_convolution_1 0.3287 ms 83.2%
  triton_convolution_3 0.3458 ms 79.1%
  triton_convolution_4 0.4729 ms 57.9%
  triton_convolution_5 0.5007 ms 54.6%
  triton_convolution_0 0.5241 ms 52.2%
  triton_convolution_2 1.0152 ms 26.9%
SingleProcess AUTOTUNE takes 4.7602 seconds
AUTOTUNE int_mm(6304x384, 384x1152, 6304x1152)
  triton_mm_8 0.0423 ms 100.0%
  triton_mm_9 0.0429 ms 98.7%
  triton_mm_15 0.0467 ms 90.6%
  triton_mm_11 0.0488 ms 86.7%
  triton_mm_10 0.0504 ms 84.0%
  triton_mm_17 0.0507 ms 83.5%
  triton_mm_16 0.0516 ms 82.0%
  triton_mm_7 0.0517 ms 81.8%
  triton_mm_14 0.0550 ms 76.9%
  triton_mm_12 0.1072 ms 39.5%
SingleProcess AUTOTUNE takes 7.2763 seconds
AUTOTUNE int_mm(6304x384, 384x384, 6304x384)
  triton_mm_26 0.0213 ms 100.0%
  triton_mm_28 0.0217 ms 98.4%
  triton_mm_27 0.0224 ms 95.3%
  triton_mm_20 0.0230 ms 92.9%
  triton_mm_22 0.0231 ms 92.3%
  triton_mm_19 0.0237 ms 90.0%
  triton_mm_25 0.0249 ms 85.7%
  triton_mm_18 0.0255 ms 83.7%
  triton_mm_21 0.0262 ms 81.4%
  triton_mm_23 0.0401 ms 53.2%
SingleProcess AUTOTUNE takes 7.2864 seconds
AUTOTUNE int_mm(6304x384, 384x1536, 6304x1536)
  triton_mm_30 0.0512 ms 100.0%
  triton_mm_31 0.0527 ms 97.2%
  triton_mm_39 0.0533 ms 96.2%
  triton_mm_38 0.0540 ms 94.9%
  triton_mm_37 0.0584 ms 87.8%
  triton_mm_36 0.0589 ms 86.9%
  triton_mm_33 0.0613 ms 83.6%
  triton_mm_32 0.0614 ms 83.5%
  triton_mm_29 0.0621 ms 82.4%
  triton_mm_34 0.1403 ms 36.5%
SingleProcess AUTOTUNE takes 7.3713 seconds
AUTOTUNE int_mm(6304x1536, 1536x384, 6304x384)
  triton_mm_50 0.0360 ms 100.0%
  triton_mm_49 0.0363 ms 99.4%
  triton_mm_48 0.0462 ms 78.0%
  triton_mm_42 0.0479 ms 75.3%
  triton_mm_41 0.0507 ms 71.1%
  triton_mm_44 0.0514 ms 70.1%
  triton_mm_43 0.0544 ms 66.2%
  triton_mm_47 0.0596 ms 60.4%
  triton_mm_40 0.0607 ms 59.4%
  triton_mm_45 0.1109 ms 32.5%
SingleProcess AUTOTUNE takes 7.1609 seconds
AUTOTUNE int_mm(32x384, 384x1000, 32x1000)
  triton_mm_540 0.0094 ms 100.0%
  triton_mm_545 0.0098 ms 95.4%
  triton_mm_543 0.0102 ms 92.0%
  triton_mm_541 0.0103 ms 90.7%
  triton_mm_539 0.0106 ms 88.5%
  triton_mm_544 0.0118 ms 79.2%
  triton_mm_537 0.0119 ms 78.6%
  triton_mm_538 0.0124 ms 75.9%
  triton_mm_536 0.0127 ms 74.0%
  triton_mm_535 0.0144 ms 65.3%
SingleProcess AUTOTUNE takes 4.0711 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:03,  8.69it/s]running benchmark:  10%|█         | 3/30 [00:00<00:02,  9.69it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:02,  9.82it/s]running benchmark:  20%|██        | 6/30 [00:00<00:02,  9.73it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:02,  9.67it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:02,  9.68it/s]running benchmark:  30%|███       | 9/30 [00:00<00:02,  9.67it/s]running benchmark:  33%|███▎      | 10/30 [00:01<00:02,  9.71it/s]running benchmark:  37%|███▋      | 11/30 [00:01<00:01,  9.75it/s]running benchmark:  43%|████▎     | 13/30 [00:01<00:01, 10.04it/s]running benchmark:  50%|█████     | 15/30 [00:01<00:01, 10.12it/s]running benchmark:  57%|█████▋    | 17/30 [00:01<00:01, 10.04it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:01, 10.08it/s]running benchmark:  70%|███████   | 21/30 [00:02<00:00, 10.12it/s]running benchmark:  77%|███████▋  | 23/30 [00:02<00:00, 10.16it/s]running benchmark:  83%|████████▎ | 25/30 [00:02<00:00, 10.21it/s]running benchmark:  90%|█████████ | 27/30 [00:02<00:00, 10.21it/s]running benchmark:  97%|█████████▋| 29/30 [00:02<00:00, 10.24it/s]running benchmark: 100%|██████████| 30/30 [00:02<00:00, 10.02it/s]
20.709x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:19, ?it/s]
cuda eval  timm_vision_transformer_large      
AUTOTUNE convolution(32x3x224x224, 1408x3x14x14)
  triton_convolution_6 0.7543 ms 100.0%
  convolution 0.7756 ms 97.3%
  triton_convolution_1 0.8331 ms 90.5%
  triton_convolution_3 0.9176 ms 82.2%
  triton_convolution_4 1.1512 ms 65.5%
  triton_convolution_0 1.3115 ms 57.5%
  triton_convolution_5 1.5356 ms 49.1%
  triton_convolution_2 4.6490 ms 16.2%
SingleProcess AUTOTUNE takes 4.9856 seconds
AUTOTUNE int_mm(8224x1408, 1408x4224, 8224x4224)
  triton_mm_17 0.3031 ms 100.0%
  triton_mm_16 0.3037 ms 99.8%
  triton_mm_8 0.4088 ms 74.2%
  triton_mm_9 0.4186 ms 72.4%
  triton_mm_14 0.4479 ms 67.7%
  triton_mm_15 0.4522 ms 67.0%
  triton_mm_11 0.4774 ms 63.5%
  triton_mm_10 0.4813 ms 63.0%
  triton_mm_7 0.5563 ms 54.5%
  triton_mm_12 1.3662 ms 22.2%
SingleProcess AUTOTUNE takes 7.6086 seconds
AUTOTUNE int_mm(8224x1408, 1408x1408, 8224x1408)
  triton_mm_27 0.1158 ms 100.0%
  triton_mm_28 0.1169 ms 99.1%
  triton_mm_19 0.1513 ms 76.5%
  triton_mm_20 0.1538 ms 75.3%
  triton_mm_26 0.1602 ms 72.3%
  triton_mm_22 0.1724 ms 67.2%
  triton_mm_21 0.1755 ms 66.0%
  triton_mm_25 0.1836 ms 63.1%
  triton_mm_18 0.2041 ms 56.8%
  triton_mm_23 0.4629 ms 25.0%
SingleProcess AUTOTUNE takes 8.1029 seconds
AUTOTUNE int_mm(8224x1408, 1408x6144, 8224x6144)
  triton_mm_38 0.4150 ms 100.0%
  triton_mm_39 0.4182 ms 99.2%
  triton_mm_30 0.5887 ms 70.5%
  triton_mm_31 0.6074 ms 68.3%
  triton_mm_37 0.6541 ms 63.4%
  triton_mm_36 0.6897 ms 60.2%
  triton_mm_32 0.6913 ms 60.0%
  triton_mm_33 0.6923 ms 59.9%
  triton_mm_29 0.7946 ms 52.2%
  triton_mm_34 1.9824 ms 20.9%
SingleProcess AUTOTUNE takes 8.0285 seconds
AUTOTUNE int_mm(8224x6144, 6144x1408, 8224x1408)
  triton_mm_49 0.3440 ms 100.0%
  triton_mm_50 0.3464 ms 99.3%
  triton_mm_48 0.5873 ms 58.6%
  triton_mm_41 0.5940 ms 57.9%
  triton_mm_42 0.5978 ms 57.5%
  triton_mm_43 0.6389 ms 53.8%
  triton_mm_44 0.6405 ms 53.7%
  triton_mm_47 0.7350 ms 46.8%
  triton_mm_40 0.8616 ms 39.9%
  triton_mm_45 1.8519 ms 18.6%
SingleProcess AUTOTUNE takes 7.7155 seconds
AUTOTUNE int_mm(32x1408, 1408x1000, 32x1000)
  triton_mm_1777 0.0144 ms 100.0%
  triton_mm_1772 0.0166 ms 86.5%
  triton_mm_1775 0.0171 ms 83.9%
  triton_mm_1776 0.0172 ms 83.8%
  triton_mm_1773 0.0180 ms 79.9%
  triton_mm_1771 0.0199 ms 72.1%
  triton_mm_1770 0.0224 ms 64.2%
  triton_mm_1769 0.0260 ms 55.4%
  triton_mm_1768 0.0287 ms 50.1%
  triton_mm_1767 0.0380 ms 37.8%
SingleProcess AUTOTUNE takes 4.6638 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:26,  1.09it/s]running benchmark:   7%|▋         | 2/30 [00:01<00:21,  1.31it/s]running benchmark:  10%|█         | 3/30 [00:02<00:19,  1.39it/s]running benchmark:  13%|█▎        | 4/30 [00:02<00:18,  1.44it/s]running benchmark:  17%|█▋        | 5/30 [00:03<00:17,  1.46it/s]running benchmark:  20%|██        | 6/30 [00:04<00:16,  1.48it/s]running benchmark:  23%|██▎       | 7/30 [00:04<00:15,  1.49it/s]running benchmark:  27%|██▋       | 8/30 [00:05<00:14,  1.50it/s]running benchmark:  30%|███       | 9/30 [00:06<00:13,  1.50it/s]running benchmark:  33%|███▎      | 10/30 [00:06<00:13,  1.51it/s]running benchmark:  37%|███▋      | 11/30 [00:07<00:12,  1.51it/s]running benchmark:  40%|████      | 12/30 [00:08<00:11,  1.51it/s]running benchmark:  43%|████▎     | 13/30 [00:08<00:11,  1.51it/s]running benchmark:  47%|████▋     | 14/30 [00:09<00:10,  1.51it/s]running benchmark:  50%|█████     | 15/30 [00:10<00:09,  1.51it/s]running benchmark:  53%|█████▎    | 16/30 [00:10<00:09,  1.51it/s]running benchmark:  57%|█████▋    | 17/30 [00:11<00:08,  1.51it/s]running benchmark:  60%|██████    | 18/30 [00:12<00:07,  1.51it/s]running benchmark:  63%|██████▎   | 19/30 [00:12<00:07,  1.52it/s]running benchmark:  67%|██████▋   | 20/30 [00:13<00:06,  1.51it/s]running benchmark:  70%|███████   | 21/30 [00:14<00:05,  1.52it/s]running benchmark:  73%|███████▎  | 22/30 [00:14<00:05,  1.51it/s]running benchmark:  77%|███████▋  | 23/30 [00:15<00:04,  1.51it/s]running benchmark:  80%|████████  | 24/30 [00:16<00:03,  1.51it/s]running benchmark:  83%|████████▎ | 25/30 [00:16<00:03,  1.51it/s]running benchmark:  87%|████████▋ | 26/30 [00:17<00:02,  1.51it/s]running benchmark:  90%|█████████ | 27/30 [00:18<00:01,  1.51it/s]running benchmark:  93%|█████████▎| 28/30 [00:18<00:01,  1.51it/s]running benchmark:  97%|█████████▋| 29/30 [00:19<00:00,  1.51it/s]running benchmark: 100%|██████████| 30/30 [00:20<00:00,  1.51it/s]running benchmark: 100%|██████████| 30/30 [00:20<00:00,  1.49it/s]
6.724x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_vovnet                        
AUTOTUNE convolution(32x3x224x224, 64x3x3x3)
  convolution 0.1651 ms 100.0%
  triton_convolution_4 0.1691 ms 97.6%
  triton_convolution_3 0.1780 ms 92.7%
  triton_convolution_5 0.2411 ms 68.5%
  triton_convolution_0 0.2696 ms 61.3%
  triton_convolution_2 0.2908 ms 56.8%
  triton_convolution_1 0.7182 ms 23.0%
SingleProcess AUTOTUNE takes 3.1808 seconds
AUTOTUNE convolution(32x64x112x112, 64x64x3x3)
  convolution 0.1843 ms 100.0%
  triton_convolution_6 0.9136 ms 20.2%
  triton_convolution_11 1.0019 ms 18.4%
  triton_convolution_12 1.2125 ms 15.2%
  triton_convolution_9 1.4849 ms 12.4%
  triton_convolution_10 1.5300 ms 12.0%
  triton_convolution_7 1.8501 ms 10.0%
  triton_convolution_8 3.5914 ms 5.1%
SingleProcess AUTOTUNE takes 4.4998 seconds
AUTOTUNE convolution(32x64x112x112, 128x64x3x3)
  convolution 0.0979 ms 100.0%
  triton_convolution_13 0.6920 ms 14.2%
  triton_convolution_16 0.7776 ms 12.6%
  triton_convolution_18 0.8571 ms 11.4%
  triton_convolution_19 0.8855 ms 11.1%
  triton_convolution_14 1.1376 ms 8.6%
  triton_convolution_17 1.3753 ms 7.1%
  triton_convolution_15 2.0806 ms 4.7%
SingleProcess AUTOTUNE takes 4.6115 seconds
AUTOTUNE convolution(32x128x56x56, 128x128x3x3)
  convolution 0.1437 ms 100.0%
  triton_convolution_23 0.8742 ms 16.4%
  triton_convolution_20 1.0454 ms 13.7%
  triton_convolution_25 1.0473 ms 13.7%
  triton_convolution_26 1.3517 ms 10.6%
  triton_convolution_24 1.5676 ms 9.2%
  triton_convolution_21 2.0060 ms 7.2%
  triton_convolution_22 3.7722 ms 3.8%
SingleProcess AUTOTUNE takes 4.1768 seconds
AUTOTUNE mm(100352x768, 768x256)
  mm 0.2082 ms 100.0%
  triton_mm_56 0.2462 ms 84.6%
  triton_mm_57 0.2463 ms 84.5%
  triton_mm_59 0.2782 ms 74.8%
  triton_mm_58 0.2795 ms 74.5%
  triton_mm_62 0.3057 ms 68.1%
  triton_mm_55 0.3225 ms 64.6%
  triton_mm_63 0.3253 ms 64.0%
  triton_mm_65 0.4967 ms 41.9%
  triton_mm_61 0.5933 ms 35.1%
SingleProcess AUTOTUNE takes 4.9043 seconds
AUTOTUNE convolution(32x256x28x28, 160x256x3x3)
  convolution 0.1054 ms 100.0%
  triton_convolution_72 0.7181 ms 14.7%
  triton_convolution_70 0.8038 ms 13.1%
  triton_convolution_67 0.9062 ms 11.6%
  triton_convolution_73 1.0576 ms 10.0%
  triton_convolution_71 1.1176 ms 9.4%
  triton_convolution_68 1.8168 ms 5.8%
  triton_convolution_69 2.7793 ms 3.8%
SingleProcess AUTOTUNE takes 4.7638 seconds
AUTOTUNE convolution(32x160x28x28, 160x160x3x3)
  convolution 0.0723 ms 100.0%
  triton_convolution_79 0.4319 ms 16.7%
  triton_convolution_77 0.4954 ms 14.6%
  triton_convolution_74 0.5292 ms 13.7%
  triton_convolution_78 0.5886 ms 12.3%
  triton_convolution_80 0.6110 ms 11.8%
  triton_convolution_75 1.0588 ms 6.8%
  triton_convolution_76 1.2768 ms 5.7%
SingleProcess AUTOTUNE takes 4.6921 seconds
AUTOTUNE mm(25088x1056, 1056x512)
  mm 0.1361 ms 100.0%
  triton_mm_104 0.1551 ms 87.7%
  triton_mm_103 0.1562 ms 87.1%
  triton_mm_105 0.1805 ms 75.4%
  triton_mm_106 0.1808 ms 75.3%
  triton_mm_102 0.2034 ms 66.9%
  triton_mm_110 0.2240 ms 60.8%
  triton_mm_109 0.2849 ms 47.8%
  triton_mm_112 0.3315 ms 41.1%
  triton_mm_108 0.3727 ms 36.5%
SingleProcess AUTOTUNE takes 6.1255 seconds
AUTOTUNE convolution(32x512x14x14, 192x512x3x3)
  convolution 0.0775 ms 100.0%
  triton_convolution_119 0.4771 ms 16.2%
  triton_convolution_117 0.6776 ms 11.4%
  triton_convolution_120 0.7659 ms 10.1%
  triton_convolution_114 0.9425 ms 8.2%
  triton_convolution_118 1.0502 ms 7.4%
  triton_convolution_115 1.5099 ms 5.1%
  triton_convolution_116 1.7809 ms 4.4%
SingleProcess AUTOTUNE takes 5.0702 seconds
AUTOTUNE convolution(32x192x14x14, 192x192x3x3)
  convolution 0.0360 ms 100.0%
  triton_convolution_126 0.1428 ms 25.2%
  triton_convolution_124 0.1793 ms 20.1%
  triton_convolution_125 0.2483 ms 14.5%
  triton_convolution_127 0.2766 ms 13.0%
  triton_convolution_121 0.3038 ms 11.8%
  triton_convolution_122 0.5657 ms 6.4%
  triton_convolution_123 0.6939 ms 5.2%
SingleProcess AUTOTUNE takes 5.1453 seconds
AUTOTUNE mm(6272x1472, 1472x768)
  triton_mm_151 0.0846 ms 100.0%
  triton_mm_150 0.0854 ms 99.1%
  mm 0.0876 ms 96.6%
  triton_mm_152 0.0979 ms 86.4%
  triton_mm_153 0.0980 ms 86.3%
  triton_mm_157 0.1090 ms 77.6%
  triton_mm_149 0.1222 ms 69.3%
  triton_mm_156 0.1438 ms 58.8%
  triton_mm_154 0.1981 ms 42.7%
  triton_mm_155 0.1988 ms 42.6%
SingleProcess AUTOTUNE takes 4.4999 seconds
AUTOTUNE convolution(32x768x14x14, 192x768x3x3)
  convolution 0.1118 ms 100.0%
  triton_convolution_166 0.6829 ms 16.4%
  triton_convolution_164 1.0227 ms 10.9%
  triton_convolution_167 1.1061 ms 10.1%
  triton_convolution_165 1.5981 ms 7.0%
  triton_convolution_161 1.6215 ms 6.9%
  triton_convolution_162 2.4515 ms 4.6%
  triton_convolution_163 2.6836 ms 4.2%
SingleProcess AUTOTUNE takes 4.7067 seconds
AUTOTUNE mm(6272x1728, 1728x768)
  triton_mm_198 0.0984 ms 100.0%
  triton_mm_197 0.0994 ms 99.1%
  mm 0.0998 ms 98.6%
  triton_mm_199 0.1127 ms 87.4%
  triton_mm_200 0.1133 ms 86.9%
  triton_mm_204 0.1257 ms 78.3%
  triton_mm_196 0.1420 ms 69.3%
  triton_mm_203 0.1672 ms 58.9%
  triton_mm_201 0.2297 ms 42.9%
  triton_mm_202 0.2306 ms 42.7%
SingleProcess AUTOTUNE takes 4.7727 seconds
AUTOTUNE convolution(32x768x7x7, 224x768x3x3)
  convolution 0.0401 ms 100.0%
  triton_convolution_213 0.7003 ms 5.7%
  triton_convolution_212 0.7629 ms 5.3%
  triton_convolution_211 0.9860 ms 4.1%
  triton_convolution_214 1.1449 ms 3.5%
  triton_convolution_208 1.7443 ms 2.3%
  triton_convolution_210 2.4657 ms 1.6%
  triton_convolution_209 2.4740 ms 1.6%
SingleProcess AUTOTUNE takes 5.2191 seconds
AUTOTUNE convolution(32x224x7x7, 224x224x3x3)
  convolution 0.0196 ms 100.0%
  triton_convolution_219 0.1394 ms 14.1%
  triton_convolution_220 0.1773 ms 11.1%
  triton_convolution_218 0.2012 ms 9.8%
  triton_convolution_221 0.3301 ms 6.0%
  triton_convolution_215 0.4078 ms 4.8%
  triton_convolution_217 0.5135 ms 3.8%
  triton_convolution_216 0.6581 ms 3.0%
SingleProcess AUTOTUNE takes 4.9225 seconds
AUTOTUNE mm(1568x1888, 1888x1024)
  mm 0.0337 ms 100.0%
  triton_mm_244 0.0416 ms 80.9%
  triton_mm_245 0.0422 ms 79.8%
  triton_mm_246 0.0454 ms 74.1%
  triton_mm_247 0.0456 ms 73.8%
  triton_mm_251 0.0575 ms 58.5%
  triton_mm_243 0.0628 ms 53.6%
  triton_mm_250 0.0763 ms 44.1%
  triton_mm_253 0.0928 ms 36.3%
  triton_mm_248 0.0929 ms 36.2%
SingleProcess AUTOTUNE takes 4.5127 seconds
AUTOTUNE convolution(32x1024x7x7, 224x1024x3x3)
  convolution 0.0506 ms 100.0%
  triton_convolution_260 0.9436 ms 5.4%
  triton_convolution_259 1.0632 ms 4.8%
  triton_convolution_258 1.2945 ms 3.9%
  triton_convolution_261 1.7673 ms 2.9%
  triton_convolution_255 1.9272 ms 2.6%
  triton_convolution_256 3.0967 ms 1.6%
  triton_convolution_257 3.2582 ms 1.6%
SingleProcess AUTOTUNE takes 4.8696 seconds
AUTOTUNE mm(1568x2144, 2144x1024)
  mm 0.0374 ms 100.0%
  triton_mm_291 0.0469 ms 79.7%
  triton_mm_292 0.0473 ms 78.9%
  triton_mm_293 0.0500 ms 74.7%
  triton_mm_294 0.0502 ms 74.4%
  triton_mm_298 0.0636 ms 58.8%
  triton_mm_290 0.0703 ms 53.2%
  triton_mm_297 0.0861 ms 43.4%
  triton_mm_295 0.1043 ms 35.8%
  triton_mm_296 0.1047 ms 35.7%
SingleProcess AUTOTUNE takes 4.6000 seconds
AUTOTUNE int_mm(32x1024, 1024x1000, 32x1000)
  triton_mm_312 0.0124 ms 100.0%
  triton_mm_307 0.0138 ms 90.0%
  triton_mm_310 0.0144 ms 86.0%
  triton_mm_308 0.0151 ms 82.0%
  triton_mm_311 0.0151 ms 82.0%
  triton_mm_306 0.0166 ms 75.0%
  triton_mm_305 0.0184 ms 67.6%
  triton_mm_304 0.0207 ms 60.1%
  triton_mm_303 0.0227 ms 54.7%
  triton_mm_302 0.0296 ms 42.0%
SingleProcess AUTOTUNE takes 4.1546 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 63.38it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 73.31it/s]running benchmark:  83%|████████▎ | 25/30 [00:00<00:00, 76.50it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 75.53it/s]
2.071x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/torch_multimodal_clip/__init__.py", line 10, in <module>
    from torchmultimodal.transforms.clip_transform import CLIPTextTransform, CLIPImageTransform
ModuleNotFoundError: No module named 'torchmultimodal'
Failed to import user benchmark module dynamo, error: No module named 'torchmultimodal'
loading model: 0it [00:00, ?it/s]WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
loading model: 0it [00:01, ?it/s]
WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
cuda eval  tts_angular                        
WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
ERROR:common:Backend eager failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/tts_angular/model.py", line 59, in forward
    d = self.layers(x)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/tts_angular/model.py", line 19, in forward
    return self.linear(o)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  vgg16                              
AUTOTUNE convolution(4x3x224x224, 64x3x3x3)
  convolution 0.0785 ms 100.0%
  triton_convolution_4 0.0858 ms 91.6%
  triton_convolution_3 0.0893 ms 88.0%
  triton_convolution_0 0.1197 ms 65.6%
  triton_convolution_5 0.1198 ms 65.5%
  triton_convolution_2 0.1302 ms 60.3%
  triton_convolution_1 0.3330 ms 23.6%
SingleProcess AUTOTUNE takes 3.4145 seconds
AUTOTUNE convolution(4x64x224x224, 64x64x3x3)
  convolution 0.1026 ms 100.0%
  triton_convolution_6 0.4788 ms 21.4%
  triton_convolution_11 0.5143 ms 19.9%
  triton_convolution_12 0.6537 ms 15.7%
  triton_convolution_9 0.7764 ms 13.2%
  triton_convolution_10 0.7870 ms 13.0%
  triton_convolution_7 0.9295 ms 11.0%
  triton_convolution_8 1.9209 ms 5.3%
SingleProcess AUTOTUNE takes 3.8870 seconds
AUTOTUNE convolution(4x64x112x112, 128x64x3x3)
  convolution 0.0480 ms 100.0%
  triton_convolution_13 0.2420 ms 19.8%
  triton_convolution_16 0.2424 ms 19.8%
  triton_convolution_18 0.2814 ms 17.1%
  triton_convolution_19 0.3100 ms 15.5%
  triton_convolution_17 0.3564 ms 13.5%
  triton_convolution_14 0.4722 ms 10.2%
  triton_convolution_15 0.9606 ms 5.0%
SingleProcess AUTOTUNE takes 4.0978 seconds
AUTOTUNE convolution(4x128x112x112, 128x128x3x3)
  convolution 0.0758 ms 100.0%
  triton_convolution_23 0.4775 ms 15.9%
  triton_convolution_20 0.5034 ms 15.1%
  triton_convolution_25 0.5560 ms 13.6%
  triton_convolution_26 0.6822 ms 11.1%
  triton_convolution_24 0.7133 ms 10.6%
  triton_convolution_21 0.9780 ms 7.8%
  triton_convolution_22 1.9123 ms 4.0%
SingleProcess AUTOTUNE takes 4.1428 seconds
AUTOTUNE convolution(4x128x56x56, 256x128x3x3)
  convolution 0.0403 ms 100.0%
  triton_convolution_32 0.2028 ms 19.9%
  triton_convolution_30 0.2092 ms 19.2%
  triton_convolution_27 0.2867 ms 14.0%
  triton_convolution_33 0.3251 ms 12.4%
  triton_convolution_31 0.3277 ms 12.3%
  triton_convolution_28 0.4428 ms 9.1%
  triton_convolution_29 0.9542 ms 4.2%
SingleProcess AUTOTUNE takes 4.6838 seconds
AUTOTUNE convolution(4x256x56x56, 256x256x3x3)
  convolution 0.0683 ms 100.0%
  triton_convolution_39 0.4461 ms 15.3%
  triton_convolution_37 0.4472 ms 15.3%
  triton_convolution_34 0.5628 ms 12.1%
  triton_convolution_40 0.6896 ms 9.9%
  triton_convolution_38 0.8167 ms 8.4%
  triton_convolution_35 0.9405 ms 7.3%
  triton_convolution_36 1.8981 ms 3.6%
SingleProcess AUTOTUNE takes 5.3370 seconds
AUTOTUNE convolution(4x256x28x28, 512x256x3x3)
  convolution 0.0453 ms 100.0%
  triton_convolution_53 0.2257 ms 20.1%
  triton_convolution_51 0.2353 ms 19.2%
  triton_convolution_54 0.3818 ms 11.9%
  triton_convolution_52 0.4193 ms 10.8%
  triton_convolution_48 0.5186 ms 8.7%
  triton_convolution_49 0.7291 ms 6.2%
  triton_convolution_50 1.1617 ms 3.9%
SingleProcess AUTOTUNE takes 5.2984 seconds
AUTOTUNE convolution(4x512x28x28, 512x512x3x3)
  convolution 0.0673 ms 100.0%
  triton_convolution_60 0.4779 ms 14.1%
  triton_convolution_58 0.6631 ms 10.2%
  triton_convolution_61 0.7551 ms 8.9%
  triton_convolution_59 1.0759 ms 6.3%
  triton_convolution_55 1.2057 ms 5.6%
  triton_convolution_56 1.4800 ms 4.5%
  triton_convolution_57 2.3626 ms 2.8%
SingleProcess AUTOTUNE takes 5.0041 seconds
AUTOTUNE convolution(4x512x14x14, 512x512x3x3)
  convolution 0.0299 ms 100.0%
  triton_convolution_73 0.4603 ms 6.5%
  triton_convolution_74 0.4956 ms 6.0%
  triton_convolution_72 0.6906 ms 4.3%
  triton_convolution_75 0.7948 ms 3.8%
  triton_convolution_69 1.1836 ms 2.5%
  triton_convolution_71 1.4370 ms 2.1%
  triton_convolution_70 1.5934 ms 1.9%
SingleProcess AUTOTUNE takes 5.0773 seconds
AUTOTUNE int_mm(4x25088, 25088x4096, 4x4096)
  triton_mm_100 0.1267 ms 100.0%
  triton_mm_99 0.1431 ms 88.6%
  triton_mm_98 0.1791 ms 70.8%
  triton_mm_95 0.1841 ms 68.8%
  triton_mm_96 0.1925 ms 65.8%
  triton_mm_94 0.2075 ms 61.1%
  triton_mm_92 0.3329 ms 38.1%
  triton_mm_91 0.3752 ms 33.8%
  triton_mm_90 0.4839 ms 26.2%
  triton_mm_93 0.5503 ms 23.0%
SingleProcess AUTOTUNE takes 4.0211 seconds
AUTOTUNE int_mm(4x4096, 4096x4096, 4x4096)
  triton_mm_111 0.0289 ms 100.0%
  triton_mm_110 0.0321 ms 90.0%
  triton_mm_109 0.0376 ms 76.9%
  triton_mm_106 0.0398 ms 72.7%
  triton_mm_107 0.0409 ms 70.7%
  triton_mm_105 0.0427 ms 67.7%
  triton_mm_103 0.0650 ms 44.5%
  triton_mm_102 0.0725 ms 39.9%
  triton_mm_101 0.0875 ms 33.1%
  triton_mm_104 0.0986 ms 29.3%
SingleProcess AUTOTUNE takes 3.9535 seconds
AUTOTUNE int_mm(4x4096, 4096x1000, 4x1000)
  triton_mm_122 0.0238 ms 100.0%
  triton_mm_121 0.0280 ms 85.1%
  triton_mm_117 0.0312 ms 76.4%
  triton_mm_120 0.0330 ms 72.2%
  triton_mm_118 0.0337 ms 70.7%
  triton_mm_116 0.0371 ms 64.2%
  triton_mm_114 0.0573 ms 41.5%
  triton_mm_113 0.0638 ms 37.3%
  triton_mm_112 0.0920 ms 25.9%
  triton_mm_115 0.1029 ms 23.1%
SingleProcess AUTOTUNE takes 3.5599 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:08,  3.25it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:08,  3.29it/s]running benchmark:  10%|█         | 3/30 [00:00<00:07,  3.41it/s]running benchmark:  13%|█▎        | 4/30 [00:01<00:07,  3.68it/s]running benchmark:  17%|█▋        | 5/30 [00:01<00:06,  3.78it/s]running benchmark:  20%|██        | 6/30 [00:01<00:06,  3.91it/s]running benchmark:  23%|██▎       | 7/30 [00:01<00:05,  4.00it/s]running benchmark:  27%|██▋       | 8/30 [00:02<00:05,  4.07it/s]running benchmark:  30%|███       | 9/30 [00:02<00:05,  4.10it/s]running benchmark:  33%|███▎      | 10/30 [00:02<00:04,  4.10it/s]running benchmark:  37%|███▋      | 11/30 [00:02<00:04,  4.13it/s]running benchmark:  40%|████      | 12/30 [00:03<00:04,  4.15it/s]running benchmark:  43%|████▎     | 13/30 [00:03<00:04,  4.17it/s]running benchmark:  47%|████▋     | 14/30 [00:03<00:03,  4.17it/s]running benchmark:  50%|█████     | 15/30 [00:03<00:03,  4.17it/s]running benchmark:  53%|█████▎    | 16/30 [00:04<00:03,  4.19it/s]running benchmark:  57%|█████▋    | 17/30 [00:04<00:03,  4.19it/s]running benchmark:  60%|██████    | 18/30 [00:04<00:02,  4.20it/s]running benchmark:  63%|██████▎   | 19/30 [00:04<00:02,  4.20it/s]running benchmark:  67%|██████▋   | 20/30 [00:04<00:02,  4.15it/s]running benchmark:  70%|███████   | 21/30 [00:05<00:02,  4.12it/s]running benchmark:  73%|███████▎  | 22/30 [00:05<00:01,  4.15it/s]running benchmark:  77%|███████▋  | 23/30 [00:05<00:01,  4.15it/s]running benchmark:  80%|████████  | 24/30 [00:05<00:01,  4.12it/s]running benchmark:  83%|████████▎ | 25/30 [00:06<00:01,  4.10it/s]running benchmark:  87%|████████▋ | 26/30 [00:06<00:00,  4.13it/s]running benchmark:  90%|█████████ | 27/30 [00:06<00:00,  4.15it/s]running benchmark:  93%|█████████▎| 28/30 [00:06<00:00,  4.16it/s]running benchmark:  97%|█████████▋| 29/30 [00:07<00:00,  4.17it/s]running benchmark: 100%|██████████| 30/30 [00:07<00:00,  4.18it/s]running benchmark: 100%|██████████| 30/30 [00:07<00:00,  4.07it/s]
158.271x
loading model: 0it [00:00, ?it/s]Downloading: "https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth" to /home/cdhernandez/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth

  0%|          | 0.00/170M [00:00<?, ?B/s][A
  2%|▏         | 2.57M/170M [00:00<00:06, 26.9MB/s][A
  8%|▊         | 13.3M/170M [00:00<00:02, 77.1MB/s][A
 12%|█▏        | 20.6M/170M [00:00<00:03, 47.1MB/s][A
 15%|█▌        | 26.0M/170M [00:00<00:03, 47.8MB/s][A
 19%|█▉        | 33.0M/170M [00:00<00:02, 54.8MB/s][A
 24%|██▎       | 40.1M/170M [00:00<00:02, 60.4MB/s][A
 28%|██▊       | 48.0M/170M [00:00<00:02, 57.7MB/s][A
 33%|███▎      | 56.0M/170M [00:01<00:02, 59.2MB/s][A
 38%|███▊      | 64.0M/170M [00:01<00:01, 59.2MB/s][A
 42%|████▏     | 72.0M/170M [00:01<00:02, 50.8MB/s][A
 46%|████▋     | 78.7M/170M [00:01<00:02, 40.1MB/s][A
 49%|████▉     | 83.0M/170M [00:01<00:02, 38.7MB/s][A
 52%|█████▏    | 88.0M/170M [00:01<00:02, 39.9MB/s][A
 57%|█████▋    | 96.0M/170M [00:02<00:01, 39.8MB/s][A
 63%|██████▎   | 107M/170M [00:02<00:01, 54.7MB/s] [A
 67%|██████▋   | 113M/170M [00:02<00:01, 45.2MB/s][A
 71%|███████   | 120M/170M [00:02<00:01, 50.6MB/s][A
 76%|███████▌  | 129M/170M [00:02<00:00, 60.2MB/s][A
 80%|███████▉  | 136M/170M [00:02<00:00, 57.0MB/s][A
 84%|████████▎ | 142M/170M [00:02<00:00, 51.1MB/s][A
 87%|████████▋ | 147M/170M [00:03<00:00, 45.5MB/s][A
 90%|████████▉ | 152M/170M [00:03<00:00, 38.9MB/s][A
 94%|█████████▍| 160M/170M [00:03<00:00, 44.9MB/s][A100%|██████████| 170M/170M [00:03<00:00, 50.3MB/s]
WARNING:common:Model vision_maskrcnn does not support bfloat16, running with float16 instead
loading model: 0it [00:07, ?it/s]
WARNING:common:Model vision_maskrcnn does not support bfloat16, running with float16 instead
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3632, in run
    model, example_inputs = runner.cast_based_on_args(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1917, in cast_based_on_args
    model, example_inputs = cast_to_fp16(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1675, in cast_to_fp16
    return cast_to(torch.float16, model, inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1657, in cast_to
    model = model.half()
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1007, in half
    return self._apply(lambda t: t.half() if t.is_floating_point() else t)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 826, in _apply
    should_use_set_data = compute_should_use_set_data(param, param_applied)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 805, in compute_should_use_set_data
    if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
TypeError: _has_compatible_shallow_copy_type(): argument 'from' (position 2) must be Tensor, not NoneType
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/yolov3/__init__.py", line 15, in <module>
    from .yolo_train import prepare_training_loop
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/yolov3/yolo_train.py", line 6, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
speedup             gmean=0.00x mean=7.285x
abs_latency         gmean=0.00x mean=7.123x
compilation_latency mean=62.233 seconds
compression_ratio   mean=0.346x
eager_peak_mem      gmean=0.00x mean=0.347x
dynamo_peak_mem     gmean=0.00x mean=0.484x
calls_captured      gmean=0.00x mean=197.294x
unique_graphs       gmean=0.00x mean=1.412x
graph_breaks        gmean=0.00x mean=0.559x
unique_graph_breaks gmean=0.00x mean=0.147x
start int8 weight only
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/canary_models/torchrec_dlrm/__init__.py", line 14, in <module>
    from pyre_extensions import none_throws
ModuleNotFoundError: No module named 'pyre_extensions'
Failed to import user benchmark module dynamo, error: No module named 'pyre_extensions'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3569, in run
    change_linear_weights_to_int8_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 117, in change_linear_weights_to_int8_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  [Previous line repeated 2 more times]
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 237, in from_float
    w_int_repr, w_scales, _ = dynamically_quantize_per_channel(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_primitives.py", line 171, in dynamically_quantize_per_channel
    min_val, max_val = torch.aminmax(x, dim=1)
RuntimeError: Object of type 'NoneType' is not an instance of 'sequence'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  Background_Matting                 
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:00, 28.84it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 37.45it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 39.96it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 41.11it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 41.70it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 42.08it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 40.82it/s]
2.090x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/DALLE2_pytorch/__init__.py", line 3, in <module>
    from dalle2_pytorch import DALLE2, Unet, Decoder, DiffusionPriorNetwork, DiffusionPrior, OpenAIClipAdapter
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/dalle2_pytorch/__init__.py", line 9, in <module>
    from dalle2_pytorch.dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/dalle2_pytorch/dalle2_pytorch.py", line 18, in <module>
    from kornia.filters import gaussian_blur2d
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/__init__.py", line 8, in <module>
    from . import augmentation, color, contrib, core, enhance, feature, io, losses, metrics, morphology, tracking, utils, x
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/x/__init__.py", line 2, in <module>
    from .trainer import Trainer
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/x/trainer.py", line 11, in <module>
    from accelerate import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  LearningToPaint                    
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 194.07it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 195.86it/s]
2.123x
loading model: 0it [00:00, ?it/s]WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
loading model: 0it [00:03, ?it/s]
WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
cuda eval  Super_SloMo                        
WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:03,  8.89it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 14.59it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:01, 16.54it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 17.43it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 17.93it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 18.23it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 18.45it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 18.59it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 18.67it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 18.73it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 18.75it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 18.79it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 18.79it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 18.81it/s]running benchmark:  97%|█████████▋| 29/30 [00:01<00:00, 18.80it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 18.16it/s]
1.620x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  alexnet                            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 139.62it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 145.70it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 144.84it/s]
1.330x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  basic_gnn_edgecnn                  
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 203.18it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 205.98it/s]
1.334x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_gcn                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 122.51it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 125.19it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 125.02it/s]
1.060x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  basic_gnn_gin                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 347.59it/s]
1.214x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  basic_gnn_sage                     
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 165.94it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 172.14it/s]
1.238x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  cm3leon_generate                   
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:01<00:36,  1.25s/it]running benchmark:   7%|▋         | 2/30 [00:02<00:34,  1.23s/it]running benchmark:  10%|█         | 3/30 [00:03<00:32,  1.22s/it]running benchmark:  13%|█▎        | 4/30 [00:04<00:32,  1.24s/it]running benchmark:  17%|█▋        | 5/30 [00:06<00:31,  1.25s/it]running benchmark:  20%|██        | 6/30 [00:07<00:30,  1.26s/it]running benchmark:  23%|██▎       | 7/30 [00:08<00:29,  1.27s/it]running benchmark:  27%|██▋       | 8/30 [00:10<00:28,  1.27s/it]running benchmark:  30%|███       | 9/30 [00:11<00:26,  1.27s/it]running benchmark:  33%|███▎      | 10/30 [00:12<00:25,  1.27s/it]running benchmark:  37%|███▋      | 11/30 [00:13<00:23,  1.26s/it]running benchmark:  40%|████      | 12/30 [00:15<00:22,  1.24s/it]running benchmark:  43%|████▎     | 13/30 [00:16<00:21,  1.25s/it]running benchmark:  47%|████▋     | 14/30 [00:17<00:19,  1.24s/it]running benchmark:  50%|█████     | 15/30 [00:18<00:18,  1.24s/it]running benchmark:  53%|█████▎    | 16/30 [00:20<00:17,  1.24s/it]running benchmark:  57%|█████▋    | 17/30 [00:21<00:16,  1.24s/it]running benchmark:  60%|██████    | 18/30 [00:22<00:14,  1.24s/it]running benchmark:  63%|██████▎   | 19/30 [00:23<00:13,  1.24s/it]running benchmark:  67%|██████▋   | 20/30 [00:24<00:12,  1.25s/it]running benchmark:  70%|███████   | 21/30 [00:26<00:11,  1.24s/it]running benchmark:  73%|███████▎  | 22/30 [00:27<00:09,  1.23s/it]running benchmark:  77%|███████▋  | 23/30 [00:28<00:08,  1.23s/it]running benchmark:  80%|████████  | 24/30 [00:29<00:07,  1.23s/it]running benchmark:  83%|████████▎ | 25/30 [00:31<00:06,  1.23s/it]running benchmark:  87%|████████▋ | 26/30 [00:32<00:04,  1.22s/it]running benchmark:  90%|█████████ | 27/30 [00:33<00:03,  1.23s/it]running benchmark:  93%|█████████▎| 28/30 [00:34<00:02,  1.24s/it]running benchmark:  97%|█████████▋| 29/30 [00:36<00:01,  1.23s/it]running benchmark: 100%|██████████| 30/30 [00:37<00:00,  1.24s/it]running benchmark: 100%|██████████| 30/30 [00:37<00:00,  1.24s/it]
4.478x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  dcgan                              
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 464.67it/s]
1.380x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
WARNING:root:demucs failed to load
Original Error: "_thnn_fused_lstm_cell_cuda" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/__init__.py", line 31, in forward
    return sources, self.model(mix)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/demucs/model.py", line 209, in forward
    x = self.lstm(x)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/demucs/model.py", line 27, in forward
    x = self.lstm(x)[0]
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/rnn.py", line 878, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
RuntimeError: "_thnn_fused_lstm_cell_cuda" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  densenet121                        
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 24.53it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 29.75it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 31.40it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 32.32it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 32.86it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 33.19it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 33.40it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 32.48it/s]
1.979x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:08, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_dc5 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:08, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_dc5 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
loading model: 0it [00:07, ?it/s]
WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
cuda eval  detectron2_fcos_r_50_fpn           
WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
[2023-12-05 04:08:24,659] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (8)
[2023-12-05 04:08:24,659] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:318)
[2023-12-05 04:08:24,659] torch._dynamo.convert_frame: [WARNING]    last reason: L['self']._pos == 0                                           # ret = self[self._pos](x)  # miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:319 in forward
[2023-12-05 04:08:24,659] torch._dynamo.convert_frame: [WARNING] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[2023-12-05 04:08:24,659] torch._dynamo.convert_frame: [WARNING] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.
skipping cudagraphs due to ['mutated inputs']
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 17.14it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:01, 18.58it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 19.38it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:01, 19.63it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 19.70it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 19.19it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 19.54it/s]running benchmark:  67%|██████▋   | 20/30 [00:01<00:00, 19.67it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 19.88it/s]running benchmark:  87%|████████▋ | 26/30 [00:01<00:00, 20.10it/s]running benchmark:  97%|█████████▋| 29/30 [00:01<00:00, 20.19it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 19.75it/s]
1.561x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_maskrcnn_r_101_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_maskrcnn_r_101_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_maskrcnn_r_50_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_maskrcnn_r_50_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:14, ?it/s]
cuda eval  dlrm                               
AUTOTUNE mixed_mm(2048x1024, 1024x1)
  triton_mm_14 0.0163 ms 100.0%
  fallback_mixed_mm 0.0173 ms 94.1%
  triton_mm_16 0.0183 ms 89.1%
  triton_mm_13 0.0198 ms 82.1%
  triton_mm_17 0.0199 ms 81.7%
  triton_mm_11 0.0206 ms 79.0%
  triton_mm_9 0.0258 ms 63.1%
  triton_mm_12 0.0279 ms 58.4%
  triton_mm_10 0.0282 ms 57.8%
  triton_mm_18 0.0410 ms 39.8%
SingleProcess AUTOTUNE takes 3.6153 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 212.67it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 214.09it/s]
1.834x
loading model: 0it [00:00, ?it/s]WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
loading model: 0it [00:05, ?it/s]
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
cuda eval  doctr_det_predictor                
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
[2023-12-05 04:10:27,988] [1/0_1] torch._inductor.utils: [WARNING] DeviceCopy in input program
skipping cudagraphs due to ['non-cuda device in graph']
malloc(): unaligned tcache chunk detected
Run failed with return code:  -6
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
loading model: 0it [00:05, ?it/s]
WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
cuda eval  doctr_reco_predictor               
WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
ERROR:common:Backend eager failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/doctr/models/recognition/crnn/pytorch.py", line 212, in forward
    logits = self.linear(logits)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/drq/__init__.py", line 11, in <module>
    from gym import spaces
ModuleNotFoundError: No module named 'gym'
Failed to import user benchmark module dynamo, error: No module named 'gym'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  fastNLP_Bert                       
ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/models/bert.py", line 265, in forward
    sequence_output = self.bert(words)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 137, in forward
    outputs = self.model(words)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 445, in forward
    max_word_piece_length = batch_word_pieces_length.sum(dim=-1).max().item()  # 表示word piece的长度(包括padding)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 482, in resume_in_forward
    bert_outputs, pooled_cls = self.encoder(word_pieces, token_type_ids=token_type_ids, attention_mask=attn_masks,
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/eval_frame.py", line 655, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 722, in _convert_frame
    result = inner_convert(frame, cache_entry, hooks, frame_state)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert
    compiled_product = _compile(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 646, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 562, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object
    transformations(instructions, code_options)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 151, in _fn
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 527, in transform
    tracer.run()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 2123, in run
    super().run()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 818, in run
    and self.step()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 781, in step
    getattr(self, inst.opname)(inst)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 470, in wrapper
    return inner_fn(self, inst)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 1213, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 652, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/functions.py", line 291, in call_function
    return self.obj.call_method(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/nn_module.py", line 494, in call_method
    return wrap_values(module.named_parameters(**get_kwargs("recurse")))
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/nn_module.py", line 430, in wrap_values
    tx.output.register_attr_or_module(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 788, in register_attr_or_module
    return wrap_name(name)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 719, in wrap_name
    return wrap_fx_proxy(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/builder.py", line 1291, in wrap_fx_proxy
    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/builder.py", line 1401, in wrap_fx_proxy_cls
    example_value = wrap_to_fake_tensor_and_record(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/builder.py", line 1767, in wrap_to_fake_tensor_and_record
    fake_e = wrap_fake_exception(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 1026, in wrap_fake_exception
    return fn()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/variables/builder.py", line 1768, in <lambda>
    lambda: tx.fake_mode.from_tensor(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/fake_tensor.py", line 1884, in from_tensor
    return self.fake_tensor_converter(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/fake_tensor.py", line 396, in __call__
    return self.from_real_tensor(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/fake_tensor.py", line 353, in from_real_tensor
    out = self.meta_converter(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 710, in __call__
    r = self.meta_tensor(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 530, in meta_tensor
    r = transform_subclass(
  File "/home/cdhernandez/local/pytorch/torch/utils/_python_dispatch.py", line 168, in transform_subclass
    transformed_tensors_dict[attr] = callback(attr, getattr(t, attr))
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 532, in <lambda>
    lambda attr, inner_t: callback(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/fake_tensor.py", line 348, in mk_fake_tensor
    make_meta_t(),
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 533, in <lambda>
    lambda: empty_create(
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 460, in empty_create
    ) = sym_sizes_strides_storage_offset(inner_t, inner_src)
  File "/home/cdhernandez/local/pytorch/torch/_subclasses/meta_utils.py", line 246, in sym_sizes_strides_storage_offset
    return shape_env.create_symbolic_sizes_strides_storage_offset(
  File "/home/cdhernandez/local/pytorch/torch/fx/experimental/symbolic_shapes.py", line 2108, in create_symbolic_sizes_strides_storage_offset
    return self._create_symbolic_sizes_strides_storage_offset(
  File "/home/cdhernandez/local/pytorch/torch/fx/experimental/recording.py", line 226, in wrapper
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/fx/experimental/symbolic_shapes.py", line 2159, in _create_symbolic_sizes_strides_storage_offset
    assert len(dynamic_dims) == dim, f"{len(dynamic_dims)} != {dim}"
AssertionError: 2 != 1

from user code:
   File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/fastNLP/modules/encoder/bert.py", line 509, in forward
    extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  functorch_dp_cifar10               
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 238.78it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 238.95it/s]
4.552x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/functorch_maml_omniglot/__init__.py", line 73, in __init__
    self.meta_inputs = torch.load(f'{root}/maml_omniglot/batch.pt')
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/models/maml_omniglot/batch.pt'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py", line 36, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Albert/__init__.py", line 10, in __init__
    super().__init__(name="hf_Albert", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.albert.modeling_albert because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 36, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Bart/__init__.py", line 10, in __init__
    super().__init__(name="hf_Bart", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.bart.modeling_bart because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 39, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_BigBird/__init__.py", line 10, in __init__
    super().__init__(name="hf_BigBird", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.big_bird.modeling_big_bird because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 32, in <module>
    from ...integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_DistilBert/__init__.py", line 10, in __init__
    super().__init__(name="hf_DistilBert", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.distilbert.modeling_distilbert because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel, SequenceSummary
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_GPT2/__init__.py", line 10, in __init__
    super().__init__(name="hf_GPT2", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel, SequenceSummary
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_GPT2_large/__init__.py", line 10, in __init__
    super().__init__(name="hf_GPT2_large", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 27, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Longformer/__init__.py", line 10, in __init__
    super().__init__(name="hf_Longformer", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.longformer.modeling_longformer because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 33, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Reformer/__init__.py", line 10, in __init__
    super().__init__(name="hf_Reformer", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.reformer.modeling_reformer because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_base/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5_base", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_generate/__init__.py", line 5, in __init__
    super().__init__(name="hf_T5_generate", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 189, in __init__
    super().__init__(name=name, test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_large/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5_large", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py", line 35, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Whisper/__init__.py", line 12, in __init__
    super().__init__(name="hf_Whisper", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.whisper.modeling_whisper because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py", line 27, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_clip/__init__.py", line 13, in <module>
    from transformers import CLIPProcessor, CLIPModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1205, in __getattr__
    value = getattr(module, name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.clip.modeling_clip because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  lennard_jones                      
AUTOTUNE mixed_mm(1000x1, 1x16)
  triton_mm_6 0.0066 ms 100.0%
  triton_mm_9 0.0066 ms 100.0%
  triton_mm_1 0.0068 ms 96.2%
  triton_mm_8 0.0068 ms 95.8%
  triton_mm_2 0.0071 ms 92.3%
  triton_mm_4 0.0071 ms 92.3%
  triton_mm_0 0.0073 ms 89.3%
  triton_mm_3 0.0074 ms 89.1%
  triton_mm_10 0.0074 ms 89.1%
  triton_mm_5 0.0074 ms 88.9%
SingleProcess AUTOTUNE takes 3.2524 seconds
AUTOTUNE mixed_mm(1000x16, 16x1)
  triton_mm_20 0.0066 ms 100.0%
  triton_mm_19 0.0068 ms 96.3%
  triton_mm_21 0.0070 ms 94.5%
  triton_mm_17 0.0071 ms 93.4%
  triton_mm_14 0.0072 ms 92.0%
  triton_mm_11 0.0074 ms 89.6%
  triton_mm_12 0.0074 ms 89.6%
  triton_mm_13 0.0074 ms 89.6%
  triton_mm_15 0.0074 ms 89.6%
  triton_mm_16 0.0074 ms 89.6%
SingleProcess AUTOTUNE takes 3.1782 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 562.43it/s]
5.064x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  llama                              
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:00, 36.97it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 38.44it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 38.96it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 38.95it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 38.93it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 39.14it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 39.20it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 38.97it/s]
7.633x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:llama_v2_7b_16h failed to load
Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/llama_v2_7b_16h/__init__.py", line 11, in __init__
    HuggingFaceAuthMixin.__init__(self)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 177, in __init__
    raise NotImplementedError("Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights")
NotImplementedError: Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights

loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/maml_omniglot/__init__.py", line 25, in <module>
    import higher
ModuleNotFoundError: No module named 'higher'
Failed to import user benchmark module dynamo, error: No module named 'higher'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  mnasnet1_0                         
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 86.43it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 108.42it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 109.17it/s]
3.163x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  mobilenet_v2                       
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 119.58it/s]running benchmark:  83%|████████▎ | 25/30 [00:00<00:00, 122.72it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 122.54it/s]
4.755x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:mobilenet_v2_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/mobilenet_v2_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  mobilenet_v3_large                 
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 87.98it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 97.72it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 97.93it/s]
4.179x
loading model: 0it [00:00, ?it/s]NCCL version 2.19.3+cuda12.0
loading model: 0it [00:04, ?it/s]
cuda eval  moco                               
ERROR:common:Backend eager failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/moco/moco/builder.py", line 130, in forward
    self._momentum_update_key_encoder()  # update the key encoder
  File "/home/cdhernandez/local/pytorch/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/moco/moco/builder.py", line 50, in _momentum_update_key_encoder
    param_k.mul_(self.m).add_(param_q.mul(1. - self.m))
TypeError: add_(): argument 'other' (position 1) must be Tensor, not NoneType
Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
number of parameters: 123.69M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: True
cuda eval  nanogpt                            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  20%|██        | 6/30 [00:00<00:00, 52.60it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 53.49it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 53.79it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 53.55it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 53.54it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 53.50it/s]
8.997x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  nvidia_deeprecommender             
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 199.02it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 203.67it/s]
0.922x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/opacus_cifar10/__init__.py", line 6, in <module>
    from opacus import PrivacyEngine
ModuleNotFoundError: No module named 'opacus'
Failed to import user benchmark module dynamo, error: No module named 'opacus'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/phi_1_5/__init__.py", line 11, in __init__
    super().__init__(name="phi_1_5", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 437, in from_config
    model_class = get_class_from_dynamic_module(class_ref, repo_id, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 497, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module.replace(".py", ""))
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 199, in get_class_in_module
    module = importlib.import_module(module_path)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/5fd430c7bcd28140560faee2014d1228338e19a0/modeling_phi.py", line 16, in <module>
    from transformers import PretrainedConfig, PreTrainedModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  phlippe_densenet                   
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 84.03it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 90.26it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 92.21it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 91.11it/s]
4.840x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  phlippe_resnet                     
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 236.51it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 237.27it/s]
4.164x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pyhpc_equation_of_state            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 218.23it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 219.26it/s]
16.994x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pyhpc_isoneutral_mixing            
skipping cudagraphs due to ['mutated inputs']
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 86.97it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 87.16it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 87.79it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 87.27it/s]
5.997x
loading model: 0it [00:00, ?it/s]WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
loading model: 0it [00:01, ?it/s]
WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
cuda eval  pyhpc_turbulent_kinetic_energy     
WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 16.91it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 53.36it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 64.52it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 71.10it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 64.14it/s]
4.628x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/__init__.py", line 13, in <module>
    from .train_cyclegan import prepare_training_loop
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/train_cyclegan.py", line 27, in <module>
    from .util.visualizer import Visualizer
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/util/visualizer.py", line 6, in <module>
    from . import util, html
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/util/html.py", line 1, in <module>
    import dominate
ModuleNotFoundError: No module named 'dominate'
Failed to import user benchmark module dynamo, error: No module named 'dominate'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/__init__.py", line 52, in __init__
    self.data_loader = self.get_data_loader(config)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/__init__.py", line 68, in get_data_loader
    celeba_loader = get_loader(config.celeba_image_dir, config.attr_path, config.selected_attrs,
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 82, in get_loader
    dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 24, in __init__
    self.preprocess()
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 33, in preprocess
    lines = [line.rstrip() for line in open(self.attr_path, 'r')]
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/data/.data/pytorch_stargan_inputs/data/celeba/list_attr_celeba.txt'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pytorch_unet                       
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:00, 37.36it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 42.75it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 44.49it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 45.28it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 45.72it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 46.00it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 45.03it/s]
1.817x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  resnet152                          
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 26.66it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 30.41it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 31.87it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 32.10it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 32.16it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 32.40it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 32.68it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 32.14it/s]
2.419x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  resnet18                           
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 228.69it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 230.64it/s]
3.756x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  resnet50                           
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 65.79it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 76.44it/s]running benchmark:  83%|████████▎ | 25/30 [00:00<00:00, 80.13it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 78.94it/s]
1.845x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:resnet50_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/resnet50_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  resnext50_32x4d                    
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 101.33it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 103.00it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 103.79it/s]
3.777x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/__init__.py", line 27, in __init__
    self.model = sam_model_registry[model_type](checkpoint=sam_checkpoint)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/build_sam.py", line 19, in build_sam_vit_h
    return _build_sam(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/build_sam.py", line 108, in _build_sam
    with open(checkpoint, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/.data/sam_vit_h_4b8939.pth'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  shufflenet_v2_x1_0                 
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 82.38it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 89.35it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 91.56it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 90.29it/s]
3.627x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/soft_actor_critic/__init__.py", line 13, in <module>
    from .envs import load_gym
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/soft_actor_critic/envs.py", line 6, in <module>
    import gym
ModuleNotFoundError: No module named 'gym'
Failed to import user benchmark module dynamo, error: No module named 'gym'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/speech_transformer/__init__.py", line 15, in <module>
    from .config import SpeechTransformerTrainConfig, SpeechTransformerEvalConfig
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/speech_transformer/config.py", line 4, in <module>
    import kaldi_io
ModuleNotFoundError: No module named 'kaldi_io'
Failed to import user benchmark module dynamo, error: No module named 'kaldi_io'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  squeezenet1_1                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 292.75it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 292.20it/s]
3.488x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/stable_diffusion_text_encoder/__init__.py", line 11, in <module>
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
ModuleNotFoundError: No module named 'diffusers'
Failed to import user benchmark module dynamo, error: No module named 'diffusers'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/stable_diffusion_unet/__init__.py", line 11, in <module>
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
ModuleNotFoundError: No module named 'diffusers'
Failed to import user benchmark module dynamo, error: No module named 'diffusers'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/timm_efficientdet/__init__.py", line 12, in <module>
    from effdet import create_model, create_loader
ModuleNotFoundError: No module named 'effdet'
Failed to import user benchmark module dynamo, error: No module named 'effdet'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_efficientnet                  
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  20%|██        | 6/30 [00:00<00:00, 53.36it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 57.78it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 59.10it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 59.74it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 59.08it/s]
2.177x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  timm_nfnet                         
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:04,  7.10it/s]running benchmark:  10%|█         | 3/30 [00:00<00:02, 11.80it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:01, 13.34it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 14.10it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 14.52it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 14.77it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:01, 14.93it/s]running benchmark:  50%|█████     | 15/30 [00:01<00:00, 15.05it/s]running benchmark:  57%|█████▋    | 17/30 [00:01<00:00, 15.12it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 15.16it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 15.19it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 15.20it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 15.21it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 15.21it/s]running benchmark:  97%|█████████▋| 29/30 [00:01<00:00, 15.25it/s]running benchmark: 100%|██████████| 30/30 [00:02<00:00, 14.70it/s]
1.844x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_regnet                        
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 23.00it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 28.46it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 30.46it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 31.36it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 31.98it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 32.27it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 32.45it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 31.38it/s]
1.473x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_resnest                       
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 64.66it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 89.54it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 97.93it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 93.40it/s]
1.669x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  timm_vision_transformer            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:00, 42.15it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 47.28it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 48.01it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 48.69it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 49.30it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 48.53it/s]
3.929x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:24, ?it/s]
cuda eval  timm_vision_transformer_large      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:16,  1.73it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:10,  2.57it/s]running benchmark:  10%|█         | 3/30 [00:01<00:08,  3.04it/s]running benchmark:  13%|█▎        | 4/30 [00:01<00:07,  3.33it/s]running benchmark:  17%|█▋        | 5/30 [00:01<00:07,  3.51it/s]running benchmark:  20%|██        | 6/30 [00:01<00:06,  3.63it/s]running benchmark:  23%|██▎       | 7/30 [00:02<00:06,  3.71it/s]running benchmark:  27%|██▋       | 8/30 [00:02<00:05,  3.77it/s]running benchmark:  30%|███       | 9/30 [00:02<00:05,  3.81it/s]running benchmark:  33%|███▎      | 10/30 [00:02<00:05,  3.83it/s]running benchmark:  37%|███▋      | 11/30 [00:03<00:04,  3.85it/s]running benchmark:  40%|████      | 12/30 [00:03<00:04,  3.86it/s]running benchmark:  43%|████▎     | 13/30 [00:03<00:04,  3.87it/s]running benchmark:  47%|████▋     | 14/30 [00:03<00:04,  3.87it/s]running benchmark:  50%|█████     | 15/30 [00:04<00:03,  3.88it/s]running benchmark:  53%|█████▎    | 16/30 [00:04<00:03,  3.88it/s]running benchmark:  57%|█████▋    | 17/30 [00:04<00:03,  3.88it/s]running benchmark:  60%|██████    | 18/30 [00:04<00:03,  3.88it/s]running benchmark:  63%|██████▎   | 19/30 [00:05<00:02,  3.88it/s]running benchmark:  67%|██████▋   | 20/30 [00:05<00:02,  3.88it/s]running benchmark:  70%|███████   | 21/30 [00:05<00:02,  3.88it/s]running benchmark:  73%|███████▎  | 22/30 [00:05<00:02,  3.88it/s]running benchmark:  77%|███████▋  | 23/30 [00:06<00:01,  3.88it/s]running benchmark:  80%|████████  | 24/30 [00:06<00:01,  3.88it/s]running benchmark:  83%|████████▎ | 25/30 [00:06<00:01,  3.88it/s]running benchmark:  87%|████████▋ | 26/30 [00:07<00:01,  3.88it/s]running benchmark:  90%|█████████ | 27/30 [00:07<00:00,  3.88it/s]running benchmark:  93%|█████████▎| 28/30 [00:07<00:00,  3.87it/s]running benchmark:  97%|█████████▋| 29/30 [00:07<00:00,  3.88it/s]running benchmark: 100%|██████████| 30/30 [00:08<00:00,  3.88it/s]running benchmark: 100%|██████████| 30/30 [00:08<00:00,  3.73it/s]
1.330x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_vovnet                        
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 64.37it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 74.11it/s]running benchmark:  83%|████████▎ | 25/30 [00:00<00:00, 77.96it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 77.04it/s]
1.972x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/torch_multimodal_clip/__init__.py", line 10, in <module>
    from torchmultimodal.transforms.clip_transform import CLIPTextTransform, CLIPImageTransform
ModuleNotFoundError: No module named 'torchmultimodal'
Failed to import user benchmark module dynamo, error: No module named 'torchmultimodal'
loading model: 0it [00:00, ?it/s]WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
loading model: 0it [00:00, ?it/s]
WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
cuda eval  tts_angular                        
WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
ERROR:common:Backend eager failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/tts_angular/model.py", line 59, in forward
    d = self.layers(x)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/tts_angular/model.py", line 19, in forward
    return self.linear(o)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  vgg16                              
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 206.77it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 210.10it/s]
1.402x
loading model: 0it [00:00, ?it/s]WARNING:common:Model vision_maskrcnn does not support bfloat16, running with float16 instead
loading model: 0it [00:06, ?it/s]
WARNING:common:Model vision_maskrcnn does not support bfloat16, running with float16 instead
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3632, in run
    model, example_inputs = runner.cast_based_on_args(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1917, in cast_based_on_args
    model, example_inputs = cast_to_fp16(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1675, in cast_to_fp16
    return cast_to(torch.float16, model, inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1657, in cast_to
    model = model.half()
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1007, in half
    return self._apply(lambda t: t.half() if t.is_floating_point() else t)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 826, in _apply
    should_use_set_data = compute_should_use_set_data(param, param_applied)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 805, in compute_should_use_set_data
    if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
TypeError: _has_compatible_shallow_copy_type(): argument 'from' (position 2) must be Tensor, not NoneType
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/yolov3/__init__.py", line 15, in <module>
    from .yolo_train import prepare_training_loop
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/yolov3/yolo_train.py", line 6, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
speedup             gmean=0.00x mean=1.995x
abs_latency         gmean=0.00x mean=7.468x
compilation_latency mean=13.171 seconds
compression_ratio   mean=0.723x
eager_peak_mem      gmean=0.00x mean=0.346x
dynamo_peak_mem     gmean=0.00x mean=0.393x
calls_captured      gmean=0.00x mean=197.426x
unique_graphs       gmean=0.00x mean=1.426x
graph_breaks        gmean=0.00x mean=0.559x
unique_graph_breaks gmean=0.00x mean=0.147x
start int4 weight only
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/canary_models/torchrec_dlrm/__init__.py", line 14, in <module>
    from pyre_extensions import none_throws
ModuleNotFoundError: No module named 'pyre_extensions'
Failed to import user benchmark module dynamo, error: No module named 'pyre_extensions'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  [Previous line repeated 2 more times]
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 433, in from_float
    input_int4x8, scales_and_zeros = groupwise_affine_quantize_tensor(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_primitives.py", line 486, in groupwise_affine_quantize_tensor
    scales, zeros = get_groupwise_affine_qparams(w, n_bit, groupsize)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_primitives.py", line 389, in get_groupwise_affine_qparams
    if groupsize > w.shape[-1]:
AttributeError: 'NoneType' object has no attribute 'shape'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
cuda eval  Background_Matting                 
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:00, 35.58it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 39.99it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 41.36it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 42.02it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 42.34it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 42.54it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 41.79it/s]
2.101x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/DALLE2_pytorch/__init__.py", line 3, in <module>
    from dalle2_pytorch import DALLE2, Unet, Decoder, DiffusionPriorNetwork, DiffusionPrior, OpenAIClipAdapter
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/dalle2_pytorch/__init__.py", line 9, in <module>
    from dalle2_pytorch.dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/dalle2_pytorch/dalle2_pytorch.py", line 18, in <module>
    from kornia.filters import gaussian_blur2d
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/__init__.py", line 8, in <module>
    from . import augmentation, color, contrib, core, enhance, feature, io, losses, metrics, morphology, tracking, utils, x
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/x/__init__.py", line 2, in <module>
    from .trainer import Trainer
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/x/trainer.py", line 11, in <module>
    from accelerate import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 424, in from_float
    assert out_features % 8 == 0, "require out_features % 8 == 0"
AssertionError: require out_features % 8 == 0
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
loading model: 0it [00:03, ?it/s]
WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
cuda eval  Super_SloMo                        
WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:03,  7.94it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 13.86it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:01, 16.04it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 17.11it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 17.70it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 18.06it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 18.30it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 18.45it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 18.57it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 18.64it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 18.67it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 18.71it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 18.74it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 18.75it/s]running benchmark:  97%|█████████▋| 29/30 [00:01<00:00, 18.78it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 17.97it/s]
1.618x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  alexnet                            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 122.42it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 128.59it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 127.94it/s]
1.278x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_edgecnn                  
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 187.39it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 198.22it/s]
1.372x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  basic_gnn_gcn                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 129.00it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 128.95it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 128.65it/s]
1.065x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_gin                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 337.65it/s]
1.211x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_sage                     
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 114.88it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 148.17it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 143.04it/s]
1.100x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  cm3leon_generate                   
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:01<00:39,  1.37s/it]running benchmark:   7%|▋         | 2/30 [00:02<00:35,  1.28s/it]running benchmark:  10%|█         | 3/30 [00:03<00:33,  1.24s/it]running benchmark:  13%|█▎        | 4/30 [00:05<00:32,  1.23s/it]running benchmark:  17%|█▋        | 5/30 [00:06<00:30,  1.22s/it]running benchmark:  20%|██        | 6/30 [00:07<00:29,  1.22s/it]running benchmark:  23%|██▎       | 7/30 [00:08<00:28,  1.22s/it]running benchmark:  27%|██▋       | 8/30 [00:09<00:26,  1.22s/it]running benchmark:  30%|███       | 9/30 [00:11<00:25,  1.22s/it]running benchmark:  33%|███▎      | 10/30 [00:12<00:24,  1.23s/it]running benchmark:  37%|███▋      | 11/30 [00:13<00:23,  1.25s/it]running benchmark:  40%|████      | 12/30 [00:14<00:22,  1.25s/it]running benchmark:  43%|████▎     | 13/30 [00:16<00:21,  1.26s/it]running benchmark:  47%|████▋     | 14/30 [00:17<00:20,  1.25s/it]running benchmark:  50%|█████     | 15/30 [00:18<00:18,  1.24s/it]running benchmark:  53%|█████▎    | 16/30 [00:19<00:17,  1.23s/it]running benchmark:  57%|█████▋    | 17/30 [00:21<00:15,  1.23s/it]running benchmark:  60%|██████    | 18/30 [00:22<00:14,  1.22s/it]running benchmark:  63%|██████▎   | 19/30 [00:23<00:13,  1.21s/it]running benchmark:  67%|██████▋   | 20/30 [00:24<00:12,  1.21s/it]running benchmark:  70%|███████   | 21/30 [00:25<00:10,  1.21s/it]running benchmark:  73%|███████▎  | 22/30 [00:27<00:09,  1.21s/it]running benchmark:  77%|███████▋  | 23/30 [00:28<00:08,  1.20s/it]running benchmark:  80%|████████  | 24/30 [00:29<00:07,  1.20s/it]running benchmark:  83%|████████▎ | 25/30 [00:30<00:05,  1.20s/it]running benchmark:  87%|████████▋ | 26/30 [00:31<00:04,  1.20s/it]running benchmark:  90%|█████████ | 27/30 [00:33<00:03,  1.20s/it]running benchmark:  93%|█████████▎| 28/30 [00:34<00:02,  1.20s/it]running benchmark:  97%|█████████▋| 29/30 [00:35<00:01,  1.20s/it]running benchmark: 100%|██████████| 30/30 [00:36<00:00,  1.20s/it]running benchmark: 100%|██████████| 30/30 [00:36<00:00,  1.22s/it]
4.348x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  dcgan                              
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 495.75it/s]
1.378x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:demucs failed to load
Original Error: "_thnn_fused_lstm_cell_cuda" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/__init__.py", line 31, in forward
    return sources, self.model(mix)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/demucs/model.py", line 209, in forward
    x = self.lstm(x)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/demucs/model.py", line 27, in forward
    x = self.lstm(x)[0]
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/rnn.py", line 878, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
RuntimeError: "_thnn_fused_lstm_cell_cuda" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  densenet121                        
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 17.22it/s]running benchmark:  20%|██        | 6/30 [00:00<00:00, 27.05it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:00, 30.03it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 31.51it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 32.31it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 32.79it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 33.13it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 33.34it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 31.70it/s]
1.984x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:10, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_dc5 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:08, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_dc5 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
loading model: 0it [00:05, ?it/s]
WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
cuda eval  detectron2_fcos_r_50_fpn           
WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
[2023-12-05 04:38:16,178] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (8)
[2023-12-05 04:38:16,178] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:318)
[2023-12-05 04:38:16,178] torch._dynamo.convert_frame: [WARNING]    last reason: L['self']._pos == 0                                           # ret = self[self._pos](x)  # miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:319 in forward
[2023-12-05 04:38:16,178] torch._dynamo.convert_frame: [WARNING] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[2023-12-05 04:38:16,178] torch._dynamo.convert_frame: [WARNING] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.
skipping cudagraphs due to ['mutated inputs']
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 17.24it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:01, 18.52it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 19.37it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 19.33it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 19.65it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 19.81it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 19.99it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 20.08it/s]running benchmark:  80%|████████  | 24/30 [00:01<00:00, 20.00it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 20.07it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 20.13it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 19.82it/s]
1.596x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
WARNING:root:detectron2_maskrcnn_r_101_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_maskrcnn_r_101_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_maskrcnn_r_50_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_maskrcnn_r_50_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:16, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 424, in from_float
    assert out_features % 8 == 0, "require out_features % 8 == 0"
AssertionError: require out_features % 8 == 0
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
loading model: 0it [00:06, ?it/s]
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
cuda eval  doctr_det_predictor                
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
[2023-12-05 04:40:09,454] [1/0_1] torch._inductor.utils: [WARNING] DeviceCopy in input program
skipping cudagraphs due to ['non-cuda device in graph']
malloc_consolidate(): invalid chunk size
Run failed with return code:  -6
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
loading model: 0it [00:08, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 424, in from_float
    assert out_features % 8 == 0, "require out_features % 8 == 0"
AssertionError: require out_features % 8 == 0
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/drq/__init__.py", line 11, in <module>
    from gym import spaces
ModuleNotFoundError: No module named 'gym'
Failed to import user benchmark module dynamo, error: No module named 'gym'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 424, in from_float
    assert out_features % 8 == 0, "require out_features % 8 == 0"
AssertionError: require out_features % 8 == 0
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  functorch_dp_cifar10               
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 234.00it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 235.22it/s]
4.590x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/functorch_maml_omniglot/__init__.py", line 73, in __init__
    self.meta_inputs = torch.load(f'{root}/maml_omniglot/batch.pt')
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/models/maml_omniglot/batch.pt'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py", line 36, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Albert/__init__.py", line 10, in __init__
    super().__init__(name="hf_Albert", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.albert.modeling_albert because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 36, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Bart/__init__.py", line 10, in __init__
    super().__init__(name="hf_Bart", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.bart.modeling_bart because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 39, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_BigBird/__init__.py", line 10, in __init__
    super().__init__(name="hf_BigBird", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.big_bird.modeling_big_bird because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 32, in <module>
    from ...integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_DistilBert/__init__.py", line 10, in __init__
    super().__init__(name="hf_DistilBert", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.distilbert.modeling_distilbert because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel, SequenceSummary
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_GPT2/__init__.py", line 10, in __init__
    super().__init__(name="hf_GPT2", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel, SequenceSummary
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_GPT2_large/__init__.py", line 10, in __init__
    super().__init__(name="hf_GPT2_large", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 27, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Longformer/__init__.py", line 10, in __init__
    super().__init__(name="hf_Longformer", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.longformer.modeling_longformer because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 33, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Reformer/__init__.py", line 10, in __init__
    super().__init__(name="hf_Reformer", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.reformer.modeling_reformer because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_base/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5_base", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_generate/__init__.py", line 5, in __init__
    super().__init__(name="hf_T5_generate", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 189, in __init__
    super().__init__(name=name, test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_large/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5_large", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py", line 35, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Whisper/__init__.py", line 12, in __init__
    super().__init__(name="hf_Whisper", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.whisper.modeling_whisper because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py", line 27, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_clip/__init__.py", line 13, in <module>
    from transformers import CLIPProcessor, CLIPModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1205, in __getattr__
    value = getattr(module, name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.clip.modeling_clip because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 424, in from_float
    assert out_features % 8 == 0, "require out_features % 8 == 0"
AssertionError: require out_features % 8 == 0
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  llama                              
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 25.52it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 30.19it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 31.53it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 32.29it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 32.79it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 33.12it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 33.26it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 32.37it/s]
2.442x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:llama_v2_7b_16h failed to load
Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/llama_v2_7b_16h/__init__.py", line 11, in __init__
    HuggingFaceAuthMixin.__init__(self)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 177, in __init__
    raise NotImplementedError("Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights")
NotImplementedError: Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights

loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/maml_omniglot/__init__.py", line 25, in <module>
    import higher
ModuleNotFoundError: No module named 'higher'
Failed to import user benchmark module dynamo, error: No module named 'higher'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  mnasnet1_0                         
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:00, 99.15it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 113.30it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 113.21it/s]
3.220x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  mobilenet_v2                       
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 84.08it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 102.26it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 103.21it/s]
5.053x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:mobilenet_v2_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/mobilenet_v2_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  mobilenet_v3_large                 
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 75.97it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 79.25it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 80.21it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 79.71it/s]
5.457x
loading model: 0it [00:00, ?it/s]NCCL version 2.19.3+cuda12.0
loading model: 0it [00:03, ?it/s]
cuda eval  moco                               
ERROR:common:Backend eager failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/moco/moco/builder.py", line 130, in forward
    self._momentum_update_key_encoder()  # update the key encoder
  File "/home/cdhernandez/local/pytorch/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/moco/moco/builder.py", line 50, in _momentum_update_key_encoder
    param_k.mul_(self.m).add_(param_q.mul(1. - self.m))
TypeError: add_(): argument 'other' (position 1) must be Tensor, not NoneType
Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
number of parameters: 123.69M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: True
cuda eval  nanogpt                            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  20%|██        | 6/30 [00:00<00:00, 57.01it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 58.03it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 58.39it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 58.43it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 58.51it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 58.32it/s]
7.956x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  nvidia_deeprecommender             
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 201.91it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 205.15it/s]
0.922x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/opacus_cifar10/__init__.py", line 6, in <module>
    from opacus import PrivacyEngine
ModuleNotFoundError: No module named 'opacus'
Failed to import user benchmark module dynamo, error: No module named 'opacus'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/phi_1_5/__init__.py", line 11, in __init__
    super().__init__(name="phi_1_5", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 437, in from_config
    model_class = get_class_from_dynamic_module(class_ref, repo_id, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 497, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module.replace(".py", ""))
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 199, in get_class_in_module
    module = importlib.import_module(module_path)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/5fd430c7bcd28140560faee2014d1228338e19a0/modeling_phi.py", line 16, in <module>
    from transformers import PretrainedConfig, PreTrainedModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 424, in from_float
    assert out_features % 8 == 0, "require out_features % 8 == 0"
AssertionError: require out_features % 8 == 0
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 424, in from_float
    assert out_features % 8 == 0, "require out_features % 8 == 0"
AssertionError: require out_features % 8 == 0
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
cuda eval  pyhpc_equation_of_state            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 169.35it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 188.98it/s]
17.436x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pyhpc_isoneutral_mixing            
skipping cudagraphs due to ['mutated inputs']
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 89.51it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 88.59it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 88.02it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 88.19it/s]
6.006x
loading model: 0it [00:00, ?it/s]WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
loading model: 0it [00:01, ?it/s]
WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
cuda eval  pyhpc_turbulent_kinetic_energy     
WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 19.10it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 56.65it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 68.08it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 73.36it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 66.84it/s]
4.564x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/__init__.py", line 13, in <module>
    from .train_cyclegan import prepare_training_loop
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/train_cyclegan.py", line 27, in <module>
    from .util.visualizer import Visualizer
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/util/visualizer.py", line 6, in <module>
    from . import util, html
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/util/html.py", line 1, in <module>
    import dominate
ModuleNotFoundError: No module named 'dominate'
Failed to import user benchmark module dynamo, error: No module named 'dominate'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/__init__.py", line 52, in __init__
    self.data_loader = self.get_data_loader(config)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/__init__.py", line 68, in get_data_loader
    celeba_loader = get_loader(config.celeba_image_dir, config.attr_path, config.selected_attrs,
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 82, in get_loader
    dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 24, in __init__
    self.preprocess()
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 33, in preprocess
    lines = [line.rstrip() for line in open(self.attr_path, 'r')]
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/data/.data/pytorch_stargan_inputs/data/celeba/list_attr_celeba.txt'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pytorch_unet                       
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:00, 37.42it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 42.76it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 44.48it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 45.27it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 45.71it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 45.98it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 45.02it/s]
1.818x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  resnet152                          
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 26.98it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 31.18it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 32.49it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 33.07it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 33.42it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 33.62it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 33.35it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 32.91it/s]
2.368x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  resnet18                           
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  83%|████████▎ | 25/30 [00:00<00:00, 240.79it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 242.04it/s]
3.469x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  resnet50                           
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 68.26it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 78.04it/s]running benchmark:  83%|████████▎ | 25/30 [00:00<00:00, 81.01it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 80.09it/s]
1.835x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:resnet50_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/resnet50_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  resnext50_32x4d                    
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 104.38it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 106.16it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 106.55it/s]
3.791x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/__init__.py", line 27, in __init__
    self.model = sam_model_registry[model_type](checkpoint=sam_checkpoint)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/build_sam.py", line 19, in build_sam_vit_h
    return _build_sam(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/build_sam.py", line 108, in _build_sam
    with open(checkpoint, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/.data/sam_vit_h_4b8939.pth'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  shufflenet_v2_x1_0                 
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 88.93it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 95.80it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 97.40it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 96.24it/s]
3.441x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/soft_actor_critic/__init__.py", line 13, in <module>
    from .envs import load_gym
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/soft_actor_critic/envs.py", line 6, in <module>
    import gym
ModuleNotFoundError: No module named 'gym'
Failed to import user benchmark module dynamo, error: No module named 'gym'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/speech_transformer/__init__.py", line 15, in <module>
    from .config import SpeechTransformerTrainConfig, SpeechTransformerEvalConfig
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/speech_transformer/config.py", line 4, in <module>
    import kaldi_io
ModuleNotFoundError: No module named 'kaldi_io'
Failed to import user benchmark module dynamo, error: No module named 'kaldi_io'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  squeezenet1_1                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 302.69it/s]
3.552x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/stable_diffusion_text_encoder/__init__.py", line 11, in <module>
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
ModuleNotFoundError: No module named 'diffusers'
Failed to import user benchmark module dynamo, error: No module named 'diffusers'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/stable_diffusion_unet/__init__.py", line 11, in <module>
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
ModuleNotFoundError: No module named 'diffusers'
Failed to import user benchmark module dynamo, error: No module named 'diffusers'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/timm_efficientdet/__init__.py", line 12, in <module>
    from effdet import create_model, create_loader
ModuleNotFoundError: No module named 'effdet'
Failed to import user benchmark module dynamo, error: No module named 'effdet'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_efficientnet                  
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  20%|██        | 6/30 [00:00<00:00, 52.47it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 57.22it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 58.81it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 59.53it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 58.78it/s]
2.185x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
cuda eval  timm_nfnet                         
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:04,  7.07it/s]running benchmark:  10%|█         | 3/30 [00:00<00:02, 11.76it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:01, 13.36it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 14.14it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 14.56it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 14.82it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:01, 14.96it/s]running benchmark:  50%|█████     | 15/30 [00:01<00:00, 15.09it/s]running benchmark:  57%|█████▋    | 17/30 [00:01<00:00, 15.17it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 15.22it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 15.24it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 15.26it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 15.28it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 15.27it/s]running benchmark:  97%|█████████▋| 29/30 [00:01<00:00, 15.26it/s]running benchmark: 100%|██████████| 30/30 [00:02<00:00, 14.73it/s]
1.858x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  timm_regnet                        
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 19.85it/s]running benchmark:  20%|██        | 6/30 [00:00<00:00, 28.91it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:00, 31.42it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 32.51it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 33.09it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 33.42it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 33.67it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 33.83it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 32.60it/s]
1.349x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_resnest                       
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 85.73it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 98.92it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 100.45it/s]
1.679x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  timm_vision_transformer            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:05,  5.17it/s]running benchmark:  10%|█         | 3/30 [00:00<00:02,  9.28it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:02, 10.83it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 11.61it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 12.06it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 12.33it/s]running benchmark:  43%|████▎     | 13/30 [00:01<00:01, 12.50it/s]running benchmark:  50%|█████     | 15/30 [00:01<00:01, 12.62it/s]running benchmark:  57%|█████▋    | 17/30 [00:01<00:01, 12.71it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 12.76it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 12.80it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 12.83it/s]running benchmark:  83%|████████▎ | 25/30 [00:02<00:00, 12.85it/s]running benchmark:  90%|█████████ | 27/30 [00:02<00:00, 12.86it/s]running benchmark:  97%|█████████▋| 29/30 [00:02<00:00, 12.87it/s]running benchmark: 100%|██████████| 30/30 [00:02<00:00, 12.27it/s]
1.046x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:20, ?it/s]
cuda eval  timm_vision_transformer_large      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:06<02:55,  6.07s/it]running benchmark:   7%|▋         | 2/30 [00:08<01:50,  3.94s/it]running benchmark:  10%|█         | 3/30 [00:10<01:27,  3.26s/it]running benchmark:  13%|█▎        | 4/30 [00:13<01:16,  2.94s/it]running benchmark:  17%|█▋        | 5/30 [00:15<01:09,  2.76s/it]running benchmark:  20%|██        | 6/30 [00:18<01:03,  2.66s/it]running benchmark:  23%|██▎       | 7/30 [00:20<00:59,  2.59s/it]running benchmark:  27%|██▋       | 8/30 [00:23<00:55,  2.54s/it]running benchmark:  30%|███       | 9/30 [00:25<00:52,  2.51s/it]running benchmark:  33%|███▎      | 10/30 [00:28<00:49,  2.49s/it]running benchmark:  37%|███▋      | 11/30 [00:30<00:47,  2.48s/it]running benchmark:  40%|████      | 12/30 [00:33<00:44,  2.47s/it]running benchmark:  43%|████▎     | 13/30 [00:35<00:41,  2.46s/it]running benchmark:  47%|████▋     | 14/30 [00:37<00:39,  2.46s/it]running benchmark:  50%|█████     | 15/30 [00:40<00:37,  2.47s/it]running benchmark:  53%|█████▎    | 16/30 [00:42<00:34,  2.46s/it]running benchmark:  57%|█████▋    | 17/30 [00:45<00:31,  2.46s/it]running benchmark:  60%|██████    | 18/30 [00:47<00:29,  2.45s/it]running benchmark:  63%|██████▎   | 19/30 [00:50<00:26,  2.45s/it]running benchmark:  67%|██████▋   | 20/30 [00:52<00:24,  2.45s/it]running benchmark:  70%|███████   | 21/30 [00:55<00:22,  2.45s/it]running benchmark:  73%|███████▎  | 22/30 [00:57<00:19,  2.45s/it]running benchmark:  77%|███████▋  | 23/30 [00:59<00:17,  2.45s/it]running benchmark:  80%|████████  | 24/30 [01:02<00:14,  2.45s/it]running benchmark:  83%|████████▎ | 25/30 [01:04<00:12,  2.45s/it]running benchmark:  87%|████████▋ | 26/30 [01:07<00:09,  2.45s/it]running benchmark:  90%|█████████ | 27/30 [01:09<00:07,  2.45s/it]running benchmark:  93%|█████████▎| 28/30 [01:12<00:04,  2.45s/it]running benchmark:  97%|█████████▋| 29/30 [01:14<00:02,  2.45s/it]running benchmark: 100%|██████████| 30/30 [01:17<00:00,  2.45s/it]running benchmark: 100%|██████████| 30/30 [01:17<00:00,  2.57s/it]
1.024x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_vovnet                        
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:05,  5.27it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:00, 41.51it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 59.64it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 69.97it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 58.17it/s]
1.755x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/torch_multimodal_clip/__init__.py", line 10, in <module>
    from torchmultimodal.transforms.clip_transform import CLIPTextTransform, CLIPImageTransform
ModuleNotFoundError: No module named 'torchmultimodal'
Failed to import user benchmark module dynamo, error: No module named 'torchmultimodal'
loading model: 0it [00:00, ?it/s]WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
loading model: 0it [00:01, ?it/s]
WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
cuda eval  tts_angular                        
WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
ERROR:common:Backend eager failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/tts_angular/model.py", line 59, in forward
    d = self.layers(x)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/tts_angular/model.py", line 19, in forward
    return self.linear(o)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  vgg16                              
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 279.90it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 278.74it/s]
1.714x
loading model: 0it [00:00, ?it/s]WARNING:common:Model vision_maskrcnn does not support bfloat16, running with float16 instead
loading model: 0it [00:04, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3571, in run
    change_linear_weights_to_int4_woqtensors(model)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 131, in change_linear_weights_to_int4_woqtensors
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 57, in _replace_with_custom_fn_if_matches_filter
    _replace_with_custom_fn_if_matches_filter(
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 54, in _replace_with_custom_fn_if_matches_filter
    new_child = replacement_fn(child)
  File "/home/cdhernandez/local/ao/torchao/quantization/quant_api.py", line 90, in insert_subclass
    cls.from_float(lin.weight, **kwargs), requires_grad=False
  File "/home/cdhernandez/local/ao/torchao/quantization/subclass.py", line 424, in from_float
    assert out_features % 8 == 0, "require out_features % 8 == 0"
AssertionError: require out_features % 8 == 0
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/yolov3/__init__.py", line 15, in <module>
    from .yolo_train import prepare_training_loop
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/yolov3/yolo_train.py", line 6, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
speedup             gmean=0.00x mean=1.611x
abs_latency         gmean=0.00x mean=24.112x
compilation_latency mean=12.122 seconds
compression_ratio   mean=0.680x
eager_peak_mem      gmean=0.00x mean=0.293x
dynamo_peak_mem     gmean=0.00x mean=0.327x
calls_captured      gmean=0.00x mean=191.941x
unique_graphs       gmean=0.00x mean=1.353x
graph_breaks        gmean=0.00x mean=0.559x
unique_graph_breaks gmean=0.00x mean=0.147x
start baseline
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/canary_models/torchrec_dlrm/__init__.py", line 14, in <module>
    from pyre_extensions import none_throws
ModuleNotFoundError: No module named 'pyre_extensions'
Failed to import user benchmark module dynamo, error: No module named 'pyre_extensions'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  BERT_pytorch                       
AUTOTUNE mm(2048x768, 768x768)
  mm 0.0203 ms 100.0%
  triton_mm_2 0.0221 ms 91.6%
  triton_mm_1 0.0222 ms 91.4%
  triton_mm_3 0.0236 ms 85.9%
  triton_mm_4 0.0242 ms 84.0%
  triton_mm_8 0.0280 ms 72.5%
  triton_mm_0 0.0307 ms 66.2%
  triton_mm_7 0.0372 ms 54.5%
  triton_mm_10 0.0412 ms 49.2%
  triton_mm_9 0.0448 ms 45.3%
SingleProcess AUTOTUNE takes 4.7461 seconds
AUTOTUNE bmm(192x128x64, 192x64x128)
  triton_bmm_32 0.0158 ms 100.0%
  triton_bmm_25 0.0165 ms 95.7%
  triton_bmm_26 0.0168 ms 93.7%
  triton_bmm_27 0.0171 ms 92.3%
  triton_bmm_28 0.0172 ms 91.5%
  triton_bmm_24 0.0177 ms 89.0%
  bmm 0.0183 ms 86.3%
  triton_bmm_34 0.0184 ms 85.9%
  triton_bmm_31 0.0187 ms 84.4%
  triton_bmm_33 0.0195 ms 81.0%
SingleProcess AUTOTUNE takes 4.7135 seconds
AUTOTUNE bmm(192x128x128, 192x128x64)
  triton_bmm_55 0.0166 ms 100.0%
  triton_bmm_48 0.0168 ms 98.9%
  triton_bmm_58 0.0175 ms 94.5%
  triton_bmm_50 0.0178 ms 93.2%
  triton_bmm_49 0.0185 ms 89.8%
  triton_bmm_52 0.0186 ms 89.3%
  triton_bmm_56 0.0186 ms 89.2%
  triton_bmm_51 0.0192 ms 86.2%
  triton_bmm_54 0.0195 ms 85.2%
  triton_bmm_53 0.0199 ms 83.3%
SingleProcess AUTOTUNE takes 4.0346 seconds
AUTOTUNE mm(2048x768, 768x3072)
  mm 0.0542 ms 100.0%
  triton_mm_73 0.0618 ms 87.7%
  triton_mm_74 0.0627 ms 86.4%
  triton_mm_75 0.0733 ms 73.9%
  triton_mm_76 0.0746 ms 72.6%
  triton_mm_79 0.0760 ms 71.3%
  triton_mm_72 0.0815 ms 66.4%
  triton_mm_80 0.0827 ms 65.5%
  triton_mm_82 0.1251 ms 43.3%
  triton_mm_78 0.1475 ms 36.7%
SingleProcess AUTOTUNE takes 5.2294 seconds
AUTOTUNE mm(2048x3072, 3072x768)
  mm 0.0490 ms 100.0%
  triton_mm_85 0.0640 ms 76.5%
  triton_mm_86 0.0645 ms 75.9%
  triton_mm_87 0.0678 ms 72.2%
  triton_mm_88 0.0681 ms 71.9%
  triton_mm_92 0.0819 ms 59.8%
  triton_mm_84 0.0899 ms 54.5%
  triton_mm_91 0.1020 ms 48.0%
  triton_mm_94 0.1438 ms 34.0%
  triton_mm_89 0.1486 ms 33.0%
SingleProcess AUTOTUNE takes 4.7370 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 65.73it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 69.36it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 70.52it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 70.32it/s]
3.267x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
cuda eval  Background_Matting                 
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:00, 33.28it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 38.97it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 40.78it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 41.57it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 42.11it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 42.38it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 41.34it/s]
2.098x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/DALLE2_pytorch/__init__.py", line 3, in <module>
    from dalle2_pytorch import DALLE2, Unet, Decoder, DiffusionPriorNetwork, DiffusionPrior, OpenAIClipAdapter
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/dalle2_pytorch/__init__.py", line 9, in <module>
    from dalle2_pytorch.dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, Unet, Decoder
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/dalle2_pytorch/dalle2_pytorch.py", line 18, in <module>
    from kornia.filters import gaussian_blur2d
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/__init__.py", line 8, in <module>
    from . import augmentation, color, contrib, core, enhance, feature, io, losses, metrics, morphology, tracking, utils, x
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/x/__init__.py", line 2, in <module>
    from .trainer import Trainer
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/kornia/x/trainer.py", line 11, in <module>
    from accelerate import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
cuda eval  LearningToPaint                    
AUTOTUNE mm(96x512, 512x65)
  triton_mm_155 0.0104 ms 100.0%
  triton_mm_152 0.0107 ms 97.5%
  triton_mm_151 0.0112 ms 93.4%
  mm 0.0121 ms 86.5%
  triton_mm_149 0.0127 ms 82.3%
  triton_mm_150 0.0127 ms 82.1%
  triton_mm_154 0.0128 ms 81.3%
  triton_mm_148 0.0145 ms 72.1%
  triton_mm_147 0.0150 ms 69.7%
  triton_mm_146 0.0193 ms 54.1%
SingleProcess AUTOTUNE takes 6.0231 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 187.77it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 196.82it/s]
2.087x
loading model: 0it [00:00, ?it/s]WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
loading model: 0it [00:04, ?it/s]
WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
cuda eval  Super_SloMo                        
WARNING:common:Model Super_SloMo does not support bfloat16, running with amp instead
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:03,  9.01it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 14.72it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:01, 16.59it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 17.51it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 17.99it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 18.30it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 18.49it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 18.61it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 18.70it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 18.74it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 18.78it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 18.79it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 18.81it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 18.82it/s]running benchmark:  97%|█████████▋| 29/30 [00:01<00:00, 18.83it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 18.20it/s]
1.621x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  alexnet                            
AUTOTUNE mm(128x9216, 9216x4096)
  mm 0.0839 ms 100.0%
  triton_mm_38 0.1176 ms 71.3%
  triton_mm_37 0.1177 ms 71.3%
  triton_mm_42 0.1241 ms 67.6%
  triton_mm_36 0.1619 ms 51.8%
  triton_mm_35 0.1620 ms 51.8%
  triton_mm_39 0.1685 ms 49.8%
  triton_mm_40 0.1694 ms 49.5%
  triton_mm_34 0.2092 ms 40.1%
  triton_mm_43 0.2121 ms 39.6%
SingleProcess AUTOTUNE takes 4.6449 seconds
AUTOTUNE mm(128x4096, 4096x4096)
  mm 0.0511 ms 100.0%
  triton_mm_50 0.0596 ms 85.7%
  triton_mm_49 0.0596 ms 85.7%
  triton_mm_54 0.0616 ms 83.0%
  triton_mm_52 0.0794 ms 64.3%
  triton_mm_51 0.0796 ms 64.1%
  triton_mm_48 0.0797 ms 64.1%
  triton_mm_47 0.0799 ms 63.9%
  triton_mm_55 0.0995 ms 51.3%
  triton_mm_46 0.1001 ms 51.0%
SingleProcess AUTOTUNE takes 4.3684 seconds
AUTOTUNE addmm(128x1000, 128x4096, 4096x1000)
  bias_addmm 0.0235 ms 100.0%
  addmm 0.0276 ms 85.3%
  triton_mm_64 0.0388 ms 60.6%
  triton_mm_63 0.0389 ms 60.5%
  triton_mm_67 0.0423 ms 55.6%
  triton_mm_66 0.0437 ms 53.8%
  triton_mm_62 0.0534 ms 44.0%
  triton_mm_61 0.0539 ms 43.6%
  triton_mm_60 0.0725 ms 32.4%
  triton_mm_59 0.0738 ms 31.9%
SingleProcess AUTOTUNE takes 4.8962 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 153.49it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 157.99it/s]
1.360x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_edgecnn                  
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 212.37it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 214.61it/s]
1.374x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_gcn                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 19.42it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 83.32it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 104.12it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 93.67it/s] 
1.064x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  basic_gnn_gin                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 356.11it/s]
1.213x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  basic_gnn_sage                     
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 162.10it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 166.39it/s]
1.101x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  cm3leon_generate                   
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:01<00:35,  1.21s/it]running benchmark:   7%|▋         | 2/30 [00:02<00:33,  1.21s/it]running benchmark:  10%|█         | 3/30 [00:03<00:32,  1.21s/it]running benchmark:  13%|█▎        | 4/30 [00:04<00:31,  1.21s/it]running benchmark:  17%|█▋        | 5/30 [00:06<00:30,  1.21s/it]running benchmark:  20%|██        | 6/30 [00:07<00:28,  1.21s/it]running benchmark:  23%|██▎       | 7/30 [00:08<00:27,  1.21s/it]running benchmark:  27%|██▋       | 8/30 [00:09<00:26,  1.20s/it]running benchmark:  30%|███       | 9/30 [00:10<00:25,  1.20s/it]running benchmark:  33%|███▎      | 10/30 [00:12<00:24,  1.20s/it]running benchmark:  37%|███▋      | 11/30 [00:13<00:22,  1.20s/it]running benchmark:  40%|████      | 12/30 [00:14<00:21,  1.20s/it]running benchmark:  43%|████▎     | 13/30 [00:15<00:20,  1.20s/it]running benchmark:  47%|████▋     | 14/30 [00:16<00:19,  1.20s/it]running benchmark:  50%|█████     | 15/30 [00:18<00:18,  1.20s/it]running benchmark:  53%|█████▎    | 16/30 [00:19<00:17,  1.22s/it]running benchmark:  57%|█████▋    | 17/30 [00:20<00:15,  1.22s/it]running benchmark:  60%|██████    | 18/30 [00:21<00:14,  1.21s/it]running benchmark:  63%|██████▎   | 19/30 [00:22<00:13,  1.21s/it]running benchmark:  67%|██████▋   | 20/30 [00:24<00:12,  1.21s/it]running benchmark:  70%|███████   | 21/30 [00:25<00:10,  1.21s/it]running benchmark:  73%|███████▎  | 22/30 [00:26<00:09,  1.21s/it]running benchmark:  77%|███████▋  | 23/30 [00:27<00:08,  1.21s/it]running benchmark:  80%|████████  | 24/30 [00:29<00:07,  1.21s/it]running benchmark:  83%|████████▎ | 25/30 [00:30<00:06,  1.21s/it]running benchmark:  87%|████████▋ | 26/30 [00:31<00:04,  1.21s/it]running benchmark:  90%|█████████ | 27/30 [00:32<00:03,  1.21s/it]running benchmark:  93%|█████████▎| 28/30 [00:33<00:02,  1.21s/it]running benchmark:  97%|█████████▋| 29/30 [00:35<00:01,  1.21s/it]running benchmark: 100%|██████████| 30/30 [00:36<00:00,  1.21s/it]running benchmark: 100%|██████████| 30/30 [00:36<00:00,  1.21s/it]
4.346x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  dcgan                              
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 420.81it/s]
1.384x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:demucs failed to load
Original Error: "_thnn_fused_lstm_cell_cuda" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/__init__.py", line 31, in forward
    return sources, self.model(mix)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/demucs/model.py", line 209, in forward
    x = self.lstm(x)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/demucs/demucs/model.py", line 27, in forward
    x = self.lstm(x)[0]
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/rnn.py", line 878, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
RuntimeError: "_thnn_fused_lstm_cell_cuda" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  densenet121                        
AUTOTUNE addmm(64x1000, 64x1024, 1024x1000)
  bias_addmm 0.0135 ms 100.0%
  triton_mm_1149 0.0152 ms 88.8%
  triton_mm_1153 0.0154 ms 87.5%
  triton_mm_1150 0.0155 ms 87.2%
  triton_mm_1152 0.0165 ms 81.6%
  addmm 0.0169 ms 79.7%
  triton_mm_1148 0.0173 ms 77.7%
  triton_mm_1147 0.0198 ms 68.0%
  triton_mm_1146 0.0221 ms 60.9%
  triton_mm_1145 0.0246 ms 54.7%
SingleProcess AUTOTUNE takes 5.2790 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 23.58it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 29.59it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 31.65it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 32.63it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 33.19it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 33.52it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 33.73it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 32.70it/s]
2.025x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:08, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:08, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_dc5 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_101_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_dc5 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_fasterrcnn_r_50_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
loading model: 0it [00:05, ?it/s]
WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
cuda eval  detectron2_fcos_r_50_fpn           
WARNING:common:Model detectron2_fcos_r_50_fpn does not support bfloat16, running with amp instead
[2023-12-05 05:09:40,395] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (8)
[2023-12-05 05:09:40,395] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:318)
[2023-12-05 05:09:40,395] torch._dynamo.convert_frame: [WARNING]    last reason: L['self']._pos == 0                                           # ret = self[self._pos](x)  # miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:319 in forward
[2023-12-05 05:09:40,395] torch._dynamo.convert_frame: [WARNING] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[2023-12-05 05:09:40,395] torch._dynamo.convert_frame: [WARNING] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.
skipping cudagraphs due to ['mutated inputs']
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:01, 17.19it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:01, 18.64it/s]running benchmark:  20%|██        | 6/30 [00:00<00:01, 19.22it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 19.64it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 19.82it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 19.84it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 19.87it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 19.97it/s]running benchmark:  73%|███████▎  | 22/30 [00:01<00:00, 20.05it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 20.07it/s]running benchmark:  93%|█████████▎| 28/30 [00:01<00:00, 20.03it/s]running benchmark: 100%|██████████| 30/30 [00:01<00:00, 19.82it/s]
1.583x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_maskrcnn_r_101_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:06, ?it/s]
WARNING:root:detectron2_maskrcnn_r_101_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
WARNING:root:detectron2_maskrcnn_r_50_c4 failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 477, in forward
    box_features = self._shared_roi_transform(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 456, in _shared_roi_transform
    x = self.pooler(features, boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 246, in forward
    return self.level_poolers[0](x[0], pooler_fmt_boxes)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:07, ?it/s]
WARNING:root:detectron2_maskrcnn_r_50_fpn failed to load
Original Error: "roi_align_forward_kernel" not implemented for 'BFloat16'
Eager model failed to run
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1931, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 150, in forward
    return self.inference(batched_inputs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/roi_heads/roi_heads.py", line 798, in _forward_box
    box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/modeling/poolers.py", line 261, in forward
    output.index_put_((inds,), pooler(x[level], pooler_fmt_boxes_level))
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/detectron2/layers/roi_align.py", line 58, in forward
    return roi_align(
  File "/home/cdhernandez/local/vision/torchvision/ops/roi_align.py", line 238, in roi_align
    return torch.ops.torchvision.roi_align(
  File "/home/cdhernandez/local/pytorch/torch/_ops.py", line 753, in __call__
    return self._op(*args, **kwargs or {})
RuntimeError: "roi_align_forward_kernel" not implemented for 'BFloat16'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 465, in load_model
    self.validate_model(model, example_inputs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1934, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

loading model: 0it [00:00, ?it/s]loading model: 0it [00:15, ?it/s]
cuda eval  dlrm                               
AUTOTUNE mm(2048x512, 512x512)
  mm 0.0166 ms 100.0%
  triton_mm_2 0.0171 ms 97.4%
  triton_mm_1 0.0172 ms 96.7%
  triton_mm_3 0.0181 ms 92.0%
  triton_mm_8 0.0183 ms 91.1%
  triton_mm_4 0.0187 ms 89.0%
  triton_mm_0 0.0217 ms 76.6%
  triton_mm_5 0.0246 ms 67.6%
  triton_mm_6 0.0249 ms 66.8%
  triton_mm_9 0.0263 ms 63.3%
SingleProcess AUTOTUNE takes 4.4183 seconds
AUTOTUNE mm(2048x512, 512x64)
  triton_mm_17 0.0115 ms 100.0%
  triton_mm_20 0.0116 ms 99.3%
  mm 0.0117 ms 98.2%
  triton_mm_18 0.0117 ms 98.2%
  triton_mm_15 0.0119 ms 96.9%
  triton_mm_21 0.0120 ms 96.3%
  triton_mm_16 0.0137 ms 83.9%
  triton_mm_13 0.0146 ms 79.0%
  triton_mm_14 0.0155 ms 74.3%
  triton_mm_12 0.0214 ms 53.7%
SingleProcess AUTOTUNE takes 4.7303 seconds
AUTOTUNE mm(2048x100, 100x1024)
  triton_mm_33 0.0141 ms 100.0%
  triton_mm_34 0.0142 ms 99.5%
  triton_mm_32 0.0142 ms 99.3%
  triton_mm_39 0.0148 ms 95.7%
  triton_mm_42 0.0149 ms 94.7%
  triton_mm_36 0.0157 ms 89.6%
  triton_mm_40 0.0159 ms 88.6%
  triton_mm_35 0.0161 ms 87.5%
  triton_mm_41 0.0188 ms 75.0%
  mm 0.0193 ms 73.3%
SingleProcess AUTOTUNE takes 4.8394 seconds
AUTOTUNE mm(2048x1024, 1024x1024)
  triton_mm_46 0.0330 ms 100.0%
  triton_mm_45 0.0336 ms 98.2%
  mm 0.0355 ms 93.0%
  triton_mm_47 0.0396 ms 83.3%
  triton_mm_48 0.0396 ms 83.1%
  triton_mm_52 0.0400 ms 82.4%
  triton_mm_44 0.0405 ms 81.5%
  triton_mm_54 0.0584 ms 56.4%
  triton_mm_51 0.0628 ms 52.5%
  triton_mm_53 0.0682 ms 48.4%
SingleProcess AUTOTUNE takes 4.9097 seconds
AUTOTUNE mm(2048x1024, 1024x1)
  mm 0.0126 ms 100.0%
  triton_mm_76 0.0147 ms 85.7%
  triton_mm_73 0.0151 ms 83.7%
  triton_mm_71 0.0155 ms 81.6%
  triton_mm_74 0.0162 ms 77.7%
  triton_mm_77 0.0169 ms 74.8%
  triton_mm_72 0.0171 ms 73.6%
  triton_mm_69 0.0208 ms 60.6%
  triton_mm_70 0.0221 ms 56.9%
  triton_mm_68 0.0338 ms 37.3%
SingleProcess AUTOTUNE takes 3.9844 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 291.62it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 290.91it/s]
1.145x
loading model: 0it [00:00, ?it/s]WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
loading model: 0it [00:05, ?it/s]
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
cuda eval  doctr_det_predictor                
WARNING:common:Model doctr_det_predictor does not support bfloat16, running with amp instead
[2023-12-05 05:12:00,802] [1/0_1] torch._inductor.utils: [WARNING] DeviceCopy in input program
skipping cudagraphs due to ['non-cuda device in graph']
malloc_consolidate(): invalid chunk size
Run failed with return code:  -6
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
loading model: 0it [00:05, ?it/s]
WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
cuda eval  doctr_reco_predictor               
WARNING:common:Model doctr_reco_predictor does not support bfloat16, running with amp instead
AUTOTUNE convolution(1x3x32x128, 64x3x3x3)
  convolution 0.0098 ms 100.0%
  triton_convolution_4 0.0116 ms 83.8%
  triton_convolution_3 0.0140 ms 70.0%
  triton_convolution_0 0.0180 ms 54.4%
  triton_convolution_5 0.0190 ms 51.3%
  triton_convolution_2 0.0223 ms 43.8%
  triton_convolution_1 0.0421 ms 23.2%
SingleProcess AUTOTUNE takes 2.9145 seconds
AUTOTUNE convolution(1x64x32x128, 64x64x3x3)
  convolution 0.0153 ms 100.0%
  triton_convolution_11 0.0388 ms 39.4%
  triton_convolution_6 0.0576 ms 26.5%
  triton_convolution_10 0.0591 ms 25.9%
  triton_convolution_9 0.0593 ms 25.8%
  triton_convolution_12 0.0626 ms 24.4%
  triton_convolution_7 0.1006 ms 15.2%
  triton_convolution_8 0.2507 ms 6.1%
SingleProcess AUTOTUNE takes 3.6041 seconds
AUTOTUNE convolution(1x64x16x64, 128x64x3x3)
  convolution 0.0112 ms 100.0%
  triton_convolution_18 0.0414 ms 27.0%
  triton_convolution_17 0.0524 ms 21.4%
  triton_convolution_16 0.0610 ms 18.4%
  triton_convolution_19 0.0646 ms 17.3%
  triton_convolution_13 0.0703 ms 15.9%
  triton_convolution_14 0.1003 ms 11.2%
  triton_convolution_15 0.2486 ms 4.5%
SingleProcess AUTOTUNE takes 4.0060 seconds
AUTOTUNE convolution(1x128x16x64, 128x128x3x3)
  convolution 0.0138 ms 100.0%
  triton_convolution_25 0.0762 ms 18.1%
  triton_convolution_24 0.0967 ms 14.2%
  triton_convolution_23 0.1141 ms 12.1%
  triton_convolution_26 0.1228 ms 11.2%
  triton_convolution_20 0.1375 ms 10.0%
  triton_convolution_21 0.2372 ms 5.8%
  triton_convolution_22 0.4978 ms 2.8%
SingleProcess AUTOTUNE takes 4.1799 seconds
AUTOTUNE convolution(1x128x8x32, 256x128x3x3)
  convolution 0.0133 ms 100.0%
  triton_convolution_31 0.0876 ms 15.2%
  triton_convolution_32 0.1034 ms 12.9%
  triton_convolution_30 0.1102 ms 12.1%
  triton_convolution_33 0.1305 ms 10.2%
  triton_convolution_29 0.1468 ms 9.1%
  triton_convolution_28 0.2418 ms 5.5%
  triton_convolution_27 0.2586 ms 5.1%
SingleProcess AUTOTUNE takes 4.1310 seconds
AUTOTUNE convolution(1x256x8x32, 256x256x3x3)
  convolution 0.0174 ms 100.0%
  triton_convolution_38 0.1749 ms 9.9%
  triton_convolution_39 0.2200 ms 7.9%
  triton_convolution_37 0.2349 ms 7.4%
  triton_convolution_40 0.2918 ms 6.0%
  triton_convolution_36 0.3013 ms 5.8%
  triton_convolution_35 0.5442 ms 3.2%
  triton_convolution_34 0.5459 ms 3.2%
SingleProcess AUTOTUNE takes 4.2467 seconds
AUTOTUNE convolution(1x256x4x32, 512x256x3x3)
  convolution 0.0179 ms 100.0%
  triton_convolution_54 0.1734 ms 10.3%
  triton_convolution_52 0.1764 ms 10.1%
  triton_convolution_50 0.1848 ms 9.7%
  triton_convolution_53 0.2233 ms 8.0%
  triton_convolution_51 0.2347 ms 7.6%
  triton_convolution_49 0.3216 ms 5.6%
  triton_convolution_48 0.5666 ms 3.2%
SingleProcess AUTOTUNE takes 3.5733 seconds
AUTOTUNE convolution(1x512x4x32, 512x512x3x3)
  convolution 0.0222 ms 100.0%
  triton_convolution_57 0.3297 ms 6.7%
  triton_convolution_61 0.4474 ms 5.0%
  triton_convolution_59 0.4540 ms 4.9%
  triton_convolution_60 0.4719 ms 4.7%
  triton_convolution_58 0.6432 ms 3.5%
  triton_convolution_56 1.0089 ms 2.2%
  triton_convolution_55 1.1690 ms 1.9%
SingleProcess AUTOTUNE takes 3.7389 seconds
AUTOTUNE convolution(1x512x2x32, 512x512x3x3)
  convolution 0.0224 ms 100.0%
  triton_convolution_71 0.2484 ms 9.0%
  triton_convolution_75 0.2981 ms 7.5%
  triton_convolution_72 0.3415 ms 6.5%
  triton_convolution_73 0.4001 ms 5.6%
  triton_convolution_74 0.4671 ms 4.8%
  triton_convolution_70 0.4740 ms 4.7%
  triton_convolution_69 1.1694 ms 1.9%
SingleProcess AUTOTUNE takes 3.1669 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 193.67it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 193.54it/s]
2.405x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/drq/__init__.py", line 11, in <module>
    from gym import spaces
ModuleNotFoundError: No module named 'gym'
Failed to import user benchmark module dynamo, error: No module named 'gym'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  fastNLP_Bert                       
AUTOTUNE mm(475x768, 768x768)
  mm 0.0133 ms 100.0%
  triton_mm_8 0.0144 ms 92.5%
  triton_mm_3 0.0161 ms 82.8%
  triton_mm_4 0.0161 ms 82.6%
  triton_mm_6 0.0167 ms 79.6%
  triton_mm_5 0.0167 ms 79.4%
  triton_mm_9 0.0173 ms 76.9%
  triton_mm_2 0.0190 ms 70.1%
  triton_mm_1 0.0190 ms 69.9%
  triton_mm_0 0.0271 ms 49.1%
SingleProcess AUTOTUNE takes 5.2544 seconds
AUTOTUNE mm(475x768, 768x3072)
  mm 0.0204 ms 100.0%
  triton_mm_49 0.0230 ms 88.7%
  triton_mm_50 0.0232 ms 88.3%
  triton_mm_51 0.0237 ms 86.2%
  triton_mm_52 0.0239 ms 85.4%
  triton_mm_56 0.0286 ms 71.5%
  triton_mm_48 0.0318 ms 64.3%
  triton_mm_55 0.0372 ms 54.9%
  triton_mm_58 0.0421 ms 48.6%
  triton_mm_54 0.0423 ms 48.3%
SingleProcess AUTOTUNE takes 5.4994 seconds
AUTOTUNE mm(475x3072, 3072x768)
  mm 0.0267 ms 100.0%
  triton_mm_68 0.0348 ms 76.7%
  triton_mm_63 0.0414 ms 64.6%
  triton_mm_64 0.0414 ms 64.5%
  triton_mm_65 0.0443 ms 60.3%
  triton_mm_66 0.0444 ms 60.1%
  triton_mm_69 0.0454 ms 58.9%
  triton_mm_61 0.0556 ms 48.1%
  triton_mm_62 0.0556 ms 48.1%
  triton_mm_60 0.0695 ms 38.4%
SingleProcess AUTOTUNE takes 4.4570 seconds
AUTOTUNE mm(1x768, 768x768)
  mm 0.0096 ms 100.0%
  triton_mm_869 0.0116 ms 82.9%
  triton_mm_872 0.0121 ms 79.4%
  triton_mm_870 0.0123 ms 78.4%
  triton_mm_868 0.0124 ms 77.6%
  triton_mm_873 0.0132 ms 73.1%
  triton_mm_867 0.0139 ms 69.4%
  triton_mm_866 0.0164 ms 58.9%
  triton_mm_865 0.0173 ms 55.6%
  triton_mm_864 0.0255 ms 37.8%
SingleProcess AUTOTUNE takes 3.6349 seconds
AUTOTUNE addmm(473x2, 473x768, 768x2)
  triton_mm_882 0.0113 ms 100.0%
  triton_mm_884 0.0119 ms 95.1%
  triton_mm_881 0.0123 ms 91.7%
  triton_mm_879 0.0125 ms 90.5%
  triton_mm_885 0.0125 ms 90.5%
  bias_addmm 0.0130 ms 86.7%
  triton_mm_880 0.0139 ms 81.1%
  triton_mm_877 0.0162 ms 69.9%
  addmm 0.0171 ms 66.0%
  triton_mm_878 0.0174 ms 64.8%
SingleProcess AUTOTUNE takes 4.2997 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:00, 46.61it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 58.47it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 62.87it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 65.29it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 63.22it/s]
2.746x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  functorch_dp_cifar10               
AUTOTUNE addmm(64x1000, 64x512, 512x1000)
  triton_mm_148 0.0115 ms 100.0%
  triton_mm_145 0.0117 ms 97.7%
  bias_addmm 0.0118 ms 97.2%
  triton_mm_144 0.0119 ms 96.6%
  triton_mm_147 0.0121 ms 94.6%
  triton_mm_143 0.0130 ms 88.3%
  triton_mm_142 0.0139 ms 82.8%
  triton_mm_141 0.0152 ms 75.6%
  addmm 0.0153 ms 75.0%
  triton_mm_140 0.0162 ms 70.7%
SingleProcess AUTOTUNE takes 4.6395 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 259.36it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 258.89it/s]
4.583x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/functorch_maml_omniglot/__init__.py", line 73, in __init__
    self.meta_inputs = torch.load(f'{root}/maml_omniglot/batch.pt')
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/cdhernandez/local/pytorch/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/models/maml_omniglot/batch.pt'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/albert/modeling_albert.py", line 36, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Albert/__init__.py", line 10, in __init__
    super().__init__(name="hf_Albert", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.albert.modeling_albert because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 36, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Bart/__init__.py", line 10, in __init__
    super().__init__(name="hf_Bart", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.bart.modeling_bart because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 39, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_BigBird/__init__.py", line 10, in __init__
    super().__init__(name="hf_BigBird", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.big_bird.modeling_big_bird because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 32, in <module>
    from ...integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_DistilBert/__init__.py", line 10, in __init__
    super().__init__(name="hf_DistilBert", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.distilbert.modeling_distilbert because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel, SequenceSummary
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_GPT2/__init__.py", line 10, in __init__
    super().__init__(name="hf_GPT2", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel, SequenceSummary
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_GPT2_large/__init__.py", line 10, in __init__
    super().__init__(name="hf_GPT2_large", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 27, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Longformer/__init__.py", line 10, in __init__
    super().__init__(name="hf_Longformer", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.longformer.modeling_longformer because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 33, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Reformer/__init__.py", line 10, in __init__
    super().__init__(name="hf_Reformer", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.reformer.modeling_reformer because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_base/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5_base", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_generate/__init__.py", line 5, in __init__
    super().__init__(name="hf_T5_generate", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 189, in __init__
    super().__init__(name=name, test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_T5_large/__init__.py", line 14, in __init__
    super().__init__(name="hf_T5_large", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py", line 35, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_Whisper/__init__.py", line 12, in __init__
    super().__init__(name="hf_Whisper", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 445, in from_config
    model_class = _get_model_class(config, cls._model_mapping)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 392, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 737, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 751, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 695, in getattribute_from_module
    if hasattr(module, attr):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.whisper.modeling_whisper because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py", line 27, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/hf_clip/__init__.py", line 13, in <module>
    from transformers import CLIPProcessor, CLIPModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1205, in __getattr__
    value = getattr(module, name)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.clip.modeling_clip because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  lennard_jones                      
AUTOTUNE mm(1000x1, 1x16)
  triton_mm_6 0.0062 ms 100.0%
  triton_mm_9 0.0062 ms 100.0%
  mm 0.0062 ms 99.5%
  triton_mm_2 0.0062 ms 99.5%
  triton_mm_4 0.0064 ms 96.0%
  triton_mm_7 0.0064 ms 96.0%
  triton_mm_0 0.0069 ms 89.8%
  triton_mm_3 0.0069 ms 89.8%
  triton_mm_1 0.0069 ms 88.9%
  triton_mm_5 0.0069 ms 88.9%
SingleProcess AUTOTUNE takes 2.7830 seconds
AUTOTUNE mm(1000x16, 16x16)
  triton_mm_10 0.0064 ms 100.0%
  triton_mm_11 0.0064 ms 100.0%
  triton_mm_12 0.0064 ms 100.0%
  triton_mm_13 0.0064 ms 100.0%
  triton_mm_15 0.0064 ms 100.0%
  triton_mm_16 0.0064 ms 100.0%
  triton_mm_18 0.0064 ms 100.0%
  triton_mm_19 0.0064 ms 100.0%
  triton_mm_14 0.0065 ms 99.5%
  triton_mm_17 0.0071 ms 90.5%
SingleProcess AUTOTUNE takes 2.5159 seconds
AUTOTUNE addmm(1000x1, 1000x16, 16x1)
  triton_mm_46 0.0064 ms 100.0%
  triton_mm_49 0.0064 ms 100.0%
  triton_mm_41 0.0065 ms 99.5%
  triton_mm_43 0.0065 ms 99.5%
  triton_mm_44 0.0069 ms 92.6%
  triton_mm_48 0.0070 ms 92.2%
  triton_mm_40 0.0071 ms 90.5%
  triton_mm_45 0.0072 ms 89.5%
  triton_mm_47 0.0075 ms 86.1%
  triton_mm_42 0.0079 ms 81.0%
SingleProcess AUTOTUNE takes 2.7423 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 1336.02it/s]
1.416x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  llama                              
AUTOTUNE mm(1024x512, 512x512)
  mm 0.0120 ms 100.0%
  triton_mm_3 0.0127 ms 94.5%
  triton_mm_4 0.0128 ms 93.5%
  triton_mm_8 0.0140 ms 85.8%
  triton_mm_1 0.0149 ms 80.3%
  triton_mm_2 0.0150 ms 79.8%
  triton_mm_9 0.0171 ms 70.2%
  triton_mm_5 0.0172 ms 69.8%
  triton_mm_6 0.0173 ms 69.2%
  triton_mm_0 0.0205 ms 58.6%
SingleProcess AUTOTUNE takes 4.7379 seconds
AUTOTUNE mm(1024x512, 512x1536)
  mm 0.0166 ms 100.0%
  triton_mm_50 0.0178 ms 93.5%
  triton_mm_51 0.0184 ms 90.4%
  triton_mm_52 0.0187 ms 88.9%
  triton_mm_56 0.0217 ms 76.8%
  triton_mm_48 0.0246 ms 67.8%
  triton_mm_55 0.0272 ms 61.1%
  triton_mm_49 0.0283 ms 58.8%
  triton_mm_58 0.0297 ms 56.0%
  triton_mm_57 0.0332 ms 50.2%
SingleProcess AUTOTUNE takes 4.6840 seconds
AUTOTUNE mm(1024x1536, 1536x512)
  mm 0.0193 ms 100.0%
  triton_mm_75 0.0244 ms 79.3%
  triton_mm_76 0.0246 ms 78.5%
  triton_mm_80 0.0262 ms 73.7%
  triton_mm_73 0.0313 ms 61.7%
  triton_mm_74 0.0319 ms 60.6%
  triton_mm_77 0.0354 ms 54.6%
  triton_mm_78 0.0356 ms 54.2%
  triton_mm_72 0.0383 ms 50.5%
  triton_mm_81 0.0394 ms 49.0%
SingleProcess AUTOTUNE takes 4.3730 seconds
AUTOTUNE mm(32x512, 512x32000)
  triton_mm_673 0.0396 ms 100.0%
  triton_mm_674 0.0398 ms 99.4%
  triton_mm_678 0.0405 ms 97.6%
  triton_mm_676 0.0408 ms 97.0%
  triton_mm_677 0.0413 ms 95.9%
  triton_mm_681 0.0415 ms 95.3%
  triton_mm_675 0.0419 ms 94.5%
  triton_mm_680 0.0428 ms 92.4%
  mm 0.0440 ms 90.0%
  triton_mm_672 0.0485 ms 81.7%
SingleProcess AUTOTUNE takes 3.7263 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 64.74it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 70.38it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 71.97it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 71.03it/s]
5.469x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:llama_v2_7b_16h failed to load
Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/llama_v2_7b_16h/__init__.py", line 11, in __init__
    HuggingFaceAuthMixin.__init__(self)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 177, in __init__
    raise NotImplementedError("Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights")
NotImplementedError: Make sure to set `HUGGING_FACE_HUB_TOKEN` so you can download weights

loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/maml_omniglot/__init__.py", line 25, in <module>
    import higher
ModuleNotFoundError: No module named 'higher'
Failed to import user benchmark module dynamo, error: No module named 'higher'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  mnasnet1_0                         
AUTOTUNE addmm(32x1000, 32x1280, 1280x1000)
  bias_addmm 0.0149 ms 100.0%
  triton_mm_417 0.0158 ms 94.3%
  triton_mm_418 0.0179 ms 83.7%
  triton_mm_420 0.0181 ms 82.7%
  triton_mm_421 0.0181 ms 82.4%
  addmm 0.0194 ms 77.2%
  triton_mm_416 0.0194 ms 77.1%
  triton_mm_415 0.0217 ms 68.9%
  triton_mm_414 0.0248 ms 60.2%
  triton_mm_413 0.0265 ms 56.4%
SingleProcess AUTOTUNE takes 4.3529 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 104.97it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 116.93it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 116.97it/s]
3.406x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  mobilenet_v2                       
AUTOTUNE addmm(16x1000, 16x1280, 1280x1000)
  bias_addmm 0.0149 ms 100.0%
  triton_mm_417 0.0159 ms 93.4%
  triton_mm_418 0.0162 ms 91.9%
  triton_mm_420 0.0171 ms 87.2%
  triton_mm_416 0.0178 ms 83.6%
  triton_mm_421 0.0180 ms 82.7%
  addmm 0.0191 ms 77.8%
  triton_mm_415 0.0196 ms 76.0%
  triton_mm_414 0.0240 ms 61.9%
  triton_mm_413 0.0261 ms 57.0%
SingleProcess AUTOTUNE takes 4.0733 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 122.49it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 122.26it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 122.24it/s]
5.453x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:mobilenet_v2_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/mobilenet_v2_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  mobilenet_v3_large                 
AUTOTUNE mm(32x960, 960x1280)
  triton_mm_547 0.0132 ms 100.0%
  mm 0.0140 ms 94.1%
  triton_mm_548 0.0144 ms 91.3%
  triton_mm_550 0.0152 ms 86.7%
  triton_mm_551 0.0156 ms 84.4%
  triton_mm_546 0.0160 ms 82.0%
  triton_mm_545 0.0172 ms 76.4%
  triton_mm_544 0.0203 ms 64.9%
  triton_mm_543 0.0214 ms 61.3%
  triton_mm_542 0.0326 ms 40.4%
SingleProcess AUTOTUNE takes 4.5421 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 104.86it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 106.73it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 107.18it/s]
4.423x
loading model: 0it [00:00, ?it/s]NCCL version 2.19.3+cuda12.0
loading model: 0it [00:03, ?it/s]
cuda eval  moco                               
[rank0]:[2023-12-05 05:19:03,869] [0/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:[2023-12-05 05:19:08,884] [1/0_1] torch._dynamo.backends.distributed: [WARNING] Some buckets were extended beyond their requested parameter capacities in order to ensure each subgraph has an output node, required for fx graph partitioning. This can be the case when a subgraph would have only contained nodes performing inplace mutation, and returning no logical outputs. This should not be a problem, unless it results in too few graph partitions for optimal DDP performance.
[rank0]:[2023-12-05 05:19:08,910] [1/0_1] torch._dynamo.backends.distributed: [WARNING] DDPOptimizer extended these buckets to ensure per-subgraph output nodes:
[rank0]:[2023-12-05 05:19:08,910] [1/0_1] torch._dynamo.backends.distributed: [WARNING] ┌─────────┬─────────────┬────────────────────────┐
[rank0]:[2023-12-05 05:19:08,910] [1/0_1] torch._dynamo.backends.distributed: [WARNING] │   Index │   Extra Ops │   Extra Param Size (b) │
[rank0]:[2023-12-05 05:19:08,910] [1/0_1] torch._dynamo.backends.distributed: [WARNING] ├─────────┼─────────────┼────────────────────────┤
[rank0]:[2023-12-05 05:19:08,910] [1/0_1] torch._dynamo.backends.distributed: [WARNING] │       0 │         157 │               44910720 │
[rank0]:[2023-12-05 05:19:08,910] [1/0_1] torch._dynamo.backends.distributed: [WARNING] └─────────┴─────────────┴────────────────────────┘
AUTOTUNE addmm(32x128, 32x2048, 2048x128)
  bias_addmm 0.0132 ms 100.0%
  addmm 0.0166 ms 79.0%
  triton_mm_540 0.0192 ms 68.4%
  triton_mm_541 0.0216 ms 61.0%
  triton_mm_543 0.0223 ms 59.1%
  triton_mm_544 0.0224 ms 58.8%
  triton_mm_539 0.0245 ms 53.8%
  triton_mm_538 0.0268 ms 49.1%
  triton_mm_537 0.0327 ms 40.3%
  triton_mm_536 0.0350 ms 37.6%
SingleProcess AUTOTUNE takes 4.0835 seconds
skipping cudagraphs due to ['mutated inputs']
[rank0]:[2023-12-05 05:19:45,127] [5/0_1] torch._inductor.utils: [WARNING] DeviceCopy in input program
skipping cudagraphs due to ['non-cuda device in graph']
AUTOTUNE mm(32x2048, 2048x128)
  mm 0.0127 ms 100.0%
  triton_mm_1087 0.0190 ms 66.7%
  triton_mm_1088 0.0208 ms 61.2%
  triton_mm_1090 0.0216 ms 58.9%
  triton_mm_1091 0.0222 ms 57.1%
  triton_mm_1086 0.0237 ms 53.5%
  triton_mm_1085 0.0267 ms 47.6%
  triton_mm_1084 0.0320 ms 39.7%
  triton_mm_1083 0.0346 ms 36.8%
  triton_mm_1082 0.0423 ms 30.0%
SingleProcess AUTOTUNE takes 3.8070 seconds
AUTOTUNE bmm(32x1x128, 32x128x1)
  triton_bmm_1097 0.0063 ms 100.0%
  triton_bmm_1099 0.0063 ms 100.0%
  bmm 0.0066 ms 96.1%
  triton_bmm_1096 0.0070 ms 90.2%
  triton_bmm_1098 0.0071 ms 89.4%
  triton_bmm_1095 0.0076 ms 83.9%
  triton_bmm_1094 0.0079 ms 80.5%
  triton_bmm_1100 0.0086 ms 73.6%
  triton_bmm_1101 0.0088 ms 71.7%
SingleProcess AUTOTUNE takes 2.3468 seconds
AUTOTUNE bmm(1x32x128, 1x128x32000)
  triton_bmm_1104 0.0146 ms 100.0%
  triton_bmm_1102 0.0146 ms 99.8%
  triton_bmm_1109 0.0148 ms 98.5%
  triton_bmm_1103 0.0153 ms 95.4%
  triton_bmm_1108 0.0156 ms 93.2%
  triton_bmm_1112 0.0160 ms 90.9%
  triton_bmm_1105 0.0163 ms 89.6%
  triton_bmm_1106 0.0164 ms 89.0%
  triton_bmm_1113 0.0171 ms 85.4%
  bmm 0.0171 ms 85.0%
SingleProcess AUTOTUNE takes 3.8570 seconds
[rank0]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:00, 30.52it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 30.81it/s]running benchmark:  40%|████      | 12/30 [00:00<00:00, 30.70it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 30.80it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 30.92it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 31.04it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 30.98it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 30.91it/s]
2.131x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
number of parameters: 123.69M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: True
cuda eval  nanogpt                            
AUTOTUNE addmm(64x2304, 64x768, 768x2304)
  bias_addmm 0.0133 ms 100.0%
  triton_mm_6 0.0140 ms 95.4%
  triton_mm_5 0.0146 ms 91.6%
  triton_mm_8 0.0150 ms 88.9%
  triton_mm_9 0.0152 ms 88.1%
  triton_mm_4 0.0160 ms 83.6%
  triton_mm_3 0.0172 ms 77.8%
  addmm 0.0177 ms 75.4%
  triton_mm_2 0.0195 ms 68.5%
  triton_mm_1 0.0209 ms 63.8%
SingleProcess AUTOTUNE takes 5.4540 seconds
AUTOTUNE mm(64x768, 768x768)
  mm 0.0121 ms 100.0%
  triton_mm_17 0.0130 ms 93.1%
  triton_mm_21 0.0131 ms 92.9%
  triton_mm_18 0.0132 ms 92.2%
  triton_mm_16 0.0141 ms 85.7%
  triton_mm_20 0.0141 ms 85.7%
  triton_mm_15 0.0162 ms 75.0%
  triton_mm_14 0.0180 ms 67.6%
  triton_mm_13 0.0196 ms 61.9%
  triton_mm_12 0.0280 ms 43.4%
SingleProcess AUTOTUNE takes 5.5059 seconds
AUTOTUNE mm(64x768, 768x3072)
  mm 0.0132 ms 100.0%
  triton_mm_30 0.0140 ms 94.1%
  triton_mm_33 0.0150 ms 87.7%
  triton_mm_29 0.0151 ms 87.5%
  triton_mm_32 0.0152 ms 86.7%
  triton_mm_28 0.0157 ms 83.9%
  triton_mm_27 0.0170 ms 77.7%
  triton_mm_26 0.0189 ms 69.6%
  triton_mm_25 0.0202 ms 65.2%
  triton_mm_24 0.0297 ms 44.3%
SingleProcess AUTOTUNE takes 4.5597 seconds
AUTOTUNE mm(64x3072, 3072x768)
  mm 0.0164 ms 100.0%
  triton_mm_42 0.0298 ms 55.2%
  triton_mm_41 0.0300 ms 54.7%
  triton_mm_45 0.0312 ms 52.6%
  triton_mm_44 0.0337 ms 48.7%
  triton_mm_40 0.0360 ms 45.6%
  triton_mm_39 0.0409 ms 40.1%
  triton_mm_38 0.0504 ms 32.6%
  triton_mm_37 0.0552 ms 29.7%
  triton_mm_36 0.0797 ms 20.6%
SingleProcess AUTOTUNE takes 4.2263 seconds
AUTOTUNE mm(1x768, 768x50304)
  triton_mm_582 0.0693 ms 100.0%
  triton_mm_577 0.0699 ms 99.2%
  triton_mm_579 0.0704 ms 98.5%
  triton_mm_580 0.0709 ms 97.8%
  triton_mm_578 0.0722 ms 96.1%
  mm 0.0728 ms 95.2%
  triton_mm_584 0.0893 ms 77.6%
  triton_mm_576 0.0898 ms 77.2%
  triton_mm_583 0.0906 ms 76.6%
  triton_mm_586 0.0953 ms 72.8%
SingleProcess AUTOTUNE takes 3.6400 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 129.90it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 132.18it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 132.05it/s]
6.711x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
cuda eval  nvidia_deeprecommender             
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 200.67it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 204.20it/s]
0.921x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/opacus_cifar10/__init__.py", line 6, in <module>
    from opacus import PrivacyEngine
ModuleNotFoundError: No module named 'opacus'
Failed to import user benchmark module dynamo, error: No module named 'opacus'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py", line 27, in <module>
    from ..integrations.deepspeed import is_deepspeed_zero3_enabled
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/__init__.py", line 14, in <module>
    from .bitsandbytes import (
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 18, in <module>
    from accelerate import init_empty_weights
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py", line 41, in <module>
    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/accelerate/tracking.py", line 43, in <module>
    from torch.utils import tensorboard
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1214, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 39, in <module>
    from .generation import GenerationConfig, GenerationMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/phi_1_5/__init__.py", line 11, in __init__
    super().__init__(name="phi_1_5", test=test, device=device, batch_size=batch_size, extra_args=extra_args)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 93, in __init__
    self.model = class_ctor.from_config(config, **kwargs).to(device)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 437, in from_config
    model_class = get_class_from_dynamic_module(class_ref, repo_id, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 497, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module.replace(".py", ""))
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/dynamic_module_utils.py", line 199, in get_class_in_module
    module = importlib.import_module(module_path)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/5fd430c7bcd28140560faee2014d1228338e19a0/modeling_phi.py", line 16, in <module>
    from transformers import PretrainedConfig, PreTrainedModel
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1204, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1216, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  phlippe_densenet                   
AUTOTUNE addmm(128x10, 128x184, 184x10)
  triton_mm_482 0.0076 ms 100.0%
  triton_mm_479 0.0077 ms 98.3%
  triton_mm_483 0.0078 ms 97.1%
  triton_mm_480 0.0079 ms 96.3%
  triton_mm_475 0.0083 ms 91.2%
  triton_mm_477 0.0084 ms 89.8%
  triton_mm_478 0.0090 ms 84.6%
  triton_mm_476 0.0093 ms 81.4%
  bias_addmm 0.0093 ms 81.2%
  triton_mm_474 0.0106 ms 71.8%
SingleProcess AUTOTUNE takes 3.9888 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 83.27it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 93.38it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 96.19it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 94.32it/s]
5.190x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
cuda eval  phlippe_resnet                     
AUTOTUNE addmm(128x10, 128x64, 64x10)
  triton_mm_137 0.0066 ms 100.0%
  triton_mm_134 0.0068 ms 96.7%
  triton_mm_139 0.0072 ms 92.2%
  triton_mm_133 0.0073 ms 90.4%
  triton_mm_140 0.0074 ms 89.2%
  triton_mm_136 0.0074 ms 89.0%
  triton_mm_132 0.0075 ms 88.2%
  triton_mm_131 0.0075 ms 88.0%
  triton_mm_135 0.0078 ms 84.1%
  triton_mm_138 0.0079 ms 83.1%
SingleProcess AUTOTUNE takes 4.3297 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 251.35it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 250.56it/s]
4.329x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
cuda eval  pyhpc_equation_of_state            
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 187.81it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 195.73it/s]
17.291x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pyhpc_isoneutral_mixing            
skipping cudagraphs due to ['mutated inputs']
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 88.85it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 88.83it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 87.36it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 87.64it/s]
6.083x
loading model: 0it [00:00, ?it/s]WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
loading model: 0it [00:02, ?it/s]
WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
cuda eval  pyhpc_turbulent_kinetic_energy     
WARNING:common:Model pyhpc_turbulent_kinetic_energy does not support bfloat16, running with amp instead
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 74.99it/s]running benchmark:  57%|█████▋    | 17/30 [00:00<00:00, 80.14it/s]running benchmark:  87%|████████▋ | 26/30 [00:00<00:00, 80.47it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 79.99it/s]
4.545x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/__init__.py", line 13, in <module>
    from .train_cyclegan import prepare_training_loop
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/train_cyclegan.py", line 27, in <module>
    from .util.visualizer import Visualizer
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/util/visualizer.py", line 6, in <module>
    from . import util, html
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_CycleGAN_and_pix2pix/util/html.py", line 1, in <module>
    import dominate
ModuleNotFoundError: No module named 'dominate'
Failed to import user benchmark module dynamo, error: No module named 'dominate'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/__init__.py", line 52, in __init__
    self.data_loader = self.get_data_loader(config)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/__init__.py", line 68, in get_data_loader
    celeba_loader = get_loader(config.celeba_image_dir, config.attr_path, config.selected_attrs,
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 82, in get_loader
    dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 24, in __init__
    self.preprocess()
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/pytorch_stargan/data_loader.py", line 33, in preprocess
    lines = [line.rstrip() for line in open(self.attr_path, 'r')]
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/data/.data/pytorch_stargan_inputs/data/celeba/list_attr_celeba.txt'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  pytorch_unet                       
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  13%|█▎        | 4/30 [00:00<00:00, 34.17it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 41.18it/s]running benchmark:  47%|████▋     | 14/30 [00:00<00:00, 43.57it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 44.72it/s]running benchmark:  80%|████████  | 24/30 [00:00<00:00, 45.34it/s]running benchmark:  97%|█████████▋| 29/30 [00:00<00:00, 45.73it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 44.36it/s]
1.816x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:04, ?it/s]
cuda eval  resnet152                          
AUTOTUNE addmm(32x1000, 32x2048, 2048x1000)
  bias_addmm 0.0160 ms 100.0%
  addmm 0.0195 ms 82.1%
  triton_mm_1594 0.0206 ms 77.4%
  triton_mm_1595 0.0234 ms 68.2%
  triton_mm_1598 0.0235 ms 67.9%
  triton_mm_1597 0.0236 ms 67.5%
  triton_mm_1593 0.0259 ms 61.8%
  triton_mm_1592 0.0286 ms 55.8%
  triton_mm_1591 0.0345 ms 46.3%
  triton_mm_1590 0.0372 ms 42.9%
SingleProcess AUTOTUNE takes 4.2934 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:00, 27.53it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 31.75it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 33.15it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 33.65it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 34.00it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 33.90it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 33.12it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 32.94it/s]
2.504x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  resnet18                           
AUTOTUNE addmm(8x1000, 8x512, 512x1000)
  triton_mm_144 0.0102 ms 100.0%
  triton_mm_145 0.0103 ms 99.7%
  triton_mm_148 0.0107 ms 95.8%
  bias_addmm 0.0107 ms 95.5%
  triton_mm_143 0.0112 ms 91.2%
  triton_mm_142 0.0122 ms 83.7%
  triton_mm_141 0.0133 ms 77.1%
  triton_mm_140 0.0140 ms 73.2%
  addmm 0.0163 ms 62.7%
  triton_mm_139 0.0198 ms 51.7%
SingleProcess AUTOTUNE takes 4.2397 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  83%|████████▎ | 25/30 [00:00<00:00, 245.73it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 246.23it/s]
3.693x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  resnet50                           
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 69.34it/s]running benchmark:  53%|█████▎    | 16/30 [00:00<00:00, 78.80it/s]running benchmark:  83%|████████▎ | 25/30 [00:00<00:00, 81.74it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 80.93it/s]
1.897x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
WARNING:root:resnet50_quantized_qat failed to load
The eval test only supports CPU.
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/resnet50_quantized_qat/__init__.py", line 21, in __init__
    raise NotImplementedError("The eval test only supports CPU.")
NotImplementedError: The eval test only supports CPU.

loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  resnext50_32x4d                    
AUTOTUNE addmm(8x1000, 8x2048, 2048x1000)
  bias_addmm 0.0177 ms 100.0%
  triton_mm_428 0.0208 ms 84.9%
  triton_mm_429 0.0216 ms 81.8%
  triton_mm_431 0.0217 ms 81.3%
  triton_mm_432 0.0228 ms 77.3%
  triton_mm_427 0.0233 ms 75.7%
  triton_mm_426 0.0265 ms 66.6%
  addmm 0.0300 ms 58.8%
  triton_mm_425 0.0334 ms 52.9%
  triton_mm_424 0.0359 ms 49.2%
SingleProcess AUTOTUNE takes 4.4346 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:00, 93.85it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 102.59it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 102.32it/s]
4.099x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 432, in load_model
    benchmark = benchmark_cls(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/util/model.py", line 24, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/__init__.py", line 27, in __init__
    self.model = sam_model_registry[model_type](checkpoint=sam_checkpoint)
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/build_sam.py", line 19, in build_sam_vit_h
    return _build_sam(
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/build_sam.py", line 108, in _build_sam
    with open(checkpoint, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/cdhernandez/local/benchmark/torchbenchmark/models/sam/.data/sam_vit_h_4b8939.pth'
Run failed with return code:  1
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  shufflenet_v2_x1_0                 
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:00, 99.94it/s]running benchmark:  70%|███████   | 21/30 [00:00<00:00, 102.13it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 101.91it/s]
3.639x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/soft_actor_critic/__init__.py", line 13, in <module>
    from .envs import load_gym
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/soft_actor_critic/envs.py", line 6, in <module>
    import gym
ModuleNotFoundError: No module named 'gym'
Failed to import user benchmark module dynamo, error: No module named 'gym'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/speech_transformer/__init__.py", line 15, in <module>
    from .config import SpeechTransformerTrainConfig, SpeechTransformerEvalConfig
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/speech_transformer/config.py", line 4, in <module>
    import kaldi_io
ModuleNotFoundError: No module named 'kaldi_io'
Failed to import user benchmark module dynamo, error: No module named 'kaldi_io'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  squeezenet1_1                      
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 274.95it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 277.24it/s]
3.478x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/stable_diffusion_text_encoder/__init__.py", line 11, in <module>
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
ModuleNotFoundError: No module named 'diffusers'
Failed to import user benchmark module dynamo, error: No module named 'diffusers'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/stable_diffusion_unet/__init__.py", line 11, in <module>
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
ModuleNotFoundError: No module named 'diffusers'
Failed to import user benchmark module dynamo, error: No module named 'diffusers'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/timm_efficientdet/__init__.py", line 12, in <module>
    from effdet import create_model, create_loader
ModuleNotFoundError: No module named 'effdet'
Failed to import user benchmark module dynamo, error: No module named 'effdet'
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_efficientnet                  
AUTOTUNE addmm(64x1000, 64x1280, 1280x1000)
  bias_addmm 0.0146 ms 100.0%
  triton_mm_730 0.0175 ms 83.6%
  triton_mm_731 0.0177 ms 82.4%
  triton_mm_734 0.0181 ms 80.7%
  addmm 0.0181 ms 80.7%
  triton_mm_733 0.0196 ms 74.3%
  triton_mm_729 0.0202 ms 72.3%
  triton_mm_728 0.0231 ms 63.4%
  triton_mm_727 0.0260 ms 56.2%
  triton_mm_726 0.0287 ms 50.9%
SingleProcess AUTOTUNE takes 5.1153 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  20%|██        | 6/30 [00:00<00:00, 52.93it/s]running benchmark:  43%|████▎     | 13/30 [00:00<00:00, 58.07it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 59.73it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 60.51it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 59.69it/s]
2.276x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:05, ?it/s]
cuda eval  timm_nfnet                         
AUTOTUNE addmm(128x1000, 128x3072, 3072x1000)
  bias_addmm 0.0204 ms 100.0%
  addmm 0.0246 ms 82.7%
  triton_mm_416 0.0306 ms 66.7%
  triton_mm_415 0.0307 ms 66.5%
  triton_mm_419 0.0338 ms 60.3%
  triton_mm_418 0.0349 ms 58.4%
  triton_mm_413 0.0423 ms 48.2%
  triton_mm_414 0.0423 ms 48.1%
  triton_mm_412 0.0569 ms 35.8%
  triton_mm_411 0.0574 ms 35.5%
SingleProcess AUTOTUNE takes 5.5090 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:07,  3.95it/s]running benchmark:  10%|█         | 3/30 [00:00<00:03,  8.81it/s]running benchmark:  17%|█▋        | 5/30 [00:00<00:02, 11.28it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:01, 12.74it/s]running benchmark:  30%|███       | 9/30 [00:00<00:01, 13.62it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:01, 14.19it/s]running benchmark:  43%|████▎     | 13/30 [00:01<00:01, 14.58it/s]running benchmark:  50%|█████     | 15/30 [00:01<00:01, 14.81it/s]running benchmark:  57%|█████▋    | 17/30 [00:01<00:00, 14.99it/s]running benchmark:  63%|██████▎   | 19/30 [00:01<00:00, 15.10it/s]running benchmark:  70%|███████   | 21/30 [00:01<00:00, 15.17it/s]running benchmark:  77%|███████▋  | 23/30 [00:01<00:00, 15.23it/s]running benchmark:  83%|████████▎ | 25/30 [00:01<00:00, 15.25it/s]running benchmark:  90%|█████████ | 27/30 [00:01<00:00, 15.31it/s]running benchmark:  97%|█████████▋| 29/30 [00:02<00:00, 15.32it/s]running benchmark: 100%|██████████| 30/30 [00:02<00:00, 14.02it/s]
1.868x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  timm_regnet                        
AUTOTUNE addmm(32x1000, 32x2240, 2240x1000)
  bias_addmm 0.0164 ms 100.0%
  addmm 0.0203 ms 80.9%
  triton_mm_555 0.0227 ms 72.5%
  triton_mm_556 0.0245 ms 66.9%
  triton_mm_558 0.0256 ms 64.2%
  triton_mm_559 0.0261 ms 62.8%
  triton_mm_554 0.0274 ms 60.0%
  triton_mm_553 0.0315 ms 52.1%
  triton_mm_552 0.0374 ms 43.9%
  triton_mm_551 0.0404 ms 40.6%
SingleProcess AUTOTUNE takes 4.4907 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  10%|█         | 3/30 [00:00<00:01, 24.91it/s]running benchmark:  23%|██▎       | 7/30 [00:00<00:00, 30.48it/s]running benchmark:  37%|███▋      | 11/30 [00:00<00:00, 32.31it/s]running benchmark:  50%|█████     | 15/30 [00:00<00:00, 33.17it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 33.67it/s]running benchmark:  77%|███████▋  | 23/30 [00:00<00:00, 33.96it/s]running benchmark:  90%|█████████ | 27/30 [00:00<00:00, 34.14it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 33.23it/s]
1.379x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_resnest                       
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  30%|███       | 9/30 [00:00<00:00, 87.97it/s]running benchmark:  67%|██████▋   | 20/30 [00:00<00:00, 100.21it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 101.60it/s]
1.710x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:01, ?it/s]
cuda eval  timm_vision_transformer            
AUTOTUNE addmm(6304x1152, 6304x384, 384x1152)
  triton_mm_8 0.0462 ms 100.0%
  triton_mm_9 0.0462 ms 100.0%
  triton_mm_11 0.0536 ms 86.1%
  bias_addmm 0.0538 ms 85.8%
  triton_mm_10 0.0539 ms 85.7%
  triton_mm_7 0.0542 ms 85.2%
  triton_mm_15 0.0589 ms 78.3%
  triton_mm_14 0.0616 ms 75.0%
  addmm 0.0640 ms 72.1%
  triton_mm_17 0.0827 ms 55.9%
SingleProcess AUTOTUNE takes 5.2914 seconds
AUTOTUNE mm(6304x384, 384x384)
  triton_mm_21 0.0201 ms 100.0%
  triton_mm_20 0.0209 ms 96.4%
  mm 0.0215 ms 93.4%
  triton_mm_22 0.0225 ms 89.2%
  triton_mm_23 0.0226 ms 89.0%
  triton_mm_19 0.0232 ms 86.8%
  triton_mm_27 0.0251 ms 80.2%
  triton_mm_26 0.0264 ms 76.3%
  triton_mm_29 0.0353 ms 57.0%
  triton_mm_24 0.0373 ms 53.9%
SingleProcess AUTOTUNE takes 4.9182 seconds
AUTOTUNE mm(6304x384, 384x1536)
  mm 0.0509 ms 100.0%
  triton_mm_32 0.0529 ms 96.2%
  triton_mm_33 0.0530 ms 95.9%
  triton_mm_34 0.0615 ms 82.7%
  triton_mm_38 0.0620 ms 82.0%
  triton_mm_35 0.0638 ms 79.7%
  triton_mm_31 0.0650 ms 78.3%
  triton_mm_39 0.0704 ms 72.2%
  triton_mm_41 0.1004 ms 50.6%
  triton_mm_37 0.1256 ms 40.5%
SingleProcess AUTOTUNE takes 4.5908 seconds
AUTOTUNE mm(6304x1536, 1536x384)
  mm 0.0484 ms 100.0%
  triton_mm_45 0.0520 ms 93.0%
  triton_mm_44 0.0523 ms 92.5%
  triton_mm_46 0.0573 ms 84.5%
  triton_mm_47 0.0579 ms 83.6%
  triton_mm_51 0.0660 ms 73.4%
  triton_mm_43 0.0853 ms 56.7%
  triton_mm_50 0.1042 ms 46.4%
  triton_mm_49 0.1092 ms 44.3%
  triton_mm_48 0.1094 ms 44.2%
SingleProcess AUTOTUNE takes 4.6202 seconds
AUTOTUNE addmm(32x1000, 32x384, 384x1000)
  triton_mm_592 0.0099 ms 100.0%
  triton_mm_588 0.0101 ms 97.5%
  triton_mm_589 0.0104 ms 95.4%
  triton_mm_587 0.0107 ms 92.8%
  triton_mm_591 0.0107 ms 92.7%
  bias_addmm 0.0112 ms 88.3%
  triton_mm_586 0.0120 ms 82.6%
  triton_mm_585 0.0126 ms 78.4%
  triton_mm_584 0.0133 ms 74.5%
  addmm 0.0149 ms 66.3%
SingleProcess AUTOTUNE takes 4.3673 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  33%|███▎      | 10/30 [00:00<00:00, 96.34it/s]running benchmark:  73%|███████▎  | 22/30 [00:00<00:00, 109.29it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 110.14it/s]
1.467x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:20, ?it/s]
cuda eval  timm_vision_transformer_large      
AUTOTUNE addmm(8224x4224, 8224x1408, 1408x4224)
  bias_addmm 0.4392 ms 100.0%
  triton_mm_8 0.5538 ms 79.3%
  triton_mm_9 0.5655 ms 77.7%
  addmm 0.5923 ms 74.1%
  triton_mm_10 0.6344 ms 69.2%
  triton_mm_11 0.6550 ms 67.0%
  triton_mm_7 0.7293 ms 60.2%
  triton_mm_15 0.7602 ms 57.8%
  triton_mm_14 0.8804 ms 49.9%
  triton_mm_17 1.1375 ms 38.6%
SingleProcess AUTOTUNE takes 5.3551 seconds
AUTOTUNE mm(8224x1408, 1408x1408)
  mm 0.1637 ms 100.0%
  triton_mm_20 0.1868 ms 87.7%
  triton_mm_21 0.1885 ms 86.9%
  triton_mm_22 0.2123 ms 77.1%
  triton_mm_23 0.2134 ms 76.7%
  triton_mm_27 0.2475 ms 66.2%
  triton_mm_19 0.2506 ms 65.3%
  triton_mm_26 0.2744 ms 59.7%
  triton_mm_29 0.4347 ms 37.7%
  triton_mm_24 0.4493 ms 36.4%
SingleProcess AUTOTUNE takes 5.0067 seconds
AUTOTUNE mm(8224x1408, 1408x6144)
  mm 0.6470 ms 100.0%
  triton_mm_32 0.7786 ms 83.1%
  triton_mm_33 0.8209 ms 78.8%
  triton_mm_35 0.9277 ms 69.7%
  triton_mm_34 0.9437 ms 68.6%
  triton_mm_31 1.1029 ms 58.7%
  triton_mm_39 1.1139 ms 58.1%
  triton_mm_38 1.2511 ms 51.7%
  triton_mm_41 1.6865 ms 38.4%
  triton_mm_37 1.9941 ms 32.4%
SingleProcess AUTOTUNE takes 5.1780 seconds
AUTOTUNE mm(8224x6144, 6144x1408)
  mm 0.6610 ms 100.0%
  triton_mm_44 0.7933 ms 83.3%
  triton_mm_45 0.8030 ms 82.3%
  triton_mm_46 0.9296 ms 71.1%
  triton_mm_47 0.9328 ms 70.9%
  triton_mm_51 1.0764 ms 61.4%
  triton_mm_43 1.0874 ms 60.8%
  triton_mm_50 1.2701 ms 52.0%
  triton_mm_53 1.7459 ms 37.9%
  triton_mm_48 1.9421 ms 34.0%
SingleProcess AUTOTUNE takes 4.9285 seconds
AUTOTUNE addmm(32x1000, 32x1408, 1408x1000)
  bias_addmm 0.0156 ms 100.0%
  triton_mm_1932 0.0167 ms 93.1%
  triton_mm_1933 0.0185 ms 84.1%
  triton_mm_1935 0.0187 ms 83.2%
  triton_mm_1936 0.0188 ms 82.8%
  addmm 0.0200 ms 77.6%
  triton_mm_1931 0.0201 ms 77.4%
  triton_mm_1930 0.0224 ms 69.4%
  triton_mm_1929 0.0269 ms 57.9%
  triton_mm_1928 0.0288 ms 53.9%
SingleProcess AUTOTUNE takes 4.1762 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:   3%|▎         | 1/30 [00:00<00:14,  1.96it/s]running benchmark:   7%|▋         | 2/30 [00:00<00:09,  3.03it/s]running benchmark:  10%|█         | 3/30 [00:00<00:07,  3.65it/s]running benchmark:  13%|█▎        | 4/30 [00:01<00:06,  4.05it/s]running benchmark:  17%|█▋        | 5/30 [00:01<00:05,  4.32it/s]running benchmark:  20%|██        | 6/30 [00:01<00:05,  4.50it/s]running benchmark:  23%|██▎       | 7/30 [00:01<00:04,  4.62it/s]running benchmark:  27%|██▋       | 8/30 [00:01<00:04,  4.70it/s]running benchmark:  30%|███       | 9/30 [00:02<00:04,  4.75it/s]running benchmark:  33%|███▎      | 10/30 [00:02<00:04,  4.78it/s]running benchmark:  37%|███▋      | 11/30 [00:02<00:03,  4.81it/s]running benchmark:  40%|████      | 12/30 [00:02<00:03,  4.83it/s]running benchmark:  43%|████▎     | 13/30 [00:02<00:03,  4.84it/s]running benchmark:  47%|████▋     | 14/30 [00:03<00:03,  4.85it/s]running benchmark:  50%|█████     | 15/30 [00:03<00:03,  4.86it/s]running benchmark:  53%|█████▎    | 16/30 [00:03<00:02,  4.86it/s]running benchmark:  57%|█████▋    | 17/30 [00:03<00:02,  4.86it/s]running benchmark:  60%|██████    | 18/30 [00:03<00:02,  4.86it/s]running benchmark:  63%|██████▎   | 19/30 [00:04<00:02,  4.86it/s]running benchmark:  67%|██████▋   | 20/30 [00:04<00:02,  4.87it/s]running benchmark:  70%|███████   | 21/30 [00:04<00:01,  4.86it/s]running benchmark:  73%|███████▎  | 22/30 [00:04<00:01,  4.86it/s]running benchmark:  77%|███████▋  | 23/30 [00:05<00:01,  4.86it/s]running benchmark:  80%|████████  | 24/30 [00:05<00:01,  4.85it/s]running benchmark:  83%|████████▎ | 25/30 [00:05<00:01,  4.85it/s]running benchmark:  87%|████████▋ | 26/30 [00:05<00:00,  4.85it/s]running benchmark:  90%|█████████ | 27/30 [00:05<00:00,  4.85it/s]running benchmark:  93%|█████████▎| 28/30 [00:06<00:00,  4.86it/s]running benchmark:  97%|█████████▋| 29/30 [00:06<00:00,  4.86it/s]running benchmark: 100%|██████████| 30/30 [00:06<00:00,  4.86it/s]running benchmark: 100%|██████████| 30/30 [00:06<00:00,  4.64it/s]
0.993x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:02, ?it/s]
cuda eval  timm_vovnet                        
AUTOTUNE addmm(32x1000, 32x1024, 1024x1000)
  triton_mm_307 0.0138 ms 100.0%
  bias_addmm 0.0142 ms 97.3%
  triton_mm_308 0.0152 ms 90.9%
  triton_mm_310 0.0152 ms 90.8%
  triton_mm_311 0.0155 ms 89.3%
  triton_mm_306 0.0165 ms 83.7%
  triton_mm_305 0.0181 ms 76.2%
  addmm 0.0187 ms 73.8%
  triton_mm_304 0.0209 ms 66.1%
  triton_mm_303 0.0228 ms 60.8%
SingleProcess AUTOTUNE takes 4.9091 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  27%|██▋       | 8/30 [00:00<00:00, 75.09it/s]running benchmark:  60%|██████    | 18/30 [00:00<00:00, 84.13it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 86.96it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 85.60it/s]
1.796x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/torch_multimodal_clip/__init__.py", line 10, in <module>
    from torchmultimodal.transforms.clip_transform import CLIPTextTransform, CLIPImageTransform
ModuleNotFoundError: No module named 'torchmultimodal'
Failed to import user benchmark module dynamo, error: No module named 'torchmultimodal'
loading model: 0it [00:00, ?it/s]WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
loading model: 0it [00:01, ?it/s]
WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
cuda eval  tts_angular                        
WARNING:common:Model tts_angular does not support bfloat16, running with amp instead
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  63%|██████▎   | 19/30 [00:00<00:00, 180.19it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 180.41it/s]
0.973x
loading model: 0it [00:00, ?it/s]loading model: 0it [00:03, ?it/s]
cuda eval  vgg16                              
AUTOTUNE mm(4x25088, 25088x4096)
  mm 0.1540 ms 100.0%
  triton_mm_95 0.1955 ms 78.8%
  triton_mm_96 0.2012 ms 76.6%
  triton_mm_98 0.2091 ms 73.7%
  triton_mm_99 0.2093 ms 73.6%
  triton_mm_94 0.2169 ms 71.0%
  triton_mm_93 0.2510 ms 61.4%
  triton_mm_92 0.3391 ms 45.4%
  triton_mm_91 0.3741 ms 41.2%
  triton_mm_90 0.5046 ms 30.5%
SingleProcess AUTOTUNE takes 4.2173 seconds
AUTOTUNE mm(4x4096, 4096x4096)
  mm 0.0428 ms 100.0%
  triton_mm_110 0.0474 ms 90.3%
  triton_mm_111 0.0478 ms 89.6%
  triton_mm_108 0.0481 ms 88.9%
  triton_mm_107 0.0484 ms 88.5%
  triton_mm_106 0.0493 ms 86.7%
  triton_mm_105 0.0529 ms 80.9%
  triton_mm_104 0.0683 ms 62.7%
  triton_mm_103 0.0734 ms 58.3%
  triton_mm_102 0.0907 ms 47.2%
SingleProcess AUTOTUNE takes 3.6738 seconds
AUTOTUNE addmm(4x1000, 4x4096, 4096x1000)
  bias_addmm 0.0194 ms 100.0%
  addmm 0.0229 ms 84.4%
  triton_mm_119 0.0336 ms 57.6%
  triton_mm_120 0.0350 ms 55.3%
  triton_mm_122 0.0364 ms 53.2%
  triton_mm_123 0.0372 ms 52.1%
  triton_mm_118 0.0387 ms 50.0%
  triton_mm_117 0.0456 ms 42.5%
  triton_mm_116 0.0595 ms 32.5%
  triton_mm_115 0.0649 ms 29.8%
SingleProcess AUTOTUNE takes 4.2116 seconds
running benchmark:   0%|          | 0/30 [00:00<?, ?it/s]running benchmark:  93%|█████████▎| 28/30 [00:00<00:00, 277.90it/s]running benchmark: 100%|██████████| 30/30 [00:00<00:00, 278.75it/s]
1.577x
loading model: 0it [00:00, ?it/s]WARNING:common:Model vision_maskrcnn does not support bfloat16, running with float16 instead
loading model: 0it [00:03, ?it/s]
WARNING:common:Model vision_maskrcnn does not support bfloat16, running with float16 instead
cuda eval  vision_maskrcnn                    
WARNING:common:Model vision_maskrcnn does not support bfloat16, running with float16 instead
AUTOTUNE mm(60800x256, 256x128)
  triton_mm_112 0.0474 ms 100.0%
  triton_mm_113 0.0492 ms 96.2%
  mm 0.0508 ms 93.4%
  triton_mm_114 0.0523 ms 90.6%
  triton_mm_115 0.0532 ms 89.0%
  triton_mm_111 0.0536 ms 88.4%
  triton_mm_119 0.0540 ms 87.7%
  triton_mm_118 0.0541 ms 87.5%
  triton_mm_121 0.0700 ms 67.7%
  triton_mm_116 0.0776 ms 61.1%
SingleProcess AUTOTUNE takes 4.5625 seconds
AUTOTUNE convolution(1x128x200x304, 128x128x3x3)
  convolution 0.0464 ms 100.0%
  triton_convolution_129 0.3074 ms 15.1%
  triton_convolution_128 0.3427 ms 13.5%
  triton_convolution_124 0.3775 ms 12.3%
  triton_convolution_123 0.3818 ms 12.2%
  triton_convolution_126 0.3993 ms 11.6%
  triton_convolution_127 0.4225 ms 11.0%
  triton_convolution_125 0.9988 ms 4.6%
SingleProcess AUTOTUNE takes 3.9619 seconds
AUTOTUNE mm(15200x512, 512x128)
  triton_mm_151 0.0272 ms 100.0%
  triton_mm_150 0.0291 ms 93.4%
  triton_mm_152 0.0298 ms 91.4%
  triton_mm_153 0.0313 ms 86.8%
  triton_mm_149 0.0318 ms 85.5%
  triton_mm_157 0.0318 ms 85.4%
  mm 0.0330 ms 82.4%
  triton_mm_156 0.0343 ms 79.2%
  triton_mm_154 0.0384 ms 70.8%
  triton_mm_155 0.0417 ms 65.3%
SingleProcess AUTOTUNE takes 4.8191 seconds
AUTOTUNE mm(15200x512, 512x256)
  triton_mm_244 0.0354 ms 100.0%
  mm 0.0362 ms 97.8%
  triton_mm_243 0.0370 ms 95.8%
  triton_mm_246 0.0389 ms 90.9%
  triton_mm_245 0.0397 ms 89.2%
  triton_mm_250 0.0421 ms 84.1%
  triton_mm_242 0.0470 ms 75.2%
  triton_mm_249 0.0536 ms 66.0%
  triton_mm_248 0.0672 ms 52.7%
  triton_mm_247 0.0684 ms 51.7%
SingleProcess AUTOTUNE takes 4.8331 seconds
AUTOTUNE convolution(1x256x100x152, 256x256x3x3)
  convolution 0.0397 ms 100.0%
  triton_convolution_259 0.3091 ms 12.8%
  triton_convolution_260 0.3503 ms 11.3%
  triton_convolution_257 0.4398 ms 9.0%
  triton_convolution_254 0.5968 ms 6.7%
  triton_convolution_258 0.6035 ms 6.6%
  triton_convolution_255 0.6633 ms 6.0%
  triton_convolution_256 1.0195 ms 3.9%
SingleProcess AUTOTUNE takes 5.3748 seconds
AUTOTUNE mm(3800x256, 256x1024)
  triton_mm_262 0.0238 ms 100.0%
  mm 0.0243 ms 97.8%
  triton_mm_263 0.0250 ms 95.1%
  triton_mm_264 0.0257 ms 92.4%
  triton_mm_265 0.0272 ms 87.4%
  triton_mm_269 0.0293 ms 81.0%
  triton_mm_268 0.0295 ms 80.7%
  triton_mm_261 0.0305 ms 78.0%
  triton_mm_271 0.0396 ms 60.1%
  triton_mm_267 0.0442 ms 53.8%
SingleProcess AUTOTUNE takes 4.7657 seconds
AUTOTUNE convolution(1x512x100x152, 1024x512x1x1)
  convolution 0.0337 ms 100.0%
  triton_convolution_276 0.0992 ms 33.9%
  triton_convolution_279 0.1165 ms 28.9%
  triton_convolution_278 0.1211 ms 27.8%
  triton_convolution_277 0.1412 ms 23.8%
  triton_convolution_273 0.2287 ms 14.7%
  triton_convolution_274 0.2514 ms 13.4%
  triton_convolution_275 0.6046 ms 5.6%
SingleProcess AUTOTUNE takes 4.6606 seconds
AUTOTUNE mm(3800x1024, 1024x256)
  mm 0.0233 ms 100.0%
  triton_mm_281 0.0272 ms 85.5%
  triton_mm_282 0.0274 ms 85.1%
  triton_mm_288 0.0299 ms 78.0%
  triton_mm_283 0.0300 ms 77.6%
  triton_mm_284 0.0303 ms 76.8%
  triton_mm_285 0.0404 ms 57.6%
  triton_mm_286 0.0412 ms 56.5%
  triton_mm_280 0.0416 ms 56.0%
  triton_mm_287 0.0457 ms 50.9%
SingleProcess AUTOTUNE takes 5.2653 seconds
AUTOTUNE convolution(1x256x50x76, 256x256x3x3)
  convolution 0.0383 ms 100.0%
  triton_convolution_297 0.2234 ms 17.2%
  triton_convolution_295 0.2477 ms 15.5%
  triton_convolution_298 0.2763 ms 13.9%
  triton_convolution_296 0.3985 ms 9.6%
  triton_convolution_292 0.5106 ms 7.5%
  triton_convolution_293 0.5795 ms 6.6%
  triton_convolution_294 0.9586 ms 4.0%
SingleProcess AUTOTUNE takes 4.8271 seconds
AUTOTUNE mm(3800x1024, 1024x512)
  triton_mm_436 0.0356 ms 100.0%
  triton_mm_437 0.0384 ms 92.8%
  mm 0.0391 ms 91.2%
  triton_mm_438 0.0423 ms 84.3%
  triton_mm_439 0.0438 ms 81.4%
  triton_mm_443 0.0452 ms 78.8%
  triton_mm_435 0.0460 ms 77.6%
  triton_mm_445 0.0656 ms 54.3%
  triton_mm_440 0.0657 ms 54.2%
  triton_mm_441 0.0667 ms 53.5%
SingleProcess AUTOTUNE takes 4.6990 seconds
AUTOTUNE convolution(1x512x50x76, 512x512x3x3)
  convolution 0.0468 ms 100.0%
  triton_convolution_452 0.5966 ms 7.8%
  triton_convolution_453 0.6872 ms 6.8%
  triton_convolution_451 0.7659 ms 6.1%
  triton_convolution_450 0.8227 ms 5.7%
  triton_convolution_448 1.2245 ms 3.8%
  triton_convolution_447 1.2786 ms 3.7%
  triton_convolution_449 1.9320 ms 2.4%
SingleProcess AUTOTUNE takes 5.1135 seconds
AUTOTUNE mm(950x512, 512x2048)
  triton_mm_455 0.0234 ms 100.0%
  triton_mm_456 0.0284 ms 82.5%
  triton_mm_457 0.0286 ms 81.7%
  triton_mm_458 0.0331 ms 70.6%
  triton_mm_462 0.0332 ms 70.5%
  triton_mm_461 0.0338 ms 69.2%
  triton_mm_454 0.0353 ms 66.2%
  triton_mm_464 0.0360 ms 64.9%
  triton_mm_460 0.0420 ms 55.7%
  mm 0.0437 ms 53.6%
SingleProcess AUTOTUNE takes 5.1776 seconds
AUTOTUNE convolution(1x1024x50x76, 2048x1024x1x1)
  convolution 0.0370 ms 100.0%
  triton_convolution_469 0.1327 ms 27.9%
  triton_convolution_471 0.1453 ms 25.5%
  triton_convolution_472 0.1524 ms 24.3%
  triton_convolution_470 0.1684 ms 22.0%
  triton_convolution_466 0.2617 ms 14.1%
  triton_convolution_467 0.2755 ms 13.4%
  triton_convolution_468 0.8171 ms 4.5%
SingleProcess AUTOTUNE takes 4.6210 seconds
AUTOTUNE mm(950x2048, 2048x512)
  triton_mm_476 0.0332 ms 100.0%
  mm 0.0356 ms 93.2%
  triton_mm_477 0.0369 ms 90.0%
  triton_mm_474 0.0414 ms 80.1%
  triton_mm_475 0.0416 ms 79.8%
  triton_mm_481 0.0444 ms 74.8%
  triton_mm_479 0.0473 ms 70.1%
  triton_mm_482 0.0522 ms 63.6%
  triton_mm_478 0.0564 ms 58.8%
  triton_mm_473 0.0623 ms 53.3%
SingleProcess AUTOTUNE takes 5.2096 seconds
AUTOTUNE convolution(1x512x25x38, 512x512x3x3)
  convolution 0.0451 ms 100.0%
  triton_convolution_490 0.4810 ms 9.4%
  triton_convolution_489 0.5880 ms 7.7%
  triton_convolution_491 0.6028 ms 7.5%
  triton_convolution_488 0.6821 ms 6.6%
  triton_convolution_485 1.0695 ms 4.2%
  triton_convolution_486 1.2377 ms 3.6%
  triton_convolution_487 1.8026 ms 2.5%
SingleProcess AUTOTUNE takes 4.6562 seconds
AUTOTUNE addmm(950x256, 950x2048, 2048x256)
  triton_mm_543 0.0271 ms 100.0%
  triton_mm_544 0.0312 ms 87.0%
  triton_mm_538 0.0335 ms 80.9%
  addmm 0.0341 ms 79.5%
  triton_mm_541 0.0342 ms 79.4%
  triton_mm_539 0.0376 ms 72.2%
  triton_mm_536 0.0410 ms 66.2%
  triton_mm_540 0.0413 ms 65.7%
  triton_mm_537 0.0415 ms 65.4%
  bias_addmm 0.0514 ms 52.8%
SingleProcess AUTOTUNE takes 5.8310 seconds
AUTOTUNE convolution(1x256x25x38, 256x256x3x3)
  convolution 0.0194 ms 100.0%
  triton_convolution_551 0.1757 ms 11.1%
  triton_convolution_552 0.2231 ms 8.7%
  triton_convolution_550 0.2315 ms 8.4%
  triton_convolution_553 0.2939 ms 6.6%
  triton_convolution_547 0.5392 ms 3.6%
  triton_convolution_548 0.6017 ms 3.2%
  triton_convolution_549 0.9002 ms 2.2%
SingleProcess AUTOTUNE takes 4.7172 seconds
AUTOTUNE addmm(60800x256, 60800x256, 256x256)
  bias_addmm 0.0688 ms 100.0%
  triton_mm_556 0.0721 ms 95.4%
  triton_mm_558 0.0787 ms 87.4%
  triton_mm_555 0.0788 ms 87.3%
  triton_mm_561 0.0812 ms 84.7%
  triton_mm_557 0.0849 ms 81.0%
  triton_mm_554 0.0880 ms 78.2%
  triton_mm_562 0.0905 ms 76.1%
  triton_mm_564 0.1194 ms 57.6%
  addmm 0.1229 ms 56.0%
SingleProcess AUTOTUNE takes 5.3490 seconds
AUTOTUNE addmm(15200x256, 15200x512, 512x256)
  bias_addmm 0.0365 ms 100.0%
  triton_mm_568 0.0369 ms 98.9%
  triton_mm_567 0.0388 ms 94.0%
  triton_mm_570 0.0412 ms 88.7%
  triton_mm_569 0.0413 ms 88.4%
  triton_mm_574 0.0441 ms 82.8%
  triton_mm_566 0.0473 ms 77.1%
  addmm 0.0500 ms 73.0%
  triton_mm_573 0.0557 ms 65.6%
  triton_mm_572 0.0686 ms 53.2%
SingleProcess AUTOTUNE takes 5.3958 seconds
AUTOTUNE addmm(3800x256, 3800x1024, 1024x256)
  bias_addmm 0.0237 ms 100.0%
  triton_mm_580 0.0281 ms 84.2%
  triton_mm_579 0.0283 ms 83.6%
  addmm 0.0296 ms 79.9%
  triton_mm_581 0.0304 ms 78.0%
  triton_mm_586 0.0305 ms 77.6%
  triton_mm_582 0.0316 ms 75.1%
  triton_mm_583 0.0412 ms 57.5%
  triton_mm_584 0.0413 ms 57.4%
  triton_mm_578 0.0418 ms 56.7%
SingleProcess AUTOTUNE takes 5.5988 seconds
AUTOTUNE convolution(1x256x200x304, 256x256x3x3)
  convolution 0.3133 ms 100.0%
  triton_convolution_595 1.8799 ms 16.7%
  triton_convolution_593 2.3012 ms 13.6%
  triton_convolution_596 2.5771 ms 12.2%
  triton_convolution_590 2.7492 ms 11.4%
  triton_convolution_594 4.8760 ms 6.4%
  triton_convolution_591 4.9998 ms 6.3%
  triton_convolution_592 8.7226 ms 3.6%
SingleProcess AUTOTUNE takes 4.7833 seconds
AUTOTUNE convolution(1x256x100x152, 256x256x3x3)
  convolution 0.0947 ms 100.0%
  triton_convolution_602 0.6173 ms 15.3%
  triton_convolution_600 0.6671 ms 14.2%
  triton_convolution_603 0.7065 ms 13.4%
  triton_convolution_601 0.9826 ms 9.6%
  triton_convolution_597 1.0423 ms 9.1%
  triton_convolution_598 1.2019 ms 7.9%
  triton_convolution_599 2.8248 ms 3.4%
SingleProcess AUTOTUNE takes 4.8948 seconds
AUTOTUNE addmm(60800x12, 60800x256, 256x12)
  triton_mm_620 0.0371 ms 100.0%
  triton_mm_619 0.0380 ms 97.6%
  triton_mm_625 0.0388 ms 95.5%
  triton_mm_618 0.0388 ms 95.5%
  triton_mm_626 0.0392 ms 94.7%
  triton_mm_622 0.0394 ms 94.1%
  triton_mm_621 0.0398 ms 93.2%
  triton_mm_623 0.0403 ms 92.0%
  triton_mm_628 0.0410 ms 90.5%
  triton_mm_624 0.0421 ms 88.1%
SingleProcess AUTOTUNE takes 3.9815 seconds
AUTOTUNE addmm(15200x12, 15200x256, 256x12)
  triton_mm_639 0.0147 ms 100.0%
  triton_mm_640 0.0148 ms 99.8%
  triton_mm_642 0.0148 ms 99.8%
  triton_mm_638 0.0148 ms 99.6%
  triton_mm_641 0.0151 ms 97.3%
  triton_mm_645 0.0151 ms 97.3%
  triton_mm_643 0.0154 ms 95.4%
  triton_mm_646 0.0160 ms 92.0%
  triton_mm_637 0.0161 ms 91.6%
  bias_addmm 0.0169 ms 87.0%
SingleProcess AUTOTUNE takes 4.2577 seconds
AUTOTUNE addmm(3800x12, 3800x256, 256x12)
  triton_mm_662 0.0097 ms 100.0%
  triton_mm_661 0.0097 ms 99.3%
  triton_mm_660 0.0099 ms 97.7%
  triton_mm_659 0.0100 ms 96.8%
  triton_mm_664 0.0100 ms 96.8%
  triton_mm_657 0.0103 ms 93.8%
  triton_mm_665 0.0107 ms 90.7%
  triton_mm_658 0.0108 ms 89.9%
  bias_addmm 0.0117 ms 82.5%
  triton_mm_656 0.0140 ms 69.1%
SingleProcess AUTOTUNE takes 4.2619 seconds
AUTOTUNE addmm(950x12, 950x256, 256x12)
  triton_mm_684 0.0087 ms 100.0%
  triton_mm_681 0.0088 ms 98.6%
  triton_mm_680 0.0095 ms 91.9%
  triton_mm_683 0.0095 ms 91.9%
  triton_mm_678 0.0096 ms 90.5%
  bias_addmm 0.0101 ms 86.3%
  triton_mm_679 0.0101 ms 86.1%
  triton_mm_676 0.0104 ms 83.7%
  triton_mm_677 0.0108 ms 81.0%
  triton_mm_675 0.0133 ms 65.2%
SingleProcess AUTOTUNE takes 4.5821 seconds
AUTOTUNE convolution(1x256x13x19, 256x256x3x3)
  convolution 0.0175 ms 100.0%
  triton_convolution_691 0.1681 ms 10.4%
  triton_convolution_692 0.2362 ms 7.4%
  triton_convolution_690 0.2405 ms 7.3%
  triton_convolution_689 0.3076 ms 5.7%
  triton_convolution_693 0.3882 ms 4.5%
  triton_convolution_687 0.5370 ms 3.3%
  triton_convolution_688 0.6111 ms 2.9%
SingleProcess AUTOTUNE takes 4.4704 seconds
AUTOTUNE addmm(247x12, 247x256, 256x12)
  bias_addmm 0.0110 ms 100.0%
  triton_mm_703 0.0144 ms 76.6%
  addmm 0.0148 ms 74.1%
  triton_mm_705 0.0163 ms 67.7%
  triton_mm_700 0.0172 ms 64.2%
  triton_mm_697 0.0173 ms 63.6%
  triton_mm_695 0.0173 ms 63.5%
  triton_mm_702 0.0174 ms 63.4%
  triton_mm_699 0.0179 ms 61.5%
  triton_mm_694 0.0184 ms 59.9%
SingleProcess AUTOTUNE takes 4.5535 seconds
AUTOTUNE addmm(60800x3, 60800x256, 256x3)
  triton_mm_708 0.0368 ms 100.0%
  triton_mm_707 0.0380 ms 96.9%
  triton_mm_706 0.0388 ms 95.0%
  triton_mm_713 0.0388 ms 95.0%
  triton_mm_710 0.0395 ms 93.3%
  triton_mm_714 0.0397 ms 92.7%
  triton_mm_709 0.0398 ms 92.5%
  triton_mm_711 0.0403 ms 91.4%
  triton_mm_716 0.0407 ms 90.4%
  triton_mm_717 0.0430 ms 85.7%
SingleProcess AUTOTUNE takes 4.0660 seconds
AUTOTUNE addmm(15200x3, 15200x256, 256x3)
  triton_mm_719 0.0141 ms 100.0%
  triton_mm_721 0.0142 ms 99.3%
  triton_mm_722 0.0146 ms 96.9%
  triton_mm_720 0.0148 ms 95.5%
  triton_mm_723 0.0154 ms 92.1%
  triton_mm_726 0.0158 ms 89.5%
  triton_mm_718 0.0162 ms 87.4%
  triton_mm_725 0.0165 ms 85.7%
  triton_mm_728 0.0202 ms 70.2%
  triton_mm_729 0.0202 ms 70.0%
SingleProcess AUTOTUNE takes 3.8739 seconds
AUTOTUNE addmm(3800x3, 3800x256, 256x3)
  triton_mm_735 0.0092 ms 100.0%
  triton_mm_733 0.0096 ms 95.3%
  triton_mm_734 0.0100 ms 92.3%
  triton_mm_738 0.0100 ms 92.0%
  triton_mm_732 0.0108 ms 85.2%
  triton_mm_731 0.0109 ms 84.4%
  triton_mm_736 0.0111 ms 82.5%
  triton_mm_739 0.0118 ms 77.8%
  triton_mm_730 0.0135 ms 68.0%
  bias_addmm 0.0142 ms 64.5%
SingleProcess AUTOTUNE takes 3.9040 seconds
AUTOTUNE addmm(950x3, 950x256, 256x3)
  triton_mm_748 0.0090 ms 100.0%
  triton_mm_751 0.0093 ms 96.6%
  triton_mm_747 0.0095 ms 95.3%
  triton_mm_750 0.0095 ms 94.9%
  triton_mm_745 0.0097 ms 93.1%
  triton_mm_743 0.0099 ms 91.6%
  triton_mm_746 0.0108 ms 83.3%
  triton_mm_744 0.0114 ms 79.4%
  bias_addmm 0.0117 ms 77.0%
  triton_mm_742 0.0130 ms 69.6%
SingleProcess AUTOTUNE takes 4.4428 seconds
AUTOTUNE addmm(247x3, 247x256, 256x3)
  bias_addmm 0.0108 ms 100.0%
  triton_mm_763 0.0142 ms 76.1%
  addmm 0.0145 ms 74.2%
  triton_mm_765 0.0162 ms 66.7%
  triton_mm_760 0.0167 ms 64.7%
  triton_mm_759 0.0173 ms 62.2%
  triton_mm_762 0.0173 ms 62.2%
  triton_mm_755 0.0177 ms 61.1%
  triton_mm_757 0.0177 ms 61.1%
  triton_mm_761 0.0184 ms 58.7%
SingleProcess AUTOTUNE takes 4.2795 seconds
ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 2378, in warmup
    fn(model, example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 529, in forward_pass
    return mod(*inputs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/vision/torchvision/models/detection/generalized_rcnn.py", line 104, in forward
    proposals, proposal_losses = self.rpn(images, features, targets)
  File "/home/cdhernandez/local/vision/torchvision/models/detection/generalized_rcnn.py", line 105, in resume_in_forward
    detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cdhernandez/local/vision/torchvision/models/detection/roi_heads.py", line 761, in forward
    box_features = self.box_roi_pool(features, proposals, image_shapes)
  File "/home/cdhernandez/local/vision/torchvision/models/detection/roi_heads.py", line 775, in resume_in_forward
    boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)
  File "/home/cdhernandez/local/vision/torchvision/models/detection/roi_heads.py", line 804, in resume_in_forward
    mask_features = self.mask_roi_pool(features, mask_proposals, image_shapes)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/eval_frame.py", line 655, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 722, in _convert_frame
    result = inner_convert(frame, cache_entry, hooks, frame_state)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert
    compiled_product = _compile(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 646, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 562, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object
    transformations(instructions, code_options)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 151, in _fn
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/convert_frame.py", line 527, in transform
    tracer.run()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 2123, in run
    super().run()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 818, in run
    and self.step()
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 781, in step
    getattr(self, inst.opname)(inst)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/symbolic_convert.py", line 2238, in RETURN_VALUE
    self.output.compile_subgraph(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 938, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 1080, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 1152, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/output_graph.py", line 1133, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/__init__.py", line 1657, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 952, in compile_fx
    return compile_fx(
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 1168, in compile_fx
    return aot_autograd(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/backends/common.py", line 55, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/aot_autograd.py", line 885, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/aot_autograd.py", line 598, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 424, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 629, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/cdhernandez/local/pytorch/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 97, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 1100, in fw_compiler_base
    return inner_compile(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/debug.py", line 305, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 320, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/home/cdhernandez/local/pytorch/torch/_inductor/compile_fx.py", line 535, in fx_codegen_and_compile
    graph.run(*example_inputs)
  File "/home/cdhernandez/local/pytorch/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/graph.py", line 516, in run
    return super().run(*args)
  File "/home/cdhernandez/local/pytorch/torch/fx/interpreter.py", line 138, in run
    self.env[node] = self.run_node(node)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/graph.py", line 809, in run_node
    result = self.call_function(n.target, args, kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/graph.py", line 689, in call_function
    raise LoweringException(e, target, args, kwargs).with_traceback(
  File "/home/cdhernandez/local/pytorch/torch/_inductor/graph.py", line 686, in call_function
    out = lowerings[target](*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/lowering.py", line 291, in wrapped
    out = decomp_fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/kernel/conv.py", line 367, in convolution
    result = convolution(x, weight, None, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/lowering.py", line 291, in wrapped
    out = decomp_fn(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/kernel/conv.py", line 457, in convolution
    return autotune_select_algorithm("convolution", choices, args, layout)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 988, in autotune_select_algorithm
    return _ALGORITHM_SELECTOR_CACHE(*args, **kwargs)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 745, in __call__
    timings = self.lookup(
  File "/home/cdhernandez/local/pytorch/torch/_inductor/codecache.py", line 291, in lookup
    timings = benchmark(choices)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 736, in autotune
    return make_benchmark_fn()(choices)
  File "/home/cdhernandez/local/pytorch/torch/_inductor/select_algorithm.py", line 862, in benchmark_in_current_process
    raise AssertionError(  # noqa: TRY200
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
LoweringException: AssertionError: Incorrect result from choice ExternKernelCaller(extern_kernels.convolution)

expected size 256==256, stride 196==1 at dim=1
  target: aten.convolution.default
  args[0]: TensorBox(StorageBox(
    InputBuffer(name='arg12_1', layout=FixedLayout('cuda', torch.float16, size=[0, 256, 14, 14], stride=[50176, 196, 14, 1]))
  ))
  args[1]: TensorBox(StorageBox(
    InputBuffer(name='arg0_1', layout=FixedLayout('cuda', torch.float16, size=[256, 256, 3, 3], stride=[2304, 9, 3, 1]))
  ))
  args[2]: TensorBox(StorageBox(
    InputBuffer(name='arg1_1', layout=FixedLayout('cuda', torch.float16, size=[256], stride=[1]))
  ))
  args[3]: [1, 1]
  args[4]: [1, 1]
  args[5]: [1, 1]
  args[6]: False
  args[7]: [0, 0]
  args[8]: 1

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

Run failed with return code:  255
Output:  None
Error:  None
loading model: 0it [00:00, ?it/s]loading model: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 36, in <module>
    run()
  File "/home/cdhernandez/local/benchmark/run_benchmark.py", line 30, in run
    benchmark.run(bm_args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/run.py", line 24, in run
    main(TorchBenchmarkRunner(), original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3131, in main
    process_entry(0, runner, original_dir, args)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3088, in process_entry
    return maybe_fresh_cache(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 1714, in inner
    return fn(*args, **kwargs)
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/common.py", line 3556, in run
    ) = runner.load_model(
  File "/home/cdhernandez/local/benchmark/userbenchmark/dynamo/dynamobench/torchbench.py", line 372, in load_model
    module = importlib.import_module(c)
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/yolov3/__init__.py", line 15, in <module>
    from .yolo_train import prepare_training_loop
  File "/home/cdhernandez/local/benchmark/torchbenchmark/models/yolov3/yolo_train.py", line 6, in <module>
    from torch.utils.tensorboard import SummaryWriter
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/__init__.py", line 12, in <module>
    from .writer import FileWriter, SummaryWriter  # noqa: F401
  File "/home/cdhernandez/local/pytorch/torch/utils/tensorboard/writer.py", line 12, in <module>
    from tensorboard.compat.proto import event_pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/event_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/summary_pb2.py", line 17, in <module>
    from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/resource_handle_pb2.py", line 16, in <module>
    from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py", line 36, in <module>
    _descriptor.FieldDescriptor(
  File "/home/cdhernandez/local/miniconda3/envs/pytorch/lib/python3.10/site-packages/google/protobuf/descriptor.py", line 553, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
Run failed with return code:  1
Output:  None
Error:  None
speedup             gmean=0.00x mean=2.028x
abs_latency         gmean=0.00x mean=7.570x
compilation_latency mean=18.781 seconds
compression_ratio   mean=0.774x
eager_peak_mem      gmean=0.00x mean=0.413x
dynamo_peak_mem     gmean=0.00x mean=0.478x
calls_captured      gmean=0.00x mean=225.338x
unique_graphs       gmean=0.00x mean=1.779x
graph_breaks        gmean=0.00x mean=0.868x
unique_graph_breaks gmean=0.00x mean=0.279x
